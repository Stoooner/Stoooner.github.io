<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stoner的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-29T13:18:09.277Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Stoner</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>模组进行视频录制及HEVC视频编解码</title>
    <link href="http://yoursite.com/2020/07/29/%E6%A8%A1%E7%BB%84%E8%BF%9B%E8%A1%8C%E8%A7%86%E9%A2%91%E5%BD%95%E5%88%B6%E5%8F%8AHEVC%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2020/07/29/%E6%A8%A1%E7%BB%84%E8%BF%9B%E8%A1%8C%E8%A7%86%E9%A2%91%E5%BD%95%E5%88%B6%E5%8F%8AHEVC%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/</id>
    <published>2020-07-29T13:08:19.000Z</published>
    <updated>2020-07-29T13:18:09.277Z</updated>
    
    <content type="html"><![CDATA[<p><strong>内容说明</strong>：记录模组录制音视频以及HEVC视频编解码过程.<br><a id="more"></a></p><h2 id="1-视频录制"><a href="#1-视频录制" class="headerlink" title="1. 视频录制"></a>1. 视频录制</h2><ol><li>将模组与电脑进行连接；</li><li>模组进行视频录制有三种方式：<blockquote><p>2.1 网页端进行视频录制；<br>2.2 <strong>VLC</strong>进行视频录制；<br>2.3 <strong>ffmpeg</strong>进行录制；</p></blockquote></li><li><p>这里使用<strong>ffmpeg</strong>的方式进行录制。打开<strong>ffmpeg</strong>，其中该文件夹下的内容如下：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595900974999.png" alt><br>其中红色框起来的四个<strong>.bat</strong>文件分别对应模组摄像头的四个<strong>Stream</strong>进行视频查看，也即打开摄像头查看外部景象，但是不会进行视频录制。这四个文件的代码基本一致，都是调用<strong>ffplay</strong>进行视频的查看播放，代码如下：</p><p> ffplay rtsp://169.254.28.10/stream0<br>上述<strong>.bat</strong>内部就是这样一句话，其中<strong>ffplay</strong>代表调用的是<strong>ffplay.exe</strong>，接下来的<strong>rtsp://169.254.28.10/stream0</strong>代表的是格式为<strong>rtsp</strong>的<strong>ip</strong>为<strong>169.254.28.10</strong>的模组的第一个流<strong>stream0</strong>；通过运行上述这一代码，便可打开摄像头开到景象：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595901416918.png" alt><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595901453964.png" alt><br>相应的，需要查看别的流是否正常的时候，就调用别的流的<strong>.bat</strong>文件即可；</p></li><li><p>要进行视频的录制，需要点击带<strong>dump</strong>的<strong>.bat</strong>文件，这种文件调用的则是<strong>ffmpeg.exe</strong>进行的视频录制：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595901761058.png" alt><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595901781078.png" alt><br>对应的代码为：</p><p> ffmpeg.exe  -i rtsp://169.254.28.10/stream0 -c copy -f h264 test_stream_1.h264<br>其中，<strong>-i, -c, -f</strong>为对应的需要传入的参数。可以看到帧的数目以及录制的视频的文件大小都在不停的变动，当视频录制完毕之后便可关闭<strong>cmd</strong>页面，相应的，在<strong>ffmpeg</strong>文件夹下也就有了对应的录制好的视频：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595901883955.png" alt><br>对于录制好的视频，需要查看其码流、码率等相关一些参数的时候，使用<strong>Elecard StreamEye Demo</strong>这一软件，直接将录制好的视频文件拖入该软件即可：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1595902095866.png" alt></p></li></ol><h2 id="2-HEVC视频编解码"><a href="#2-HEVC视频编解码" class="headerlink" title="2. HEVC视频编解码"></a>2. HEVC视频编解码</h2><h3 id="2-1-HEVC编解码器工程文件编译"><a href="#2-1-HEVC编解码器工程文件编译" class="headerlink" title="2.1 HEVC编解码器工程文件编译"></a>2.1 HEVC编解码器工程文件编译</h3><ol><li>首先，需要将<strong>Visual Studio2015</strong>打开，然后在主界面点击<strong>文件—打开—项目/解决方案(P)</strong>：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1596009063506.png" alt></li><li>找到<strong>HEVC</strong>编码器的工程文件<strong>HM-16.7</strong>，打开<strong>HM-16.7/build/HM_vc2013.sln</strong>文件；</li><li>在主界面右边的<strong>HM_vc2013</strong>项目这里右键点击<strong>生产解决方案(B)</strong>选项；</li><li>等待项目编译，项目编译完毕之后，在下面的控制台会看到：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1596009318020.png" alt></li></ol><h3 id="2-2-HEVC解码-编码"><a href="#2-2-HEVC解码-编码" class="headerlink" title="2.2 HEVC解码/编码"></a>2.2 HEVC解码/编码</h3><blockquote><p>使用<strong>HEVC</strong>编码器对视频进行编解码，相应的操作流程和命令行格式如下：</p></blockquote><ol><li>首先<strong>HM</strong>项目编译完毕之后，会在其<strong>\bin\vc2013\Win32\Debug </strong>文件夹下生产包括编解码器等可执行文件(<strong>exe</strong>)在内的多个文件，其中使用解码器就用<strong>TAppDecoder.exe</strong>，使用编码器就用<strong>TAppEncoder.exe</strong>：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1596009911404.png" alt></li><li>通过模组上的摄像头以及<strong>ffmpeg</strong>进行视频拍摄之后，得到相应的编码文件<strong>ksh_stream_1.h265</strong>，需要注意的是在<strong>mplayer_h264_dump_1.bat</strong>文件中，设定的格式是<strong>h264</strong>(<strong>ffmpeg</strong>暂不支持<strong>h265</strong>)，但是实际上，我们的<strong>Stream0</strong>设定的格式是<strong>h265</strong>的，所以这里<strong>ffmpeg</strong>设定的格式对其无影响；</li><li>对编码文件<strong>ksh_stream_1.h265</strong>进行解码，解码还原为<strong>YUV</strong>文件，相应的命令行格式如下：</li></ol><pre><code>TAppDecoder.exe -b \xx路径\xxname.h265 -o \xx路径\xxx.yuv</code></pre><p>其中<strong>-b</strong>后面接的是需要进行解码的编码码流，<strong>-o</strong>后面接的是解码后对应的解码文件；需要注意的是，<strong>POC 0-24</strong>代表的是<strong>GOP</strong>组中对应的<strong>I帧/P帧/B帧</strong>等图片；</p><ol><li>相应的，对编码了的码流能够进行编码，也就相应的可以对解码了的<strong>YUV</strong>文件进行编码，使用的是命令行格式如下：</li></ol><pre><code>TAppEncoder.exe -c xxxname.cfg -wdt 1280 -hgt 720 -fr 25 -f 500 -i xxxname.yuv -b xxxxname.h265</code></pre><p>其中，一些参数的解释如下：</p><blockquote><ul><li>-wdt 横向分辨率</li><li>-hgt 纵向分辨率</li><li>-fr 帧率</li><li>-f 编码帧数(对原解码YUV文件进行多少帧的编码)</li><li>-i 输入文件</li><li>-b 输出文件</li></ul></blockquote><p><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1596012520636.png" alt><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1596012577881.png" alt><br>其中，<strong>.cfg</strong>文件是编码时候需要选择的编码方式配置文件，这个配置文件可以在<strong>\HM-16.7\cfg\\</strong>文件夹下进行选择的。</p><h2 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3. 参考资料"></a>3. 参考资料</h2><blockquote><ol><li><a href="https://tieba.baidu.com/p/2325281215?pid=32688262646&amp;see_lz=1&amp;red_tag=0406128317" target="_blank" rel="noopener">hm10编码教程；</a></li><li><a href="https://blog.csdn.net/weixin_30236595/article/details/94985105" target="_blank" rel="noopener">各种视频编码器的命令行格式；</a></li><li><a href="https://blog.csdn.net/wangcm04/article/details/25813237" target="_blank" rel="noopener">HM13.0 TAppDecoder参数设置；</a></li><li><a href="https://blog.csdn.net/u010485442/article/details/38308013" target="_blank" rel="noopener">HM学习心得2；</a></li><li><a href="https://blog.csdn.net/leixiaohua1020/article/details/49912113" target="_blank" rel="noopener">HEVC官方软件HM源代码简单分析-编码器TAppEncoder；</a></li><li><a href="https://space.bilibili.com/432184234/channel/detail?cid=129412" target="_blank" rel="noopener">音视频高手开发；</a></li><li><a href="https://space.bilibili.com/542307689/video?keyword=ffmpeg" target="_blank" rel="noopener">FFmpeg入门；</a></li><li><a href="https://space.bilibili.com/576287359/video?keyword=%E9%9F%B3%E8%A7%86%E9%A2%91" target="_blank" rel="noopener">音视频开发；</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;内容说明&lt;/strong&gt;：记录模组录制音视频以及HEVC视频编解码过程.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="视频编解码" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/"/>
    
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
      <category term="模组" scheme="http://yoursite.com/tags/%E6%A8%A1%E7%BB%84/"/>
    
      <category term="codec" scheme="http://yoursite.com/tags/codec/"/>
    
  </entry>
  
  <entry>
    <title>评分卡模型中的IV和WOE详解</title>
    <link href="http://yoursite.com/2020/07/27/IV-WOE/"/>
    <id>http://yoursite.com/2020/07/27/IV-WOE/</id>
    <published>2020-07-27T14:49:09.000Z</published>
    <updated>2020-07-29T13:05:38.079Z</updated>
    
    <content type="html"><![CDATA[<p><strong>内容说明</strong>：评分卡中各个特征的筛选和模型性能评估的指标.<br><a id="more"></a></p><h2 id="1-IV的用途"><a href="#1-IV的用途" class="headerlink" title="1. IV的用途"></a>1. IV的用途</h2><p><strong>IV</strong>的全称是<strong>Information Value</strong>，中文意思是信息价值，或者信息量。我们在用逻辑回归、决策树等模型方法构建分类模型时，经常需要对自变量(也就是我们说的特征，<strong>Features</strong>)进行筛选。比如我们有$200$个候选自变量，通常情况下，不会直接把$200$个变量直接放到模型中去进行拟合训练，而是会用一些方法，从这$200$个自变量中挑选一些出来，放进模型，形成入模变量列表。那么我们怎么去挑选入模变量呢？</p><p>挑选入模变量过程是个比较复杂的过程，需要考虑的因素很多，比如：</p><blockquote><ul><li><strong>变量的预测能力</strong>；</li><li><strong>变量之间的相关性</strong>；</li><li><strong>变量的简单性（容易生成和使用）</strong>；</li><li><strong>变量的强壮性（不容易被绕过）</strong>；</li><li><strong>变量在业务上的可解释性（被挑战时可以解释的通）等等</strong>。</li></ul></blockquote><p>但是，其中最主要和最直接的衡量标准是变量的预测能力。</p><p><strong>“变量的预测能力”</strong>这个说法很笼统，很主观，非量化，在筛选变量的时候我们总不能说：“我觉得这个变量预测能力很强，所以他要进入模型”吧？我们需要一些具体的量化指标来衡量每自变量的预测能力，并根据这些量化指标的大小，来确定哪些变量进入模型。<strong>IV</strong>就是这样一种指标，他可以用来衡量自变量的预测能力。类似的指标还有<strong>信息增益、基尼系数</strong>等等。</p><h2 id="2-对IV的直观理解"><a href="#2-对IV的直观理解" class="headerlink" title="2. 对IV的直观理解"></a>2. 对IV的直观理解</h2><p>从直观逻辑上大体可以这样理解<strong>“用IV去衡量变量预测能力”</strong>这件事情：我们假设在一个分类问题中，<strong>目标变量(也就是我们所说的标签)</strong>类别两类：$Y_1，Y_2$。</p><p>对于一个待预测的个体$A$(也就是一个样本)，要判断$A$属于类别$Y_1$还是$Y_2$，我们是需要一定的信息的，假设这个信息总量是$I$，而这些所需要的信息，就蕴含在所有的自变量$C_1，C_2，C_3，……，C_n$中。</p><p>那么，对于其中的一个变量$C_i$来说，其蕴含的信息越多，那么它对于判断$A$属于$Y_1$还是$Y_2$的贡献就越大，$C_i$的信息价值就越大，$C_i$的$IV$就越大，它就越应该进入到入模变量列表中。</p><h2 id="3-IV的计算"><a href="#3-IV的计算" class="headerlink" title="3. IV的计算"></a>3. IV的计算</h2><p>前面我们从感性角度和逻辑层面对<strong>IV</strong>进行了解释和描述，那么回到数学层面，对于一个待评估变量，他的<strong>IV</strong>值究竟如何计算呢？为了介绍<strong>IV</strong>的计算方法，我们首先需要认识和理解另一个概念——<strong>WOE</strong>，因为<strong>IV</strong>的计算是以<strong>WOE</strong>为基础的。</p><h3 id="3-1-WOE"><a href="#3-1-WOE" class="headerlink" title="3.1 WOE"></a>3.1 WOE</h3><p><strong>WOE</strong>的全称是<strong>“Weight of Evidence”</strong>，即证据权重。<strong>WOE</strong>是对原始自变量的一种编码形式。</p><p>要对一个变量进行<strong>WOE</strong>编码，需要首先把这个变量进行分组处理（也叫离散化、分箱等等，说的都是一个意思）。分组后，对于第<strong>i</strong>组，<strong>WOE</strong>的计算公式如下：</p><script type="math/tex; mode=display">WOE_i = \ln \frac{P_{y_i}}{P_{n_i}} = \ln \frac{\#y_i/\#y_T}{\#n_i/\#n_T}</script><p>其中:</p><blockquote><ul><li>$P_{y_i}$是这个组中响应客户(风险模型中，对应的是违约客户，总之，指的是模型中预测变量或者说标签取值为<strong>“是”</strong>或者取值为$1$的个体)占所有样本中所有响应客户比例;</li><li>$P_{n_i}$是这个组中未响应客户占样本中所有未响应客户的比例;</li><li>$\#y_i$是这个组中响应客户的数量，$\#n_i$是这个组中未响应客户的数量，$\#y_T$是样本中所有响应客户的数量，$\#n_T$是样本中所有未响应客户的数量。</li></ul></blockquote><p>从这个公式中我们可以体会到，<strong>WOE</strong>表示的实际上是<strong>“当前分组中响应客户占所有响应客户的比例”</strong>和<strong>“当前分组中没有响应的客户占所有没有响应的客户的比例”</strong>的差异。对这个公式做一个简单变换，可以得到：</p><script type="math/tex; mode=display">WOE_i = \ln \frac{P_{y_i}}{P_{n_i}} = \ln \frac{\#y_i/\#y_T}{\#n_i/\#n_T} = \ln \frac{\#y_i/\#n_i}{\#y_T/\#n_T}</script><p>变换以后我们可以看出，<strong>WOE</strong>也可以这么理解，<strong><em>他表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。</em></strong>这个差异是用这两个比值的比值，再取对数来表示的。</p><p><strong>WOE</strong>越大，这种差异越大，这个分组里的样本响应的可能性就越大，<strong>WOE</strong>越小，差异越小，这个分组里的样本响应的可能性就越小(其实，<strong>WOE</strong>的公式就是一个<strong>Odds</strong>的公式)。关于<strong>WOE</strong>编码所表示的意义，大家可以自己再好好体会一下。</p><h3 id="3-2-IV的计算公式"><a href="#3-2-IV的计算公式" class="headerlink" title="3.2 IV的计算公式"></a>3.2 IV的计算公式</h3><p>有了前面的介绍，我们可以正式给出<strong>IV</strong>的计算公式。对于一个分组后的变量，第<strong>i</strong>组的<strong>WOE</strong>前面已经介绍过，是这样计算的：</p><script type="math/tex; mode=display">WOE_i = \ln \frac{P_{y_i}}{P_{n_i}} = \ln \frac{\#y_i/\#y_T}{\#n_i/\#n_T} = \ln \frac{\#y_i/\#n_i}{\#y_T/\#n_T}</script><p>同样，对于分组<strong>i</strong>，也会有一个对应的<strong>IV</strong>值，计算公式如下：</p><script type="math/tex; mode=display">IV_i = (P_{y_i} - P_{n_i}) * WOE_i = (P_{y_i} - P_{n_i}) * \ln\frac{P_{y_i}}{P_{n_i}} \\ = (\#y_i/\#y_T - \#n_i/\#n_T) * \ln\frac{\#y_i/\#y_T}{\#n_i/\#n_T}</script><p>有了一个变量各分组的<strong>IV</strong>值，我们就可以计算整个变量的<strong>IV</strong>值，方法很简单，就是把各分组的<strong>IV</strong>相加：</p><script type="math/tex; mode=display">IV = \sum_i^{n}IV_i</script><p>其中$n$为变量分组个数(也就是这个特征被离散化之后的份数)。</p><h3 id="3-3-用实例介绍IV的计算和使用"><a href="#3-3-用实例介绍IV的计算和使用" class="headerlink" title="3.3 用实例介绍IV的计算和使用"></a>3.3 用实例介绍IV的计算和使用</h3><p>下面我们通过一个实例来讲解一下IV的使用方式。</p><h4 id="3-3-1-实例"><a href="#3-3-1-实例" class="headerlink" title="3.3.1 实例"></a>3.3.1 实例</h4><p>假设我们需要构建一个预测模型，这个模型是为了预测公司的客户集合中的每个客户对于我们的某项营销活动是否能够响应，或者说我们要预测的是客户对我们的这项营销活动响应的可能性有多大。假设我们已经从公司客户列表中随机抽取了<strong>100000</strong>个客户进行了营销活动测试，收集了这些客户的响应结果，作为我们的建模数据集，其中响应的客户有<strong>10000</strong>个。另外假设我们也已经提取到了这些客户的一些变量，作为我们模型的候选变量集，这些变量包括以下这些(实际情况中，我们拥有的变量可能比这些多得多，这里列出的变量(特征)仅是为了说明我们的问题)：</p><blockquote><ul><li><strong>最近一个月是否有购买</strong>；</li><li><strong>最近一次购买金额</strong>；</li><li><strong>最近一笔购买的商品类别</strong>；</li><li><strong>是否是公司VIP客户</strong>；</li></ul></blockquote><p>假设，我们已经对这些变量进行了离散化，统计的结果如下面几张表所示。</p><ul><li><strong>最近一个月是否有过购买：</strong><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583806730177.png" alt="图1. 最近一个月的购买情况"></li><li><strong>最近一次购买金额：</strong><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583806768409.png" alt="图2. 最近一次购买金额情况"></li><li><strong>最近一笔购买的商品类别：</strong><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583806789695.png" alt="图3. 最近一笔购买的商品类别情况"></li><li><strong>是否是公司VIP客户：</strong><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583806825151.png" alt="图4. 是否是公司的VIP客户"></li></ul><h4 id="3-3-2-计算WOE和IV"><a href="#3-3-2-计算WOE和IV" class="headerlink" title="3.3.2 计算WOE和IV"></a>3.3.2 计算WOE和IV</h4><p>我们以其中的一个变量<strong>“最近一次购买金额”</strong>变量为例：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583806875756.png" alt><br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583806886656.png" alt><br>我们把这个变量离散化为了$4$个分段：$&lt;100元，[100,200)，[200,500)，\geqslant500$元。首先，根据<strong>WOE</strong>计算公式，这四个分段的<strong>WOE</strong>分别为：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583807164160.png" alt><br>插播一段，从上面的计算结果中我们可以看一下<strong>WOE</strong>的基本特点：</p><blockquote><ul><li>当前分组中，响应的比例越大，<strong>WOE</strong>值越大；</li><li>当前分组<strong>WOE</strong>的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，<strong>WOE</strong>为负<strong>(对数函数的性质决定的)</strong>；相应的，当前分组的比例大于整体比例时，<strong>WOE</strong>为正；当前分组的比例和整体比例相等时，<strong>WOE</strong>为$0$;</li><li><strong>WOE</strong>的取值范围是全体实数。</li></ul></blockquote><p>我们进一步理解一下<strong>WOE</strong>，会发现，<strong>WOE</strong>其实描述了这个离散化的变量(特征)在当前的离散化分组下，对判断个体是否会响应（或者说属于哪个类）所起到影响方向和大小，当<strong>WOE</strong>为正时，变量当前取值对判断个体是否会响应起到的正向的影响，当<strong>WOE</strong>为负时，起到了负向影响。而<strong>WOE</strong>值的大小，则是这个影响的大小的体现。</p><p>好，回到正题，计算完<strong>WOE</strong>，我们分别计算四个分组的<strong>IV</strong>值：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583808418826.png" alt><br>再插播一段，从上面IV的计算结果我们可以看出<strong>IV</strong>的以下特点：</p><blockquote><ul><li>对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，$IV$值越大，否则，$IV$值越小；</li><li>极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，$IV$值为$0$；</li><li>$IV$值的取值范围是$[0,+\infty)$，也就是说前面的权重$(P_{y_i} - P_{n_i})$的正负与后面的$WOE_i$的正负性是一致的，且当当前分组中只包含响应客户或者未响应客户时，$IV = +\infty$。</li></ul></blockquote><p>再次回到正题。最后，我们计算变量总<strong>IV</strong>值：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583809291499.png" alt></p><h4 id="3-3-3-IV值的比较和变量预测能力的排序"><a href="#3-3-3-IV值的比较和变量预测能力的排序" class="headerlink" title="3.3.3 IV值的比较和变量预测能力的排序"></a>3.3.3 IV值的比较和变量预测能力的排序</h4><p>我们已经计算了四个变量中其中一个的<strong>WOE</strong>和<strong>IV</strong>值。另外三个的计算过程我们不再详细的说明，直接给出<strong>IV</strong>结果:</p><blockquote><ul><li>最近一个月是否有过购买：$IV = 0.250224725$；</li><li>最近一笔购买的商品类别：$IV = 0.615275563$；</li><li>是否是公司<strong>VIP</strong>客户：$IV = 1.56550367$。</li></ul></blockquote><p>前面我们已经计算过，最近一次购买金额的$IV$为$0.49270645$</p><p>这四个变量$IV$排序结果是这样的：</p><script type="math/tex; mode=display">是否是公司VIP客户 > 最近一笔购买的商品类别 > \\ 最近一次购买金额 > 最近一个月是否有过购买</script><p>我们发现<strong>“是否是公司VIP客户”</strong>这个特征是预测能力最高的变量，<strong>“最近一个月是否有过购买”</strong>这个特征是预测能力最低的变量。如果我们需要在这四个变量中去挑选变量，就可以根据<strong>IV</strong>从高到低去挑选了。在应用实践中，<strong>IV</strong>信息量大小与指标判别力有一个经验的规则：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583809611915.png" alt></p><h2 id="4-关于IV和WOE的进一步思考"><a href="#4-关于IV和WOE的进一步思考" class="headerlink" title="4. 关于IV和WOE的进一步思考"></a>4. 关于IV和WOE的进一步思考</h2><h3 id="4-1-为什么用IV而不是直接用WOE"><a href="#4-1-为什么用IV而不是直接用WOE" class="headerlink" title="4.1 为什么用IV而不是直接用WOE"></a>4.1 为什么用IV而不是直接用WOE</h3><p>从上面的内容来看，变量各分组的<strong>WOE</strong>和<strong>IV</strong>都隐含着这个分组对目标变量的预测能力这样的意义。那我们为什么不直接用<strong>WOE</strong>相加或者绝对值相加作为衡量一个变量整体预测能力的指标呢？</p><p>并且，从计算公式来看，对于变量的一个分组，<strong>IV</strong>是<strong>WOE</strong>乘以这个分组响应占比和未响应占比的差。而一个变量的IV等于各分组<strong>IV</strong>的和。如果愿意，我们同样也能用<strong>WOE</strong>构造出一个这样的一个和出来，我们只需要把变量各个分组的<strong>WOE</strong>和取绝对值再相加，即（取绝对值是因为<strong>WOE</strong>可正可负，如果不取绝对值，则会把变量的区分度通过正负抵消的方式抵消掉）：</p><script type="math/tex; mode=display">WOE = \sum_i^n |WOE_i|, 其中，n为变量分组个数</script><p>那么我们为什么不直接用这个<strong>WOE</strong>绝对值的加和来衡量一个变量整体预测能力的好坏，而是要用<strong>WOE</strong>处理后的<strong>IV</strong>呢。</p><p>我们这里给出两个原因。<strong>IV</strong>和<strong>WOE</strong>的差别在于<strong>IV</strong>在<strong>WOE</strong>基础上乘以的那个权重系数$(P_{y_i} - P_{n_i})$，我们暂且用$P_{yn}$来代表这个值，也即$P_{yn} = (P_{y_i} - P_{n_i})$。</p><blockquote><ul><li>第一个原因，当我们衡量一个变量的预测能力时，我们所使用的指标值不应该是负数，否则，说一个变量的预测能力的指标是$-2.3$，听起来很别扭。从这个角度讲，乘以$P_{yn}$这个系数，保证了变量每个分组的结果都是非负数，你可以验证一下，当一个分组的<strong>WOE</strong>是正数时，$P_{yn}$也是正数，当一个分组的<strong>WOE</strong>是负数时，$P_{yn}$也是负数，而当一个分组的$WOE=0$时，$P_{yn}$也是$0$。当然，上面的原因不是最主要的，因为其实我们上面提到的$WOE = \sum_i^n |WOE_i|$这个指标也可以完全避免负数的出现；</li><li>更主要的原因，也就是第二个原因是，乘以$P_{yn}$后，体现出了变量当前分组中个体的数量占整体个体数量的比例，对变量预测能力的影响。</li></ul></blockquote><p>怎么理解第二个原因呢？我们还是举个例子。假设我们上面所说的营销响应模型中，还有一个变量$A$，其取值只有两个：$0,1$，数据如下：<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583811147102.png" alt><br>我们从上表可以看出，当变量$A$取值$1$时，其响应比例达到了$90\%$，非常的高，但是我们能否说变量$A$的预测能力非常强呢？不能。为什么呢？<strong>原因就在于，$A$取$1$时，响应比例虽然很高，但这个分组的客户数太少了，占的比例太低了。</strong>虽然，如果一个客户在$A$这个变量上取$1$，那他有$90\%$的响应可能性，但是一个客户变量$A$取$1$的可能性本身就非常的低。所以，对于样本整体来说，变量的预测能力并没有那么强。我们分别看一下变量各分组和整体的$WOE，IV$。<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583811340703.png" alt><br>从这个表我们可以看到，变量取$1$时，响应比达到$90\%$，对应的<strong>WOE</strong>很高，但对应的<strong>IV</strong>却很低，原因就在于<strong>IV</strong>在<strong>WOE</strong>的前面乘以了一个系数，而这个系数很好的考虑了这个分组中样本占整体样本的比例，比例越低，这个分组对变量整体预测能力的贡献越低。相反，如果直接用<strong>WOE</strong>的绝对值加和，会得到一个很高的指标，这是不合理的。</p><h3 id="4-2-IV的极端情况以及处理方式"><a href="#4-2-IV的极端情况以及处理方式" class="headerlink" title="4.2 IV的极端情况以及处理方式"></a>4.2 IV的极端情况以及处理方式</h3><p><strong>IV</strong>依赖<strong>WOE</strong>，并且<strong>IV</strong>是一个很好的衡量自变量对目标变量影响程度的指标。但是，使用过程中应该注意一个问题：变量的任何分组中，不应该出现响应数$=0$或非响应数$=0$的情况。</p><p>原因很简单，当变量一个分组中，响应数$=0$时，<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583811451181.png" alt><br>此时对应的$IV_i$为$-\infty$。<br>而当变量一个分组中，没有响应的数量$= 0$时，<br><img src= "/img/loading.gif" data-src="http://qe0z9wdl5.bkt.clouddn.com/1583811514031.png" alt><br>此时对应的$IV_i$为$+\infty$。</p><p>$IV_i$无论等于负无穷还是正无穷，都是没有意义的。<br>由上述问题我们可以看到，使用<strong>IV</strong>其实有一个缺点，就是不能自动处理变量的分组中出现响应比例为<strong>0</strong>或$100\%$的情况。那么，遇到响应比例为$0$或者$100\%$的情况，我们应该怎么做呢？建议如下：</p><blockquote><ul><li>如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件；</li><li>重新对变量进行离散化或分组，使每个分组的响应比例都不为$0$且不为$100\%$，尤其是当一个分组个体数很小时（比如小于$100$个），强烈建议这样做，因为本身把一个分组个体数弄得很小就不是太合理。</li><li>如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为$0$，可以人工调整响应数为$1$，如果非响应数原本为$0$，可以人工调整非响应数为$1$.</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;内容说明&lt;/strong&gt;：评分卡中各个特征的筛选和模型性能评估的指标.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="评分卡" scheme="http://yoursite.com/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"/>
    
      <category term="评估指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode——674/830题解</title>
    <link href="http://yoursite.com/2020/07/26/leetcode-674-830/"/>
    <id>http://yoursite.com/2020/07/26/leetcode-674-830/</id>
    <published>2020-07-26T14:19:40.000Z</published>
    <updated>2020-07-27T14:48:22.652Z</updated>
    
    <content type="html"><![CDATA[<p><strong>内容说明</strong>：LeetCode的题目674和830的分析过程.<br><a id="more"></a></p><h2 id="1-题目描述"><a href="#1-题目描述" class="headerlink" title="1. 题目描述"></a>1. 题目描述</h2><blockquote><p><strong>674. 最长连续递增序列</strong><br><strong>题目：</strong>给定一个未经排序的整数数组，找到最长且连续的的递增序列，并返回该序列的长度。<br><strong>示例1：</strong><br>输入: [1,3,5,4,7]<br>输出: 3<br>解释: 最长连续递增序列是 [1,3,5], 长度为3。尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为5和7在原数组里被4隔开。<br><strong>示例2：</strong><br>输入: [2,2,2,2,2]<br>输出: 1<br>解释: 最长连续递增序列是 [2], 长度为1。</p><hr><p><strong>830. 较大分组的位置</strong><br><strong>题目：</strong>在一个由小写字母构成的字符串 S 中，包含由一些连续的相同字符所构成的分组。例如，在字符串 S = “abbxxxxzyy” 中，就含有 “a”, “bb”, “xxxx”, “z” 和 “yy” 这样的一些分组。我们称所有包含大于或等于三个连续字符的分组为较大分组。找到每一个较大分组的起始和终止位置。最终结果按照字典顺序输出。<br><strong>示例 1：</strong><br>输入: “abbxxxxzzy”<br>输出: [[3,6]]<br>解释: “xxxx” 是一个起始于 3 且终止于 6 的较大分组。<br><strong>示例2：</strong><br>输入: “abc”<br>输出: []<br>解释: “a”,”b” 和 “c” 均不是符合要求的较大分组。<br><strong>示例3：</strong><br>输入: “abcdddeeeeaabbbcd”<br>输出: [[3,5],[6,9],[12,14]]</p></blockquote><h2 id="2-提交代码"><a href="#2-提交代码" class="headerlink" title="2. 提交代码"></a>2. 提交代码</h2><blockquote><p>题目<strong>674</strong>的代码：</p><pre><code>    class Solution:        def findLengthOfLCIS(self, nums):                result = index = 0                for i in range(len(nums)):                    if nums[i-1] &gt;= nums[i]:                        index = i                    result = max(result, i-index+1)                return result</code></pre><p>题目<strong>830</strong>的代码：</p><pre><code>    class Solution:        def largeGroupPositions(self, S: str):            # 哨兵法则            if not S:                return 0            S = S + &#39;0&#39;            result = index = 0            l = len(S)            count = []            for i in range(l):                if S[i-1] != S[i]:                    result = i - index                    if result &gt;= 3:                        count.append([index, i-1])                    index = i            return count</code></pre></blockquote><h2 id="3-题目分析"><a href="#3-题目分析" class="headerlink" title="3. 题目分析"></a>3. 题目分析</h2><p><strong>674_最长连续递增序列</strong>：<br>题解：对于题目来说，根据题意，在数组中有一段序列是连续增长且最长的，那么首先想到的就是使用双指针的方式，一个放在头部，一个放在尾部，让两个指针一前一后的进行前进，然后每次循环的时候都用尾指针减去头指针，看看现在的长度是否最大。在这个双指针的想法中，有几点想法是很重要的：</p><blockquote><ol><li>整个递增序列的前后出现了断层，也即：<strong>nums[i-1] &gt;= nums[i]</strong>，此时有序的序列断开，这个条件在循环中不断的判断递增序列是否到头了；</li><li>由于整个数组可能存在多个递增有序序列，因此需要判断出哪个递增序列最长，最笨的办法就是将有序序列的长度都保存下来，然后后续再开一个循环进行比较，但是这样明显就麻烦了，其实可以一个循环解决完有序序列判断和长度比较的工作，也即需要使用到一个变量来每次循环的时候比较一下当前这个变量保存的长度和新的有序序列的长度谁更长；</li></ol></blockquote><p>在实际操作过程中，其实可以不用费劲的去使用双指针，而是使用哨兵原则进行处理，也就是使用一个指针即可，因为在进行<strong>for</strong>循环的时候，每次循环的下标天然的就是另一个指针。因此利用那个新设定的指针<strong>index</strong>，最开始让新指针<strong>index</strong>指向数组第一个元素，当循环的<strong>i</strong>在不断的增加的时候，如果每次前后两个元素都满足<strong>nums[i-1] &lt; nums[i]</strong>，那么相应的<strong>index</strong>不动，此时随着<strong>i</strong>的移动，两个指标间的距离天然的就标识出了有序序列的长度，而当<strong>nums[i-1] &gt;= nums[i]</strong>时，预示着上一个有序序列结束了，此时哨兵<strong>index</strong>就立刻跟上<strong>i</strong>的脚步(<strong>index = i</strong>)，然后开启下一段有序序列的长度记录，顺便，上一段序列的长度与记录序列长度的变量<strong>result</strong>进行比较，得出大小，因此，整个题目最妙的就在于哨兵指标<strong>index</strong>的设立，使得指针变量的设立不必太多，一个就好；</p><hr><p><strong>830. 较大分组的位置</strong>：<br>这个题和上面的题同样的异曲同工，同样适用哨兵原则进行处理，不需要再设立两个指针才能指示长度了，因此更细的题解就不谈了；但是有一点需要注意的是，这两个题能够适用哨兵原则的关键点在于：</p><blockquote><ol><li>需要探寻整个数组中的某个序列：这个序列可能是递增的最长序列；可能是连续的最大序列；不管在进行循环的时候条件怎么不同，对于需要寻求的那个序列前后都是有明显的断层的，而且这个断层就是循环里那个关键的条件，能够用来判断是否符合题意的那个序列的条件的断层条件；</li><li>可以使用双指针进行相关的那个子序列的求解；<br>对于满足上面这两个条件的这一类题就可以使用<strong>哨兵原则</strong>进行求解了，这个就是这一类题的关键所在。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;内容说明&lt;/strong&gt;：LeetCode的题目674和830的分析过程.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="题解" scheme="http://yoursite.com/categories/%E9%A2%98%E8%A7%A3/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="LeetCode" scheme="http://yoursite.com/tags/LeetCode/"/>
    
      <category term="数组" scheme="http://yoursite.com/tags/%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>数字视频编码技术原理</title>
    <link href="http://yoursite.com/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/</id>
    <published>2020-07-25T02:37:45.000Z</published>
    <updated>2020-07-25T13:46:46.734Z</updated>
    
    <content type="html"><![CDATA[<p><strong>内容说明</strong>：关于《数字视频编码技术》相关书籍的知识点记录.<br><a id="more"></a></p><h2 id="1-第1章-概论"><a href="#1-第1章-概论" class="headerlink" title="1. 第1章 概论"></a>1. 第1章 概论</h2><h3 id="1-0-前言"><a href="#1-0-前言" class="headerlink" title="1.0 前言"></a>1.0 前言</h3><blockquote><ol><li>一帧图像：视频是按一定时间间隔获取的图像序列，序列中的一幅图像也被成为<strong>一帧图像</strong>；</li><li>数字化：为了处理、传输和存储方便，需要将图像在空间上的连续模拟量进行采样和量化，将其变为数字量的过程；</li><li>模拟视频信号在数字化过程中的过采样是导致数字视频中存在大量的数据冗余的根本原因。</li><li>在实际使用中，由于很难测定图像信号的频率，人们通常简单地将图像按照一个约定俗成的密度进行采样，例如每行$1920$个采样点，每幅图像$1080$行等。每个采样点得到的数据成为<strong>像素</strong>，一幅图像采样的行数和列数称为图像的<strong>分辨率</strong>。</li></ol></blockquote><h3 id="1-1-视觉感知"><a href="#1-1-视觉感知" class="headerlink" title="1.1 视觉感知"></a>1.1 视觉感知</h3><blockquote><ol><li>光源的能量使用<strong>光通量</strong>度量，<strong>光通量</strong>是<strong>每单位时间到达、离开或通过曲面的光能数量。</strong>光通量的单位是<strong>流明</strong>，记作：$lm$，指的是每瓦激发光源的辐射光能；</li><li>人的视觉系统<strong>human visual system(HVS)</strong>对光的感知具有融合处理能力，可以将三种基色视锥细胞的输出融合为<strong>亮度(Y)</strong>和<strong>色度/色饱和度(U、V)</strong>；</li></ol></blockquote><h3 id="1-2-数字视频"><a href="#1-2-数字视频" class="headerlink" title="1.2 数字视频"></a>1.2 数字视频</h3><blockquote><ol><li>一个数字视频系统包含采集（照相机、摄像机等）、处理（压缩、传输及解码等）和显示等几个重要模块。</li><li><strong>图像传感器（sensor）</strong>：<strong>电耦合器（CCD）</strong>和<strong>互补性氧化金属半导体（CMOS）</strong> ，其中<strong>CCD</strong>由高感光度的半导体材料构成，可以将光信号转化为电信号，所有感光单元产生的信号组合起来构成一副完整的数字图像，<strong>CCD</strong>的输出是模拟信号，连续的模拟信号通过模数转换器（A/D）转换成数字信号，<strong>CCD</strong>在扫描所有数据后将信号放大，而<strong>CMOS</strong>每扫描一个像素即刻对信号进行放大，因此用很少的能量消耗就可以进行快速扫描，<strong>CCD</strong>成像质量优于<strong>CMOS</strong>，但是成本更高；</li><li>二维视频图像的视觉质量受到单位面积采样点多少(解析度)的影响，对同样面积的视频场景，如果用高解析度的图像来表示会有更清晰的效果，相反低解析度的图像会导致模糊的效果；</li><li>通过在不同时间点上采样二维的视频场景图像，可以得到视频的多个时间采样点，数字视频序列就是由一段时间间隔内的空间采样点组成的；</li></ol></blockquote><h4 id="1-2-2-色彩空间"><a href="#1-2-2-色彩空间" class="headerlink" title="1.2.2 色彩空间"></a>1.2.2 色彩空间</h4><blockquote><ol><li>常用的色彩空间表示方法由<strong>RGB</strong>和<strong>YUV</strong>等。用<strong>RGB</strong>色彩空间表示视频图像时，一个像素需要用三个样值(即<strong>R</strong>、<strong>G</strong>、<strong>B</strong>三个色度值)表示，若每种色度成分用<strong>8 bit</strong>表示，那么彩色图像的一个像素需要<strong>24 bit</strong>表示；</li><li>人类的视觉系统对亮度的感知比对色度的感知更加敏感，因此将色彩空间分解为亮度(<strong>Y</strong>)和色度(<strong>U、V</strong>)两个基本成分，根据对<strong>Y、U、V</strong>三个分量的采样比率不同，数字视频图像可分为$4:2:0、4:2:2、4:4:4$；<br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595405121362.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595405109994.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595405135108.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595405149289.png" alt="Alt text"></li><li><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595405297957.png" alt="Alt text"></li></ol></blockquote><h4 id="1-2-3-视频格式"><a href="#1-2-3-视频格式" class="headerlink" title="1.2.3 视频格式"></a>1.2.3 视频格式</h4><blockquote><ol><li>一帧图像的像素点按行排列可以分为偶数行和奇数行，如果偶数行和奇数行的像素是在相同时间点采样得到的，则称该帧为<strong>逐行帧</strong>；如果偶数行和奇数行的像素在不同的时间点采样得到的，则该帧成为<strong>隔行帧</strong>，<strong>隔行帧</strong>中所有偶数行构成该图像的订场，所有奇数行构成该图像的低场；</li><li>视频在获取或者显示的时候，每秒钟按逐行扫描处理的图像数成为帧率，每秒钟按隔行扫描处理的图像数称为场率，例如每秒25帧或者50场等；</li></ol></blockquote><h3 id="1-3-视频数据冗余"><a href="#1-3-视频数据冗余" class="headerlink" title="1.3 视频数据冗余"></a>1.3 视频数据冗余</h3><blockquote><ol><li>从数字信号的统计特征方面，一般将这些数据冗余分成<strong>空间冗余、时间冗余和信息熵冗余</strong>三大类；<br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595406584850.png" alt="Alt text"></li><li></li></ol></blockquote><h2 id="2-第2章-视频编码基础"><a href="#2-第2章-视频编码基础" class="headerlink" title="2. 第2章 视频编码基础"></a>2. 第2章 视频编码基础</h2><h3 id="2-2-香农编码定理"><a href="#2-2-香农编码定理" class="headerlink" title="2.2 香农编码定理"></a>2.2 香农编码定理</h3><ol><li><strong>离散无记忆(memoryless)信源：</strong>离散信源的一种类型. 若表示信源输出消息的随机变量序列是一个<strong>相互独立同分布的随机变量序列</strong>，在此情况下，信源先后发出的一个个符号是统计独立同分布的，即对任意正整数$N$，其$N$维联合概率分布满足：<script type="math/tex; mode=display">P(X_1, X2, ..., X_N) = \prod_{i=i}^N P(X_i)</script>换句话说，随机变量序列中的任意$N$维随机变量的联合概率分布可用随机变量中单个随机变量的概率分布的乘积来表示。这种信源称为<strong>离散无记忆信源</strong>。离散无记忆信源可用它输出的一个随机变量$X$来表示。离散无记忆信源是最简单的离散信源，可以用完备的离散型概率空间来描述，其主要特点是离散和无记忆。离散指的是信源可能输出的消息的种类是有限的或者是可数的。消息的样本空间R是一个离散集合。由于信源的每一次输出都是按照消息发生的概率输出R中的一种消息，因此信源输出的消息可以用离散随机变量X表示。无记忆是指不同的信源输出消息之间相互独立。（<a href="https://baike.baidu.com/item/%E7%A6%BB%E6%95%A3%E6%97%A0%E8%AE%B0%E5%BF%86%E4%BF%A1%E6%BA%90/19141006?fr=aladdin" target="_blank" rel="noopener">百度百科——离散无记忆信源</a>）</li></ol><hr><ol><li>目前主流的视频编码器采用的技术主要有<strong>预测、变换、量化、熵编码和环路滤波</strong>，这些技术在编码器中的基本次序关系如下图。主流的编码方法都是将图像划分成块进行编码，其中第二代标准都是划分成$16 \times 16$的宏块，第三代标准引入了更大块的划分，比如最大可到$64 \times 64$块的编码单元，以编码单元为单位进行编码。将每帧图像进行划分后，按照从上之下，从左至右的顺序对每个划分进行处理。<br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595473825907.png" alt="Alt text"></li></ol><hr><ol><li><a href="https://zhuanlan.zhihu.com/p/75804693" target="_blank" rel="noopener">知乎专栏—码率是什么？比特率是干嘛的？帧速率是啥？分辨率又是什么？</a></li><li><a href="https://zhuanlan.zhihu.com/p/27037506" target="_blank" rel="noopener">知乎专栏—视频码率；</a></li><li><a href="https://www.zhihu.com/question/27460676/answer/36736875" target="_blank" rel="noopener">知乎—视频音频比特率（码率）与采样率有什么联系？</a></li><li><a href="http://tv.zol.com.cn/621/6216268.html" target="_blank" rel="noopener">网站视频不清晰?浅谈不可忽视的比特成本</a></li><li><a href="https://blog.csdn.net/tkp2014/article/details/51111303?locationNum=1&amp;fps=1" target="_blank" rel="noopener">什么是码率？</a></li><li><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595475863722.png" alt="Alt text"></li><li><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595475895327.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595475907316.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595475942755.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595475950952.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595475987968.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476029260.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476020931.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476043686.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476080072.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476091251.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476108553.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595476117468.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595478722039.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595478748680.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/25/%E6%95%B0%E5%AD%97%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/1595478780251.png" alt="Alt text"><blockquote><ol><li><a href="https://www.jianshu.com/p/028196b8ca14" target="_blank" rel="noopener">码率，分辨率，帧率 …</a></li><li><a href="https://www.cnblogs.com/yongdaimi/p/10651537.html" target="_blank" rel="noopener">音视频编解码: 比特率 码率</a></li><li><a href="https://www.cnblogs.com/linuxAndMcu/p/12113242.html" target="_blank" rel="noopener">音视频基础知识—-音频编码格式（转）</a></li><li><a href="https://juejin.im/post/5cf07dfdf265da1b8466ca8c" target="_blank" rel="noopener">视频的基本参数及H264编解码相关概念</a></li><li><a href="https://www.codercto.com/a/87804.html" target="_blank" rel="noopener">ios平台实现视频H264硬编码及软编码(附完整demo)</a></li><li><a href="https://blog.csdn.net/weixin_34004750/article/details/91469605" target="_blank" rel="noopener">ios视频实现H264硬编码和软编码编译ffmpeg库及环境搭建(附完整demo)</a></li><li><a href="https://wenku.baidu.com/view/583082be590216fc700abb68a98271fe900eaf67.html" target="_blank" rel="noopener">离散无记忆的扩展信源.PPT</a></li><li><a href="https://wenku.baidu.com/view/4a2b33666ad97f192279168884868762caaebbf1.html" target="_blank" rel="noopener">二次扩展信源的熵2.PPT</a></li><li><a href="https://www.cnblogs.com/tid-think/p/10616789.html" target="_blank" rel="noopener">图像原始格式(YUV444 YUV422 YUV420)一探究竟</a></li><li><a href="https://blog.csdn.net/xjhhjx/article/details/80291465" target="_blank" rel="noopener">YUV图解 （YUV444, YUV422, YUV420, YV12, NV12, NV21）</a></li><li><a href="https://www.cnblogs.com/ziyi--caolu/p/8034367.html" target="_blank" rel="noopener">直播一：H.264编码基础知识详解</a></li><li><a href="https://www.cnblogs.com/imstudy/p/11887915.html" target="_blank" rel="noopener">零基础，史上最通俗视频编码技术入门（重要）</a></li><li><a href="http://www.52im.net/thread-228-1-1.html" target="_blank" rel="noopener">即时通讯音视频开发（一）：视频编解码之理论概述</a></li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;内容说明&lt;/strong&gt;：关于《数字视频编码技术》相关书籍的知识点记录.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="视频编解码" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/"/>
    
    
      <category term="codec" scheme="http://yoursite.com/tags/codec/"/>
    
  </entry>
  
  <entry>
    <title>模组与电脑连接的整体步骤</title>
    <link href="http://yoursite.com/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/"/>
    <id>http://yoursite.com/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/</id>
    <published>2020-07-21T13:35:05.000Z</published>
    <updated>2020-07-25T13:42:19.867Z</updated>
    
    <content type="html"><![CDATA[<p><strong>内容说明</strong>：记录整个模组与电脑连接的相关步骤，便于记录和后续查看.<br><a id="more"></a></p><h2 id="1-模组样式及所需线材"><a href="#1-模组样式及所需线材" class="headerlink" title="1. 模组样式及所需线材"></a>1. 模组样式及所需线材</h2><p><strong>模组样式：</strong><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1.1.jpg" alt="Alt text"><br><strong>所需线材：</strong><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1.2.jpg" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1.3.jpg" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1.4.jpg" alt="Alt text"></p><h2 id="2-模组对应线材的连接方式"><a href="#2-模组对应线材的连接方式" class="headerlink" title="2. 模组对应线材的连接方式"></a>2. 模组对应线材的连接方式</h2><ol><li>首先将电源线与模组进行连接：、<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/2.1.jpg" alt="Alt text"></li><li>然后将网线的一段连接到模组对应的接口：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/2.2.jpg" alt="Alt text"><br>相应的，网线的另一端连接到电脑的网线接口，使两个设备连接到同一个局域网中，方便进行连接，<strong>此时模组端的网线接口处闪现绿灯</strong>；<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/a.jpeg" alt="Alt text"></li><li>将一段有特殊细长方口、另一端则是<strong>USB</strong>接口的线分别连接模组和电脑对应接口:<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/b.jpeg" alt="Alt text"><br>当此线分别连接模组和电脑之后，在电脑端会自动出现相应的提示框提示连接到了相应的设备：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1.png" alt="Alt text"><br>相应的，在电脑的设备管理器中也能看到相应的设备，其中<strong>COM3</strong>是设备的名称；<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/2.png" alt="Alt text"></li></ol><ul><li>【备注】<strong>串行接口：</strong>简称串口，也称串行通信接口或串行通讯接口（通常指<strong>COM接口</strong>），是采用串行通信方式的扩展接口。串行接口 （<strong>Serial Interface</strong>）是指数据一位一位地顺序传送。其特点是通信线路简单，只要一对传输线就可以实现双向通信（可以直接利用电话线作为传输线），从而大大降低了成本，特别适用于远距离通信，但传送速度较慢。</li></ul><ol><li><p>点击提示框中的确定，然后打开<strong>XShell</strong>进行接下来的连接操作。在<strong>XShell</strong>中新建会话，设置相应的会话信息，点击<strong>SERIAL</strong>(串口设备)，填写会话名称，名称可随意设定，协议栏选择<strong>SERIAL</strong>：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/3.png" alt="Alt text"></p></li><li><p>然后再次点击左边<strong>SERIAL</strong>，出现下图右边的情况，其中<strong>Port</strong>选择现在连接的模组设备的名称，也即<strong>COM3</strong>；<strong>Baud Rate</strong>设定为<strong>115200</strong>，然后剩下的选项不动，点击下方的连接按钮：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/4.png" alt="Alt text"></p></li></ol><ol><li><p>连接成功的话可以看到下面几幅图的样子，其中模组内部其实是一个<strong>Linux</strong>系统，因此连接成功后会出现让你<strong>login</strong>的语句，其中登陆名为<strong>root</strong>，无登陆密码：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/5.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/6.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/7.png" alt="Alt text"></p></li><li><p>登陆成功之后，按照<strong>《Goke 固件组 EVB基础操作中》</strong>的第10页PPT的两句命令进行执行：</p><pre><code> init.sh adidemo -a</code></pre></li><li>执行完毕之后查看电脑的<strong>ip地址</strong>与模组的<strong>ip地址</strong>：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/9.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/8.png" alt="Alt text"><br>可以发现，两者的<strong>ip地址</strong>并不相同，可以通过：<pre><code> ifconfig eth0 169.254.28.10</code></pre>将两者<strong>ip地址</strong>统一到一起；</li></ol><ul><li>【备注】<strong>eth0代表的是第一块物理网卡，更多的解释请参照：<a href="https://blog.csdn.net/pzqingchong/article/details/75675618" target="_blank" rel="noopener">CSDN-eth0 eth0:1 eth0.1 的区别</a></strong></li></ul><ol><li><p>然后根据<strong>《Goke 固件组 EVB基础操作中》</strong>的第11页PPT中网络查看的内容，打开软件<strong>VLC</strong>，点击左上角的<strong>媒体——打开网络串流</strong>，然后在框中输入：</p><pre><code> rtsp://169.254.28.10/stream0</code></pre><p>其中169.254.28.10即是我们为模组设定的IP地址。<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595327517526.png" alt="Alt text"><br>点击播放按钮，就能看到模组的摄像头模块中拍摄到的实际画面了：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595327636347.png" alt="Alt text"></p></li><li><p>最后关闭模组的时候直接拔出相应的插头即可。</p></li></ol><h2 id="3-注意事项"><a href="#3-注意事项" class="headerlink" title="3. 注意事项"></a>3. 注意事项</h2><ol><li>需要测试模组上的摄像头模块是否正常的时候需要在登陆模组的<strong>Linux系统</strong>之后，输入下面命令：<pre><code> init.sh adidemo -a</code></pre>进入到<strong>adi@goke#</strong>用户才能测试摄像头；<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595382610141.png" alt="Alt text"></li><li>进行<strong>XML配置</strong>的时候，首先需要退出<strong>adi@goke#</strong>用户到<strong>root</strong>目录下：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595382673519.png" alt="Alt text"><br>然后进入<strong>/usr/local/bin</strong>目录中，使用<strong>vi video.xml</strong>进行<strong>xml</strong>文件的查看和修改：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595382872449.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595382906306.png" alt="Alt text"><blockquote><p><strong>需要注意的是</strong>：<strong>Stream下的</strong>$<type>3</type>$中的<strong>3</strong>代表的是<strong>H.265</strong>，<strong>2</strong>代表的是<strong>JPEG</strong>，<strong>1</strong>代表的是<strong>H.264</strong>，<strong>0</strong>代表开关，需要注意的是<strong>Stream0/1/2/3</strong>这四个流可以同时都打开，相应的在<strong>VLC</strong>中展示的时候就可以出现多个窗口了；</p></blockquote></li></ol><h2 id="4-video-xml文件代码分析"><a href="#4-video-xml文件代码分析" class="headerlink" title="4. video.xml文件代码分析"></a>4. video.xml文件代码分析</h2><ol><li><p><strong>vin</strong>区域的代码：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595385044147.png" alt="Alt text"><br>进入到<strong>video.xml</strong>文件中，<strong>vin</strong>包含的是摄像头固件的相应信息，其中：</p><p> <vi_width_a>1920</vi_width_a>代表摄像头的分辨率宽度为1920；<br> <vi_height_a>1080</vi_height_a>代表摄像头的分辨率高度为1080；<br> <vi_framerate_a>25</vi_framerate_a>代表摄像头的帧率FPS为25；<br>需要注意的是，上述的摄像头的参数是有上限和下限，不能过高或者过低，这是硬件的限制，如果设定的值过高或者过低，将造成摄像头无法正常工作，相应的，无法通过命令行进入到<strong>adi@goke#</strong>用户，也就无法拍摄到图像了。</p></li><li><p><strong>Stream</strong>区域的代码：<br><img src= "/img/loading.gif" data-src="/2020/07/21/1-%E6%A8%A1%E7%BB%84%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91/1595385764935.png" alt="Alt text"><br>模组仅有一个输入模块，也即摄像头，但是输出的流<strong>Stream</strong>却有多个，在上图中即可通过：</p><p> <StreamSetting num="4"><br>可以看出输出的流有<strong>4</strong>个。其中在<strong>Stream0</strong>中：</StreamSetting></p></li></ol><pre><code>&lt;type&gt;3&lt;/type&gt;代表图像的编码格式为3-H.265,2-JPEG,1-H.264,0-代表开关，需要注意的是Stream0/1/2/3这四个流可以同时都打开，相应的在VLC中展示的时候就可以出现多个窗口了;&lt;width&gt;1280&lt;/width&gt;代表输出流Stream0的图像分辨率宽度为1920；&lt;height&gt;720&lt;/height&gt;代表输出流Stream0的图像分辨率高度为1080；&lt;fps&gt;25&lt;/fps&gt;代表输出流Stream0的帧率FPS为25；</code></pre><blockquote><p><strong>注意：</strong>输出流的一些参数受限于摄像头的对应参数，比方说分辨率或者帧率，输出流的分辨率只能小于等于$1920\times1080$，因为这是摄像头的硬件极限，过大或者过小都会造成摄像头无法正常工作，相应的参数如帧率也是同样的道理。这里大概测试了一下<strong>Stream0</strong>在分辨率上的下限为$720\times480$，帧率的下限为$10 FPS$.</p></blockquote><h2 id="5-一些其他需要补充的知识点"><a href="#5-一些其他需要补充的知识点" class="headerlink" title="5. 一些其他需要补充的知识点"></a>5. 一些其他需要补充的知识点</h2><blockquote><ol><li><a href="https://blog.csdn.net/pc9319/article/details/79621352" target="_blank" rel="noopener">码率（Bitrate）、帧率（FPS）、分辨率和清晰度的联系与区别；</a></li><li><a href="https://blog.csdn.net/leixiaohua1020/article/details/18893769" target="_blank" rel="noopener">[总结]视音频编解码技术零基础学习方法；</a></li><li><a href="https://www.cnblogs.com/xkfz007/category/322714.html" target="_blank" rel="noopener">视频编解码学习总结博客；</a></li><li><a href="https://blog.csdn.net/qq_41603966/article/details/97525277" target="_blank" rel="noopener">串口（串行接口）相关概念；</a></li><li><a href="https://www.cnblogs.com/dalyday/p/11380252.html" target="_blank" rel="noopener">win10下安装FFmpeg步骤；</a></li><li><a href="https://www.diangon.com/m417981.html" target="_blank" rel="noopener">模拟信号为什么叫模拟信号？它到底模拟了啥？</a></li><li><a href="http://www.pomeas.cn/mobile/newsview/581.html" target="_blank" rel="noopener">什么是逐行和隔行扫描？它们有什么优缺点？</a></li><li><a href="https://blog.csdn.net/xiaojun111111/article/details/70255048" target="_blank" rel="noopener">逐行与隔行扫描的区别和原理；</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;内容说明&lt;/strong&gt;：记录整个模组与电脑连接的相关步骤，便于记录和后续查看.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="视频编解码" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/"/>
    
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
      <category term="模组" scheme="http://yoursite.com/tags/%E6%A8%A1%E7%BB%84/"/>
    
      <category term="codec" scheme="http://yoursite.com/tags/codec/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2020/07/20/hello-world/"/>
    <id>http://yoursite.com/2020/07/20/hello-world/</id>
    <published>2020-07-20T12:45:34.114Z</published>
    <updated>2020-07-26T13:03:57.072Z</updated>
    
    <content type="html"><![CDATA[<p><strong>内容说明：</strong>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;内容说明：&lt;/strong&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>TensorFlow分布式与.pd文件相关网页</title>
    <link href="http://yoursite.com/2019/06/19/pb-website/"/>
    <id>http://yoursite.com/2019/06/19/pb-website/</id>
    <published>2019-06-19T05:09:30.000Z</published>
    <updated>2020-07-25T13:56:03.764Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>如题目所述.</strong><br><a id="more"></a></p></blockquote><h2 id="1-分布式"><a href="#1-分布式" class="headerlink" title="1. 分布式"></a>1. 分布式</h2><blockquote><ol><li><a href="https://blog.csdn.net/tiangcs/article/details/85952007" target="_blank" rel="noopener">TensorFlow分布式采坑记</a></li><li><a href="https://blog.csdn.net/luodongri/article/details/52596780" target="_blank" rel="noopener">白话tensorflow分布式部署和开发</a></li><li><a href="https://blog.csdn.net/xs11222211/article/details/82931120" target="_blank" rel="noopener">深度学习分布式训练实战（一）</a></li><li><a href="https://blog.csdn.net/xs11222211/article/details/82933764" target="_blank" rel="noopener">深度学习分布式训练实战（二）——TF</a></li><li><a href="https://blog.csdn.net/zwqjoy/article/details/89552866" target="_blank" rel="noopener">[深度学习] 一文理解分布式Tensorflow原理</a></li><li><a href="https://www.cnblogs.com/hellcat/p/9194115.html" target="_blank" rel="noopener">『TensorFlow』分布式训练_其三_多机分布式</a></li><li><a href="https://blog.csdn.net/u011026329/article/details/79190537" target="_blank" rel="noopener">TensorFlow 分布式（Distributed TensorFlow）</a></li><li><a href="https://www.jianshu.com/p/bf17ac9e6357" target="_blank" rel="noopener">TensorFlow分布式部署</a></li><li><a href="https://mc.ai/tensorflow%E4%B8%8A%E6%89%8B4-%E5%88%9D%E6%8E%A2%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" target="_blank" rel="noopener">Tensorflow上手4: 初探分布式训练</a></li><li><a href="https://zhuanlan.zhihu.com/p/64604071" target="_blank" rel="noopener">你必须知道的六个深度炼丹好习惯</a></li><li><a href="https://blog.csdn.net/hjimce/article/details/61197190" target="_blank" rel="noopener">深度学习（五十五）tensorflow分布式训练</a>\</li><li><a href="https://www.jianshu.com/p/c189450a017d?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">学习笔记TF061:分布式TensorFlow，分布式原理、最佳实践</a></li><li><a href="http://xudongyang.coding.me/distributed-tensorflow/" target="_blank" rel="noopener">一文说清楚Tensorflow分布式训练必备知识</a></li></ol></blockquote><h2 id="2-pb文件"><a href="#2-pb文件" class="headerlink" title="2. pb文件"></a>2. pb文件</h2><blockquote><ol><li><a href="https://blog.csdn.net/CoderPai/article/details/68952258" target="_blank" rel="noopener">TensorFlow学习系列（三）：保存/恢复和混合多个模型</a></li><li><a href="https://blog.csdn.net/mogoweb/article/details/83064819" target="_blank" rel="noopener">如何合并两个TensorFlow模型</a></li><li><a href="https://www.jianshu.com/p/0f9f2bb962f4" target="_blank" rel="noopener">Tensorflow数据读取机制</a></li><li><a href="https://blog.csdn.net/qq26983255/article/details/85797707" target="_blank" rel="noopener">TensorFlow：将多个pb文件模型合并成一个</a></li><li><a href="https://blog.csdn.net/hustwayne/article/details/89482873" target="_blank" rel="noopener">Tensorflow： .Pb模型合并（子图合并成大图）</a></li><li><a href="https://www.huachao1001.cn/753d79ea5726181c01a2b3a983efd3cf.html" target="_blank" rel="noopener">从Tensorflow模型文件中解析并显示网络结构图（pb模型篇）</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;如题目所述.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TFServing" scheme="http://yoursite.com/tags/TFServing/"/>
    
  </entry>
  
  <entry>
    <title>Python代码加密</title>
    <link href="http://yoursite.com/2019/06/15/password-python/"/>
    <id>http://yoursite.com/2019/06/15/password-python/</id>
    <published>2019-06-15T06:32:36.000Z</published>
    <updated>2020-07-26T13:09:23.120Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>介绍如何以一种安全的方式加密python代码.</strong><br><a id="more"></a></p><p>我们以<strong>MNIST数据集预测手写体数字为例</strong>，实现代码加密。这里生成的是<strong>pyd</strong>文件，这个是开发<strong>cython</strong>生成的二进制脚本，可以直接当库导入，安全性来说，<strong>pyd</strong>是二进制文件，被反编译的难度相当高，其实<strong>pyd</strong>就是<strong>dll</strong>改个名字，只是必须实现特定的导出符号，按照约定的规则工作。不过即使是<strong>pyd</strong>，也不保证完全不泄漏。跟踪汇编代码就可以找到工作过程，只是这样会比反向源码成本大上超级多。<br>原始项目如下：<br><img src= "/img/loading.gif" data-src="/2019/06/15/password-python/1560607700111.png" alt="Alt text"><br>其中：</p><ul><li><strong>MNIST_data</strong>文件夹存放<strong>MNIST数据集</strong>;</li><li><strong>MNIST_model</strong>文件夹用于存放保存模型参数的文件;</li><li><strong>mnist_inference.py</strong>文件保存的是卷积网络的架构;</li><li><strong>mnist_train.py</strong>文件保存整个模型训练的代码.</li></ul></blockquote><hr><blockquote><p>整体过程如下：</p><ol><li>首先在在项目中创建<strong>setup.py</strong>文件，内部代码如下：<br>```<br>from distutils.core import setup<br>from Cython.Build import cythonize</li></ol><p>setup(name=’Hello World app’,  ext_modules=cythonize([‘mnist_train.py’, ‘mnist_inference.py’]))<br>```</p><ol><li>编译为 <strong>.c</strong>，再进一步编译为 <strong>.so</strong> 或 <strong>.pyd</strong>:<br><code>python setup.py build_ext --inplace</code></li><li>上一步完成之后会在项目中生成如下：<br><img src= "/img/loading.gif" data-src="/2019/06/15/password-python/1560608169867.png" alt="Alt text"><br>其中，<strong>build</strong>文件夹是生成过程使用到的临时文件。两个<strong>.c</strong>文件也是临时文件，可以打开看看传说中的<strong>D</strong>语言代码()。两个<strong>.pyd</strong>文件是我们所需的文件。</li><li>然后在项目中创建<strong>main.py</strong>文件，用于进入整个项目的入口：<br><img src= "/img/loading.gif" data-src="/2019/06/15/password-python/1560608401445.png" alt="Alt text"></li><li>最后对于临时文件，可以将其删除掉了(不删除也是没有问题的)：<br><img src= "/img/loading.gif" data-src="/2019/06/15/password-python/1560608494056.png" alt="Alt text"><br>最终仅留下一个可以看到源码的<strong>main.py</strong>文件，其他加密文件都是不可见的。</li><li>在运行的时候，不仅可以在<strong>IDE</strong>中直接运行，还可以在<strong>CMD</strong>下运行，只需要将需要的数据文件夹保留即可。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;介绍如何以一种安全的方式加密python代码.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python代码加密" scheme="http://yoursite.com/tags/Python%E4%BB%A3%E7%A0%81%E5%8A%A0%E5%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Inception Networks 相关网页</title>
    <link href="http://yoursite.com/2019/06/10/Inception-Network-pages2/"/>
    <id>http://yoursite.com/2019/06/10/Inception-Network-pages2/</id>
    <published>2019-06-10T06:48:12.000Z</published>
    <updated>2020-07-26T13:07:35.563Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>关于Inception Network架构详解的网页.</strong><br><a id="more"></a></p><ol><li><a href="https://blog.csdn.net/guyuealian/article/details/81560537" target="_blank" rel="noopener">使用自己的数据集训练GoogLenet InceptionNet V1 V2 V3模型（TensorFlow）</a></li><li><a href="https://blog.csdn.net/lujiandong1/article/details/53385092" target="_blank" rel="noopener">tensorflow将训练好的模型freeze,即将权重固化到图里面,并使用该模型进行预测</a></li><li><a href="https://blog.csdn.net/u013841196/article/details/80659132" target="_blank" rel="noopener">卷积神经网络的网络结构——Inception V3</a></li><li><a href="https://blog.csdn.net/loveliuzz/article/details/79135583" target="_blank" rel="noopener">深度学习卷积神经网络——经典网络GoogLeNet(Inception V3)网络的搭建与实现</a></li><li><a href="https://blog.csdn.net/sunbaigui/article/details/50807418" target="_blank" rel="noopener">深度学习之图像分类模型inception v2、inception v3解读</a></li><li><a href="https://blog.csdn.net/u010402786/article/details/52433324" target="_blank" rel="noopener">深入浅出——网络模型中Inception的作用与结构全解析</a></li><li><a href="https://blog.csdn.net/zziahgf/article/details/82801098" target="_blank" rel="noopener">网络结构之 Inception V3</a></li><li><a href="https://www.aiuai.cn/aifarm463.html" target="_blank" rel="noopener">网络结构之 Inception V3</a></li><li><a href="https://blog.csdn.net/stesha_chen/article/details/81259857" target="_blank" rel="noopener">GoogleLeNet(Inception-V1)论文及代码解析</a></li><li><a href="https://www.jianshu.com/p/57cccc799277" target="_blank" rel="noopener">经典分类CNN模型系列其三：Inception v1</a></li><li><a href="https://www.cnblogs.com/leebxo/p/10315490.html" target="_blank" rel="noopener">Inception Module-深度解析</a></li><li><a href="https://www.jianshu.com/p/544e2354b30a" target="_blank" rel="noopener">卷积神经网络工作原理研究 - Inception V3源代码</a></li><li><a href="https://blog.csdn.net/zzc15806/article/details/83447006" target="_blank" rel="noopener">【深度学习】GoogLeNet系列解读 —— Inception v1</a></li><li><a href="https://blog.csdn.net/q199502092010/article/details/80262348" target="_blank" rel="noopener">Inception-V1到Inception-V4</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649029565&amp;idx=1&amp;sn=330e398a4007b7b24fdf5203a5bf5d91&amp;chksm=871345c0b064ccd6dd7d954c90d63f1f3b883c7d487844cbe3424bec3c9abb66625f1837edbd#rd" target="_blank" rel="noopener">【模型解读】GoogLeNet中的inception结构，你看懂了吗</a></li><li><a href="https://php.ctolib.com/article/wiki/75208" target="_blank" rel="noopener">Simple TensorFlow Serving：通用和易于使用得部署机器学习模型</a></li><li><a href="https://www.infoq.cn/article/2016/02/TensorFlow-Serving" target="_blank" rel="noopener">谷歌发布 TensorFlow Serving 开源项目：更快的将深度学习模型产品商业化</a></li><li><a href="https://blog.csdn.net/l7H9JA4/article/details/82879112" target="_blank" rel="noopener">TensorFlow Serving入门</a></li><li><a href="https://zhuanlan.zhihu.com/p/45918984" target="_blank" rel="noopener">[L1]TensorFlow模型持久化~模型保存</a></li><li><a href="https://blog.csdn.net/lujiandong1/article/details/53385092" target="_blank" rel="noopener">tensorflow将训练好的模型freeze,即将权重固化到图里面,并使用该模型进行预测</a></li><li><a href="https://blog.csdn.net/u013841196/article/details/80673688" target="_blank" rel="noopener">卷积神经网络的网络结构——Inception V4</a></li><li><a href="https://blog.csdn.net/u014114990/article/details/52583912" target="_blank" rel="noopener">深入浅出——网络模型中Inceptionv1到 v4 的作用与结构全解析</a></li><li><a href="https://www.cnblogs.com/shouhuxianjian/p/7786760.html" target="_blank" rel="noopener">Feature Extractor[Inception v4]</a></li><li><a href="https://blog.csdn.net/u011021773/article/details/80791650" target="_blank" rel="noopener">极简解释inception V1 V2 V3 V4</a></li><li><a href="https://www.aiuai.cn/aifarm464.html" target="_blank" rel="noopener">网络结构之 Inception V4</a></li><li><a href="https://www.cnblogs.com/sdu20112013/p/10702173.html" target="_blank" rel="noopener">TensorRT是什么</a></li><li><a href="https://www.cnblogs.com/ranjiewen/p/8987195.html" target="_blank" rel="noopener">初见-TensorRT简介&lt;转&gt;</a></li><li><a href="https://zhuanlan.zhihu.com/p/35657027" target="_blank" rel="noopener">高性能深度学习支持引擎实战——TensorRT</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;关于Inception Network架构详解的网页.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="Inception Network" scheme="http://yoursite.com/tags/Inception-Network/"/>
    
  </entry>
  
  <entry>
    <title>重要卷积神经网络结构分析</title>
    <link href="http://yoursite.com/2019/06/10/Import-CNN-Framework/"/>
    <id>http://yoursite.com/2019/06/10/Import-CNN-Framework/</id>
    <published>2019-06-10T05:13:24.000Z</published>
    <updated>2020-07-26T13:25:04.635Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>详细分析多个重要的卷积网络框架.</strong><br><a id="more"></a></p></blockquote><h2 id="1-Inception-v1-Networks"><a href="#1-Inception-v1-Networks" class="headerlink" title="1. Inception-v1 Networks"></a>1. Inception-v1 Networks</h2><h3 id="1-1-1x1卷积"><a href="#1-1-1x1卷积" class="headerlink" title="1.1 1x1卷积"></a>1.1 1x1卷积</h3><blockquote><p>$通过1 \times 1$卷积操作来压缩或者增加输入层的信道数目，从而改变(减少)计算量。<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560128241604.png" alt="Alt text"></p></blockquote><h3 id="1-2-Inception-Module"><a href="#1-2-Inception-Module" class="headerlink" title="1.2 Inception Module"></a>1.2 Inception Module</h3><blockquote><p>在构建卷积层的时候，经常会需要决定过滤器的大小究竟是$1 \times 3$、$3 \times 3$还是$5 \times 5$，或者是否需要添加池化层，而<strong>Inception</strong>网络则帮助你自动的决定如何做出选择。虽然网络架构变得很复杂，但是网络表现却非常的好。<br>以下图为例：<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560128537535.png" alt="Alt text"><br><strong>Inception</strong>模块用来代替人工去确定卷积层中的过滤类型或者是否需要创建卷积层或池化层，上图中输入层的维度为$28 \times 28 \times 192$，假设最上面使用$1 \times 1 \times 192 \times 64$卷积，那么输出为$1 \times 1 \times 64$；如果使用使用$3 \times 3 \times 192 \times 128$，那么输出为$28 \times 28 \times 128$(使用了<strong>SAME</strong>卷积)，然后我们把第二个计算出来的值堆叠到第一个值上面去；接着使用$5 \times 5$以及$MAX-POOLING$层等，以此将结果堆叠在一起。<br>依照上图可以得到：<strong>Inception</strong>模块输入为$28 \times 28 \times 192$，输出为$28 \times 28 \times 256$。<br><strong>Inception网络的基本思想是不需要人为的决定使用哪种过滤器或者是否使用池化，而是由网络自行确定这些参数，你可以为网络添加所有可能的尺寸的过滤器或者池化层，然后把这些输出连接起来，让网络自己学习它需要的参数、采用哪种过滤器组合。</strong><br>但是这里就有一个比较重要的问题了，那就是<strong>计算成本</strong>。<br>我们以上图中$5 \times 5$过滤器为例，计算一下它的计算成本是多少：<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560129417723.png" alt="Alt text"><br>计算成本为：输出的大小代表要计算的数字个数，则这里要计算$28 \times 28 \times 32$个数字，对其中每个数字都需要执行$5 \times 5 \times 192$次乘法运算。所以乘法运算的总的次数为<strong>输出值个数 x 每个输出值需要计算的乘法运算次数 = </strong>$28 \times 28 \times 32 \times 5 \times 5 \times 192 = 1.2 亿$。<br>这是非常高的计算量了，而如果加入$1 \times 1$卷积，则可以将这个计算量缩减到原来的十分之一：<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560130369650.png" alt="Alt text"><br>上图中使用$1 \times 1$卷积将输入压缩到$16$个信道，而中间这个被压缩的层有时候被称为<strong>bottleneck layer(瓶颈层)</strong>，第一层卷积的计算量为($1 \times 1$卷积)：$28 \times 28 \times 16 \times 1 \times 1 \times 192 \approx 240万$，第二层卷积的计算量为: $28 \times 28 \times 32 \times 5 \times 5 \times 16 \approx 1000万$，所以总的计算成本约为<strong>1240万</strong>，相当于上面<strong>1.2亿</strong>的十分之一。<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560131172506.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560131415340.png" alt="Alt text"></p></blockquote><h3 id="1-3-Residual-Networks"><a href="#1-3-Residual-Networks" class="headerlink" title="1.3 Residual Networks"></a>1.3 Residual Networks</h3><blockquote><p><strong>Residual block(残差块)</strong>：<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560132641217.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560132733434.png" alt="Alt text"><br>也就是说上面在深层的网络中加上的这个$a^{[l]}$，产生了一个<strong>残差块</strong>。<br>使用残差块可以训练非常深的神经网络，而<strong>ResNet</strong>就是很多的残差块堆叠在一起组成的一个深度神经网络。<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560133115635.png" alt="Alt text"><br>上图中对于普通的网络来说(作图，没有残差块的网络)，凭着经验会发现随着网络层数的加深，训练错误会先减少，然后增多，但是理论上应该是随着网络层数的加深，应该训练的越来越好才对；但是对于<strong>ResNet</strong>则不同，即便网络再深(即使训练深度达到100层的网络也不例外)，训练的表现却都很不错，<strong>这种方式能够将中间的激活传递到更深的网络中去，有助于解决梯度消失和梯度爆炸问题。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560133853526.png" alt="Alt text"><br>在上图中，假设对于一个很深的网络后面再添加两层神经网络，并使用残差块结构（这里使用的激活函数为<strong>Relu</strong>），则：</p><script type="math/tex; mode=display">a^{[l+2]} = g(z^{[l+2]} + a^{[l]})</script><script type="math/tex; mode=display">= g(w^{[l+2]}·a^{[l+1]} + b^{[l+2]} + a^{[l]})</script><script type="math/tex; mode=display">如果此时w^{[l+2]}和b^{[l+2]}都为0</script><script type="math/tex; mode=display">= g(a^{[l]})</script><script type="math/tex; mode=display">= a^{[l]} = a^{[l+2]}</script><p><strong>结果表明，残差块学习这个恒等函数并不难，这就意味着即使给神经网络增加两层，它的效率也并不逊色与更简单的神经网络。想象一下，如果添加的这两层网络还学到了一些有用信息的话，那么它比学习恒等函数的表现将更好，那么这样的网络不仅仅保持了网络的效率，而且还提升了网络的效率。</strong><br><strong>另一个需要注意的点是</strong>，假设$z^{[l+2]}$和$a^{[l]}$是具有相同的维度的，所以在<strong>ResNet</strong>中使用了很多的<strong>SAME</strong>卷积，<br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560134986732.png" alt="Alt text"></p></blockquote><h2 id="2-Inception-v3-Network"><a href="#2-Inception-v3-Network" class="headerlink" title="2. Inception-v3 Network"></a>2. Inception-v3 Network</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560147519168.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560148453948.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560148694586.png" alt="Alt text"></p><hr><p><strong>网络结构图中第一次出现的三个模块组：</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560149370767.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560149488062.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560149503984.png" alt="Alt text"></p><hr><p><strong>网络结构图中第二次出现的五个模块组：</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560149729631.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560150090714.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560150129838.png" alt="Alt text"><br><strong>这种既不改变空间分辨率，又不改变特征通道数的做法是为了把不同通道的特征给融合起来。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560150349813.png" alt="Alt text"></p><hr><p><strong>网络结构图中第三次出现的三个模块组：</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560150514126.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560150688225.png" alt="Alt text"></p><h2 id="3-Inception-v4-Network"><a href="#3-Inception-v4-Network" class="headerlink" title="3. Inception-v4 Network"></a>3. Inception-v4 Network</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560151352648.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560151643979.png" alt="Alt text"><br><strong>所谓特征融合其实就是将多个特征通道的值融合成一个数；而特征通道压缩指的是输出的特征通道的数目小于输入的特征通道的数目。</strong><br><strong>Branch1中的平均池化是对特征图做了一个空间模糊，而空间模糊则相当于对原来的特征图可能存在的一些噪音给去掉，而Branch2不使用对特征图进行空间模糊的平均池化是因为，空间模糊还有可能把真正有用的边缘信息或者物体的边缘轮廓也给去掉，因此下面采用折中的方式将两种方式都采用。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560153363346.png" alt="Alt text"></p><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560153739082.png" alt="Alt text"><br><strong>需要注意的是将一个</strong>$n \times n$<strong>的卷积分解成</strong>$1 \times n$<strong>和</strong>$n \times 1$<strong>的卷积</strong> <strong>相当于增加了非线性映射的深度。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560154116572.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560154845267.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560155421987.png" alt="Alt text"></p><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560155829143.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560156112315.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560156848895.png" alt="Alt text"></p><h2 id="4-Residual-Networks"><a href="#4-Residual-Networks" class="headerlink" title="4. Residual Networks"></a>4. Residual Networks</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560156959897.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157028430.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157150706.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157174268.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157276393.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157408304.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157588198.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157721323.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560157892573.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158197277.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158240945.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158492223.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158554251.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158694678.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158779559.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560158839431.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159089731.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159180492.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159271653.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159333154.png" alt="Alt text"></p><h2 id="5-Inception-ResNet-v1-v2"><a href="#5-Inception-ResNet-v1-v2" class="headerlink" title="5. Inception-ResNet-v1,v2"></a>5. Inception-ResNet-v1,v2</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159430770.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159672226.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159757091.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159832467.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560159973808.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160026383.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160130969.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160183813.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160235461.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160525435.png" alt="Alt text"></p><h2 id="6-SqueezeNet"><a href="#6-SqueezeNet" class="headerlink" title="6. SqueezeNet"></a>6. SqueezeNet</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160669640.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160711543.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160738129.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160762977.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160802319.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560160934523.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560161024034.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560161120186.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560161183724.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560161195229.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/Import-CNN-Framework/1560161258736.png" alt="Alt text"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;详细分析多个重要的卷积网络框架.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="卷积网络框架" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>深度学习理论</title>
    <link href="http://yoursite.com/2019/06/10/DeepLearningTheory/"/>
    <id>http://yoursite.com/2019/06/10/DeepLearningTheory/</id>
    <published>2019-06-10T05:10:38.000Z</published>
    <updated>2020-07-25T14:03:28.010Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>记录深度学习理论课程相关内容.</strong><br><a id="more"></a></p></blockquote><h2 id="1-1-Can-shallow-network-fit-any-function"><a href="#1-1-Can-shallow-network-fit-any-function" class="headerlink" title="1.1 Can shallow network fit any function"></a>1.1 Can shallow network fit any function</h2><blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558080065257.png" alt="Alt text"></p><p>上图是说：给定一个<strong>network</strong>，以及相应的<strong>parameter(weights and biases)</strong>，那么这个<strong>network</strong>代表了一个<strong>function</strong>，而同样<strong>structure</strong>的<strong>network</strong>，在填入不同的<strong>parameter(weights and biases)</strong>的时候就对应不同的<strong>function</strong>。对于一个给定了的<strong>network</strong>的架构(<strong>structure</strong>)的时候，在给定各式各样的<strong>parameter</strong>的情况之下，其实就是在所有可能的<strong>function</strong>里面划定一个范围(<strong>network space</strong>)。</p><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558080739654.png" alt="Alt text"></p><p>上图是说需要一个<strong>network</strong>去拟合图上的函数，在某一要求下，比方说要求<strong>network</strong>拟合函数达到某个精度的要求下，<strong>deep</strong>的<strong>network</strong>要比<strong>shallow</strong>的<strong>network</strong>的参数量少。</p><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558080994943.png" alt="Alt text"></p><hr><p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558081221451.png" alt="Alt text"></p><p> 上图是说如果一开始一个很<strong>shallow(small)</strong>的<strong>network</strong>不能覆盖到<strong>target function</strong>所在的<strong>function space</strong>，那么随着它的<strong>units</strong>的数目增长的越来越多，最终肯定是能<strong>fit</strong>到我们给定的<strong>target function</strong>的(后面讲到).</p><p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558081479151.png" alt="Alt text"></p><hr><p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558081540719.png" alt="Alt text"></p><p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082005754.png" alt="Alt text"></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082208813.png" alt="Alt text"></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082496627.png" alt="Alt text"></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082840572.png" alt="Alt text"></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558083173657.png" alt="Alt text"><br>上图中<strong>l</strong>的两个绿色点直接连线的距离，而<strong>L</strong>是<strong>L-Lipschitz</strong>中的那个<strong>L</strong>。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558084419662.png" alt="Alt text"></p><p>$error &lt; \Vert{f(x_1) - f(x_2)}\Vert \leq L\Vert{x_1 - x_2}\Vert &lt;l \times L \leq \xi    \Longrightarrow  l \leq \frac{\xi}{L}$<br>其中$ \Vert{f(x_1) - f(x_2)}\Vert \leq L\Vert{x_1 - x_2}\Vert$，且$\max_{0 \leq x \leq 1} \vert{f(x) - f^*(x)}\vert  \leq \xi$</p><p>因此，给定一个<strong>L-Lipschitz Function</strong>$f^<em>(x)$，需要找到一个<strong>piece-wise linear Function</strong>，让两者之间的<strong>error</strong>小于等于$\xi$，只需要做到在取线段的时候，相互之间的距离都取$\frac{\xi}{L}$就可以了，这样<strong>error</strong>就是小于等于$\xi$<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558086718838.png" alt="Alt text"><br>由此还可以得出，假设区间范围是$[0, 1]$之间，每个<strong>segment</strong>都是$\frac{\xi}{L}$，则<strong>segment</strong>的个数为$\frac{L}{\xi}$，也就是有$\frac{L}{\xi}$个<strong>piece-wise linear Function</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087000210.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087049947.png" alt="Alt text"><br>因此，按照证明，只要做出上面那个<strong>piece-wise linear function</strong>，我们就能<strong>fit</strong>我们的<strong>target function</strong>(<strong>L-Lipschitz function</strong>)，让它们的<strong>error</strong>小于等于$\xi$，那么有没有办法通过只有一个<strong>hidden layer</strong>，且<strong>activation function</strong>都是<strong>relu</strong>的<strong>network</strong>制造出上图中绿线那样的<em>*piece-wise linear function</em></em>呢？答案是可以的。方法之一：</p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087441622.png" alt="Alt text"></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087711894.png" alt="Alt text"></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087994599.png" alt="Alt text"><br>上面两张图就是在解释使用两个<strong>relu</strong>可以组合成<strong>1</strong>个<strong>segment</strong>，而$\frac{L}{\xi}$个<strong>segment</strong>(<strong>piece-wise linear Function</strong>)，需要$\frac{2L}{\xi}$个<strong>relu neurons</strong>，就可以近似一个<strong>L-Lipschitz function</strong>，但是需要声明的是这个只是说可以做到，并没有说是效率最高的做法。这里有一个比较俗语的解释是上面成立的一个原因是因为，<strong>L</strong>越大，代表<strong>function</strong>的变化越快，则是一个越复杂的函数，那越复杂的函数就需要越多的神经元；相应的，$\xi$越小，则说明要求的精准度越高，则$\xi$越小在需要的神经元个数就越多。</p></blockquote><h2 id="1-2-Potential-of-Deep"><a href="#1-2-Potential-of-Deep" class="headerlink" title="1.2 Potential of Deep"></a>1.2 Potential of Deep</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097021232.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097421561.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097519589.png" alt="Alt text"></p><blockquote><p>类似逻辑电路一样(类比)，层数更多的逻辑电路需要的<strong>gate</strong>更少，而层数更多的神经网络在拟合同一个函数的时候，需要的神经元更少<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097545982.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097751934.png" alt="Alt text"><br>上图所说的是：在参数量相同的情况下，<strong>deep network</strong>产生的线性分段要比<strong>shallow network</strong>更多。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558098139481.png" alt="Alt text"><br>上图中，<strong>relu</strong>是分段的，一部分的输出是<strong>0</strong>，还有一部分的输出和输入相同，对于采用<strong>relu</strong>作为激活函数的<strong>network</strong>，输出为<strong>0</strong>的那些可以拿掉，当做不存在一样，剩下的部分就好像是一个<strong>linear function</strong>，不过需要注意的是不能因此说<strong>relu</strong>是<strong>linear function</strong>，它是<strong>piece-wise linear function</strong>，当输入值达到某个阈值的时候<strong>relu</strong>就会从一种<strong>linear</strong>的输出状态到达另一种<strong>linear</strong>的输出状态。由于每个神经元在<strong>relu</strong>的左右下都有<strong>两种</strong>激活模式<strong>（ activation pattern）</strong>，因此如果有<strong>N</strong>个神经元，则有$2^N$种<strong>activation pattern</strong>，每一个<strong>activation pattern</strong>都制造了一个<strong>linear piece</strong>，所以总的也就是有$2^N$个<strong>linear pieces</strong>，但是需要注意的是这个是一个<strong>upper bound(最佳上限)</strong>，而这个上限可能永远都没有办法实现，有些<strong>pattern</strong>可能是永远也不会出现的，比方说：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099092748.png" alt="Alt text"></p></blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099266392.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099615364.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099694294.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099865914.png" alt="Alt text"></p><blockquote><p>从上面的图可以看出，当我们用<strong>deep structure</strong>的时候，每多加一层，线段的数目都变成<strong>double</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558100023891.png" alt="Alt text"><br>在上面的图中，每个<strong>abs activation function</strong>都是由两个<strong>relu activation function</strong>组成，有前面所讲的，每次两个神经元(<strong>每个神经元的激活都是relu</strong>)组成一个线段，那么对于一个<strong>shallow network</strong>，如果需要产生<strong>100</strong>个线性分段，则需要<strong>200</strong>个神经元；但是如果使用的是<strong>deep network</strong>，每层有两个神经元，每次多加一层，线段的数目就会增加两倍，所以对于同样的<strong>100</strong>个线性分段，则只需要<strong>7</strong>层就可以实现了。因此需要产生比较多的线性片段的时候，使用<strong>deep</strong>是比较有效率的。深层的网络把之前的<strong>pattern</strong>重复的组合起来，有规律的重复出来。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558101610973.png" alt="Alt text"><br>由上面的宽度为<strong>2</strong>，深度为<strong>N</strong>可以得出有$2^N$个线性分段的推论可以得出：宽度为$K$，深度为$H$的网络可以得出$K^H$个线性分段，以上就是<strong>network</strong>架构可以产生的线性分段的<strong>lower bound</strong>。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558102008385.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558102213746.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558102275286.png" alt="Alt text"><br>上面两图是验证<strong>network</strong>的<strong>low layer</strong>的参数是比较重要的：左边的是在<strong>CIFAR 10</strong>上面做的实验，其中往不同层的参数上加上噪音，如果噪音加在最后一层的话，对结果几乎没有什么影响，但是一样的噪声加在第一层，整个结果就坏掉了。右边的是在<strong>MNIST</strong>数据集上的，然后分别对第一层进行训练，其他都是<strong>random</strong>的，以及分别往后，一直到只对最后一层进行训练，其他都是<strong>random</strong>的，最上面的紫色就是只训练第一层的结果，也就是只训练第一层，别的不管的情况下，结果都是很不错的，然后，只训练最后一层，结果就会坏掉。</p><hr></blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103014973.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103428049.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103746080.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103919220.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104057330.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104144772.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104349911.png" alt="Alt text"></p><h2 id="1-3-Is-Deep-better-than-Shallow"><a href="#1-3-Is-Deep-better-than-Shallow" class="headerlink" title="1.3 Is Deep better than Shallow"></a>1.3 Is Deep better than Shallow</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104397074.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104528234.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104600075.png" alt="Alt text"><br>需要注意的是，上图中右边是实际<strong>relu</strong>达不到的一种状态，因为是不连续的，只是这里为了后面的证明，假设在梦幻状态下可以达到。</p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558142786551.png" alt="Alt text"><br>现在假设在横轴上取两点$x_0$和$x_0 + l$，两点之间的间隔为$l$，然后我们假设要取一条直线$ax + b$取拟合这条红色的线($y = x^2$)，这条直线不需要它的头和尾在<strong>target function</strong>上面，那么这条直线在给定间距$l$的情况下能够拟合的多好呢？那么实际的也就是求解 $error^2 = \int_{x_0}^{x_0+l} (x^2 - (ax + b))^2 \, {\rm d}x \Longleftrightarrow \sqrt{\int_0^1 \vert f(x) - f^*(x) \vert ^2 \, {\rm d}x} \leq \xi$，然后找出一个$a$和$b$使得$error$最小，其最小结果为：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558143543203.png" alt="Alt text"><br>其中的数学推导提醒：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558143795151.png" alt="Alt text"><br>那么上面是对于一条直线进行拟合的过程，那么如果现在在区间$[0, 1]$之间有$n$条线段($\sum_{i=1}^{n} l_i = 1$)进行拟合，那如何分配这$n$条线段使得$error$的值最小：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558144193000.png" alt="Alt text"><br>由于对于一条线段的时候$e^2 = \frac{l^5}{180}$，那么对于多个分段的线段来说，每个的$(e_i)^2 = \frac{(l_i)^5}{180}$，因此总的$error^2$为：</p><script type="math/tex; mode=display">E^2 = \sum_{i=1}^{n} (e_i)^2 = \sum_{i=1}^{n} \frac{(l_i)^5}{180}</script><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558144599224.png" alt="Alt text"><br>所以现在需要考虑的是如何分配 $l_1$ 到 $l_n$ 使得 $E^2$ 最小呢？正确的答案是间隔$l_i$平均分配(感觉有点像是最大熵模型)：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558144869756.png" alt="Alt text"><br>由于采用间隔平均分配，因此：</p><script type="math/tex; mode=display">l_i = \frac{1}{n}</script><script type="math/tex; mode=display">E^2 = \sum_{i=1}^{n} \frac{(1/n)^5}{180} = \frac{1}{180}·\frac{1}{n^4}</script><p>平均分配的原因：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558145611525.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558145771413.png" alt="Alt text"><br>因此，当$n$取上面的值的时候(也就是有上面那么多的<strong>linear pieces</strong>)才能让$Error \leq \xi$，而我们知道，每一个<strong>piece</strong>在<strong>shallow</strong>的状况下都需要一个<strong>neuron</strong>来制造，因此在<strong>shallow</strong>状况下我们仍然需要$O(\frac{1}{\sqrt \xi})$个<strong>neurons</strong>才能够去拟合$y = x^2$<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558146848316.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558146860041.png" alt="Alt text"><br>因此在给了<strong>shallow</strong>梦幻状态的情况之下，结果<strong>shallow</strong>的最佳状态依旧是$O(\frac{1}{\sqrt \xi})$，因此可以得出<strong>deep</strong>是比<strong>shallow</strong>好的(虽然没有去证实$O(\log_2(\frac{1}{\sqrt \xi})))$是否是<strong>deep</strong>最好的状态).<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147073605.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147082431.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147223285.png" alt="Alt text"><br><strong>需要注意的是，上面说的是存在某一个function(文章中的应该？)，而不是说所有的function都不行。</strong></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147459349.png" alt="Alt text"><br>上面这张图就是给出再上面的两张图中的某一个函数，这里给出的是一个球状的函数，球外面全是<strong>0</strong>，球里面全是<strong>1</strong>，结果是不管浅层的网络的宽度如何增加，损失依旧没有办法降下来，但是深层的网络，宽度只在<strong>100</strong>的时候就能降下来了。</p><p>但是需要主要的是不是所有的<strong>function</strong>都是<strong>deep</strong>比<strong>shallow</strong>的好(比方说<strong>linear</strong>)，因此下面的文章给出结论：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147593069.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147845171.png" alt="Alt text"><br>因此结论是：比$x^2$还简单的(比如$y = x$)是不符合<strong>deep</strong>比<strong>shallow</strong>更好的，但是比$x^2$复杂的就是深得比浅的好，所以深度学习是有用的。</p><h2 id="2-1-When-Gradient-is-Zero"><a href="#2-1-When-Gradient-is-Zero" class="headerlink" title="2.1 When Gradient is Zero"></a>2.1 When Gradient is Zero</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558148468485.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558148984222.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558149644026.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162352495.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162375183.png" alt="Alt text"></p><h3 id="2-1-1-Hessian-Matrix-When-Gradient-is-Zero"><a href="#2-1-1-Hessian-Matrix-When-Gradient-is-Zero" class="headerlink" title="2.1.1 Hessian Matrix: When Gradient is Zero"></a>2.1.1 Hessian Matrix: When Gradient is Zero</h3><blockquote><p><strong>Critical Point: </strong>驻点；<strong>stuck: </strong>卡住，无法移动；<strong>curvature: </strong>曲率；<strong>invertible：</strong>可逆的；<br><strong>dominant：</strong>主导<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162442951.png" alt="Alt text"></p><p>需要注意的是训练到一定程度的时候，出现损失基本不再下降，梯度趋近到<strong>0</strong>的时候，不要随随便便就说走到了<strong>local minima/global minima</strong>，因为还有可能是<strong>saddle point(鞍点: 并非极值，但是梯度依旧为0)</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162665301.png" alt="Alt text"><br>那么要区分是上面哪一种的时候，就需要使用<strong>Hessian Matrix: </strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558163693705.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558163703389.png" alt="Alt text"></p></blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558165032965.png" alt="Alt text"></p><blockquote><p>对于上图的解释是: 假设$f(x)$实际是图中蓝色的曲线，如果现在排除掉包含$H$的那一项，只考虑前面的两项，则实际得到的结果是只能得到绿色的虚线。因此蓝色实线和绿色虚线的差异就是在于<strong>H</strong>所在的那一项。</p></blockquote><p>因此有了<strong>Hessian Matrix</strong>之后就可以定义新的<strong>Optimize</strong>的方法(也就是<strong>Newton’s method</strong>)。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558166465037.png" alt="Alt text"><br>上图中，黑色是$f(x)$，前两项是绿色的实现，梯度无变化；而三项都考虑则变成红色的曲线，梯度则有了变化，我们期望通过某种方法找到红色曲线的最小值(梯度为<strong>0</strong>)的地方，方法就是对整个式子求一下导数为<strong>0</strong>的地方。</p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558167309487.png" alt="Alt text"><br>上图中，相较于梯度下降仅能靠梯度指引方向的方式，牛顿法能够通过$H^{-1}$($H^{-1}$自动决定了<strong>learning_rate</strong>要有多大)一步直达最低点。</p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558168321883.png" alt="Alt text"><br>上图中$f(\theta)$为黑色的曲线，而后面三项的组合为红色的曲线，以某个点$x_0$为初始点，当$\theta$和$\theta_0$非常接近的时候，红色的线才和黑色的线很贴近，那从初始点，初始的红线开始，牛顿法会一步走到初始红线的最低点，得到$x_1$，然后在$x_1$这个地方会重新计算$g$和$H$，然后得到新的第二条红线，然后牛顿法又是一步到第二条红色的二次曲线最低点，得到$x_2$，然后继续。。。所以相较于<strong>Gradient descent</strong>，牛顿法每次可以走一大步。当$f(x)$是二次函数的时候(<strong>quadratic function</strong>)，牛顿法直接一步到位。</p><p>但是为何有这么强方法，却不使用呢？因为牛顿法在深度学习中不好用，如下：</p><blockquote><ol><li>需要计算$H^{-1}$，因为在深度学习中参数的量动则就是几千万甚至上亿，由于<strong>H</strong>是$n \times n$的，所以要算它的$H^{-1}$基本是不可能的。</li><li>牛顿法是寻找梯度为<strong>0</strong>的点，但是梯度为<strong>0</strong>的点不一定是最值点，有可能是鞍点，因为在深度学习所定义出来的<strong>Loss function</strong>里，有很多<strong>gradient</strong>为<strong>0</strong>的点，不是<strong>local minima</strong>，而是<strong>saddle point</strong>.</li><li><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558168505545.png" alt="Alt text"></li></ol></blockquote><p>那么上面讲解<strong>Hessian Matrix</strong>的意义何在呢？<br>假设在用梯度下降的时候<strong>updata</strong>参数的时候，到了某一步，梯度很小的时候，这个时候$f(\theta)$的特性就会变成由<strong>Hessian</strong>主导，通过<strong>H</strong>告诉我们<strong>critical point</strong>到底属于那种情况.<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558168940567.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558169051514.png" alt="Alt text"></p><blockquote><p><strong>positive/negative definite: </strong>正定/负定<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558169310295.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558170532234.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558170569338.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558171129367.png" alt="Alt text"><br>上图是说，我们从$\theta_0$这个地方移动一个$H$的特征向量$v$，我们会增加特征值大小的值(假设特征值为正的时候).<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558171518613.png" alt="Alt text"><br>上图是说，使用<strong>Hessian</strong>的特征值和特征向量分析<strong>critical point</strong>的时候：<br>① 当$u$是$H$的特征向量的线性组合(这个随意的方向$u$一定是$H$的特征向量的线性组合，对称矩阵的性质)，且所有的特征值都是正的，那么往$u$的方向走过去的时候，$f(\theta)$会增加对应特征值大小的值，则现在的状态是对于<strong>local minima</strong>;<br>② 当$u$是$H$的特征向量的线性组合，且所有的特征值都是负的，那么往$u$的方向走过去的时候，$f(\theta)$会减小对应特征值大小的值，则现在的状态是对于<strong>local maxima</strong>;<br>③ 当$u$是$H$的特征向量的线性组合，且所有的特征值有正有负，那么往某些方向的时候，$f(\theta)$会增加，相反，$f(\theta)$会减少，则现在的状态是对于<strong>saddle point</strong>;</p></blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558172883403.png" alt="Alt text"></p><blockquote><p>上面的例子中如何计算<strong>Hessian Matrix</strong>的种类：<br>假设特征向量如下：<script type="math/tex">v = \begin{bmatrix} a & b \\  \end{bmatrix}</script><br>因此：</p><script type="math/tex; mode=display">v^\mathrm T H v</script><script type="math/tex; mode=display">= \begin{bmatrix} a & b \\  \end{bmatrix} ^\mathrm T· \begin{bmatrix} 2 & 0 \\ 0 & 6 \\  \end{bmatrix} · \begin{bmatrix} a & b \\  \end{bmatrix}</script><script type="math/tex; mode=display">= \begin{bmatrix} 2a & 6b \\  \end{bmatrix}·\begin{bmatrix} a  \\ b \\  \end{bmatrix}</script><script type="math/tex; mode=display">= 2a^2 + 6b^2 > 0</script><script type="math/tex; mode=display">\because (a,b \neq 0)</script><script type="math/tex; mode=display">\therefore H \implies Positive-definite</script><script type="math/tex; mode=display">\implies local-minima</script></blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558172939358.png" alt="Alt text"></p><hr><blockquote><p><strong>Degenerate: </strong>衰退，退化<br>退化矩阵(奇异矩阵)：奇异阵是行列式为0的方阵，是特征值（奇异值）含有0的方阵。<br><strong>Degenarate Hessian has at least one zero eigen value代表的意思是往相应的那个方向的特征向量走，不增也不减.</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558183857197.png" alt="Alt text"><br>④因此当我们发现$H$有$zero vecror$的时候，我们没有办法判断在$zero eigen value$的方向是增加还是减少。当$Hessian$那项为<strong>0</strong>的时候，其他被抹去的项就起作用了。所以到$H$有特征值为<strong>0</strong>的时候，这个时候用<strong>Hessian</strong>判断是<strong>local minima、local maxima、saddle point</strong>就不适用了(无法判断了，都有可能，这个时候其中用去决定是哪一种的就是被抹去的更高次的项了)。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558184814846.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558184921722.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185042995.png" alt="Alt text"></p></blockquote><hr><blockquote><p>当你在训练的时候，<strong>loss</strong>不再下降了，上面的那些讨论都是假设是出于<strong>critical point(gradient为0的情况)</strong>，然后基于<strong>Hessian</strong>来讨论是出于<strong>local minima/local maxima/saddle point</strong>哪种；但是实际上还有可能<strong>training</strong>停下来了，并不代表你一定走到了<strong>critical point(gradient为0的情况)</strong>，因为这个时候你的<strong>gradient</strong>还有可能不为<strong>0</strong>。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185156166.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185608066.png" alt="Alt text"><br>上面也是这样的情况，并非到了<strong>critical point</strong>的地方，分析可能是更加复杂的地方，<strong>gradient</strong>变小的区域里，可能逐渐靠近一个<strong>saddle point</strong>，增大则可能是逃出了这个<strong>saddle point</strong>，在变小则可能是靠近了另一个<strong>saddle point</strong>… …</p></blockquote><h2 id="2-2-Deep-Linear-Network"><a href="#2-2-Deep-Linear-Network" class="headerlink" title="2.2 Deep Linear Network"></a>2.2 Deep Linear Network</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185790834.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558186237347.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558186630036.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558187390194.png" alt="Alt text"></p><blockquote><p>上面计算出来在$w_1、w_2 = 0$的原点处是一个<strong>saddle point</strong>.而在$w_1·w_2 = 1$处则是<strong>critical point</strong>(在上面的热力图中可以看出，而且是<strong>global minima</strong>)。<br><strong>利用Hessian 计算critical point的性质.</strong><br><strong>hessian 有0的特征值的时候是，那个方向是分析是哪种情况是不好分析的，除非以某些具体数据去算算看。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558188314135.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558188466702.png" alt="Alt text"></p></blockquote><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189160490.png" alt="Alt text"></p><blockquote><p>对于上面的深度线性网络，可以证明，只要满足一些非常宽松的条件，这个线性网络不管叠加多少层，它的所有局部极小值都是全局最小值。比方说一些文章给出的宽松条件是<strong>Hidden layer size &gt;= input dim, output dim</strong>, 比如输入和输出的维度是5维，那么中间每一个隐层的输出维度都要大于或者等于5维即可。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189596902.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189619568.png" alt="Alt text"></p></blockquote><h2 id="2-3-Does-Deep-Network-have-Local-Minima"><a href="#2-3-Does-Deep-Network-have-Local-Minima" class="headerlink" title="2.3 Does Deep Network have Local Minima"></a>2.3 Does Deep Network have Local Minima</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189862012.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558190399092.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558190572757.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558190855626.png" alt="Alt text"><br><strong>上图说明初始化很重要，否则容易陷入盲点，造成训练落入local_minima,最后崩坏。</strong></p><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191303644.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191457103.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191502066.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191556193.png" alt="Alt text"><br><strong>这一节就是说Deep Non-linear Network</strong>是存在<strong>local minima</strong>的，上面那些文献就是在设计各种实验去找出什么情况下容易遇到<strong>local minima</strong>，可能以后会有新的理论告诉我们应该如何设计网络(如何初始化，什么样的结构，什么样的数据…)，使用什么样的条件，就能有效的避开<strong>local minial</strong>，从而找到<strong>global minima</strong></p><h2 id="2-4-Geometry-of-Loss-Surface-Conjecture"><a href="#2-4-Geometry-of-Loss-Surface-Conjecture" class="headerlink" title="2.4 Geometry of Loss Surface(Conjecture)"></a>2.4 Geometry of Loss Surface(Conjecture)</h2><blockquote><ol><li><strong>Geometry: </strong>n.    几何(学); 几何形状; 几何图形; 几何结构;</li><li><strong>Conjecture: </strong>n.    猜测; 推测; 揣测; 臆测; v.    猜测; 推测;</li><li><strong>Empirical: </strong>adj.    以实验(或经验)为依据的; 经验主义的;</li></ol></blockquote><p>现在在<strong>Deep Learning</strong>的领域有这么一个推论：几乎所有的<strong>local minimum</strong>它的<strong>loss</strong>，跟<strong>global optinum</strong>都是差不多的，因此如果卡在了<strong>local minimum</strong>也不要惊慌，因它跟<strong>global minimum</strong>的<strong>loss</strong>是差不多的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230298561.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230627503.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230781646.png" alt="Alt text"></p><blockquote><p>上图是基于再上面一张图的假设(假设特征值一半为正，一半为负，至于为什么？不管，文章这么假设的)说明我们的网络越大，遇见<strong>saddle point</strong>的可能性越大。</p></blockquote><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230976183.png" alt="Alt text"></p><blockquote><p>现在假设几率不再是$\frac{1}{2}$, 而是跟现在的<strong>loss</strong>有关了，现在假设几率为$p$(上图中的<strong>error</strong>指的就是<strong>loss</strong>).<br>需要注意的是：<strong>Larger error，larger p</strong>是一个假设($p$是负的<strong>eigenvalue</strong>出现的几率，而<strong>负的eigenvalue</strong>代表<strong>loss</strong>有一条路往下降)。其实这个假设也是蛮合理的，想想看，刚开始的时候，在<strong>loss</strong>还比较高的地方，应该会有很多条路让你往<strong>loss</strong>更低的地方走，当<strong>loss</strong>很大的时候是比较有可能出现<strong>negative eigenvalue</strong>，后面训练到后来，<strong>loss</strong>很低时候，出现<strong>negative eigenvalue</strong>的几率$p$就变得很小了。因此上图的图中<strong>loss</strong>随着大小的不同，走到<strong>critical point</strong>，对应的<strong>eigenvalue</strong>的<strong>分布</strong>的变化。<br>从上图还能看出：在<strong>loss</strong>比较大的时候，<strong>eigenvalue</strong>为正的概率和为负的概率基本一半；<strong>loss</strong>比较小的时候，<strong>eigenvalue</strong>基本为正；因此<strong>saddle point</strong>比较容易出现在<strong>loss</strong>比较大的时候，而<strong>local minimum</strong>比较容易出现在<strong>loss</strong>比较低的地方(因为此时<strong>eigenvalue</strong>基本都是为正的)。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232413671.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232523313.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232630425.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232645925.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232744765.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232952444.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232970689.png" alt="Alt text"></p></blockquote><h2 id="2-5-Geometry-of-Loss-Surface-Empirical"><a href="#2-5-Geometry-of-Loss-Surface-Empirical" class="headerlink" title="2.5 Geometry of Loss Surface(Empirical)"></a>2.5 Geometry of Loss Surface(Empirical)</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233086374.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233094775.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233469124.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233650824.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233775339.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234122697.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234269079.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234347827.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234639087.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234762963.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234803475.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234907367.png" alt="Alt text"><br><strong>不同初始化，造就的模型不同。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235070770.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235215231.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235350683.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235486657.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235587901.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235644335.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235807379.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235877541.png" alt="Alt text"></p><h2 id="3-1-Generalization-Capability-of-Deep-Learning"><a href="#3-1-Generalization-Capability-of-Deep-Learning" class="headerlink" title="3.1 Generalization Capability of Deep Learning"></a>3.1 Generalization Capability of Deep Learning</h2><blockquote><p>需要注意的是，不管你的数据的分布是什么样子的，都会有$1-\delta$概率($\delta$是你自己定的一个值)出现下面的式子：</p><script type="math/tex; mode=display">E_{test} \leq E_{train} + \Omega(R, M, \delta)</script><p>因此有：</p><script type="math/tex; mode=display">E_{train} \leq E_{test} \leq E_{train} + \Omega(R, M, \delta)</script><p>其中$\Omega(R, M, \delta)$与$\delta$有关，至于这一项是什么后面再说。<br>因此$\delta$越小，上面式子发生的概率越大，则中间的$E_{test}$所在的区间就得越大(这样才有很大概率出现在这个区间之内)，因此相应的$\Omega(R, M, \delta)$也就越大。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558270653247.png" alt="Alt text"></p><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558270867907.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558270971276.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558271086884.png" alt="Alt text"><br><strong>capacity of model</strong>的含义是对于错误标注的数据进行拟合，达到百分百正确时使用的数据的量就是模型的$VC dimension$。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558271290720.png" alt="Alt text"></p><hr><p><strong>Overparameterized: </strong>过参数化，参数过多；<br>一个比较反直觉的结论：在深度学习中，如果训练集的错误率很小了，而测试集的错误率比较高的情况下，如果这个时候再增加模型中的参数量，测试集的结果反而会变好。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558271774929.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558272967476.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273007272.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273269484.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273613610.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273738169.png" alt="Alt text"><br>上面的都是在验证模型大的情况下，泛化反而会相比较之前的较差结果要好一些。至于原因暂时还没有定论结果。</p><hr><p>下面这个实验是说：神经网络是自带正则化的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273984950.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274308341.png" alt="Alt text"></p></blockquote><h2 id="3-2-Indicator-of-Generalization"><a href="#3-2-Indicator-of-Generalization" class="headerlink" title="3.2 Indicator of Generalization"></a>3.2 Indicator of Generalization</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274443913.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274626635.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274741946.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275064641.png" alt="Alt text"></p><blockquote><p>上图中左图最下面的虚线正确率先增加后减少，说明模型在初期是先从$20 %$的真实标注数据中学习到了有用的东西的，而后遇到<strong>80 %</strong>错误标注的时候的正确率开始下降。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275402082.png" alt="Alt text"></p><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275431578.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275669761.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275998776.png" alt="Alt text"><br>其实$x$的<strong>sensitivity</strong>还是很直觉的，也就是说现在输入的数据如果有一个小量的变化的话，那对我的输出到底是有多大的变化。<strong>sensitivity</strong>和泛化之间的关系就是如果某个网络对某一份数据非常的<strong>sensitive</strong>，那么意味着这个<strong>Network</strong>不够<strong>roboost</strong>。<br>而<strong>Regularization</strong>也可以看做是<strong>minimize sensitivity</strong>，因此正则化的时候<strong>weight</strong>越接近0越好，其实就是让<strong>output</strong>越平滑，<strong>output</strong>越平滑其实就是<strong>sensitivity</strong>越小，因为<strong>input</strong>有变化的时候，<strong>output</strong>变化是比较小的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558276492716.png" alt="Alt text"><br>那么对于一个没有标签的测试集，就可以通过计算<strong>sensitivity</strong>的值去判断<strong>test</strong>的表现结果如何。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558276779000.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558276927319.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277096050.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277114755.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277165544.png" alt="Alt text"></p></blockquote><hr><p>下面是说不同锋利程度的<strong>local minimum</strong>与泛化程度的关系(的一种可能猜想)：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277403045.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277607275.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277786862.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277908659.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558278039524.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558278075345.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558278095267.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558356388010.png" alt="Alt text"></p><h2 id="4-1-1-Batch-Normalization"><a href="#4-1-1-Batch-Normalization" class="headerlink" title="4.1.1  Batch_Normalization"></a>4.1.1  Batch_Normalization</h2><p><strong>Batch_Norm:</strong></p><script type="math/tex; mode=display">\mu = \frac{1}{m} \sum_{i} z^{(i)}</script><script type="math/tex; mode=display">{\sigma}^2 = \frac{1}{m} (z^{(i)} - \mu)^2</script><script type="math/tex; mode=display">z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><script type="math/tex; mode=display">\tilde{z}^{(i)} = \gamma·z_{norm}^{(i)} + \beta</script><p>其中 $\epsilon$ 是为了保证数值稳定. $\gamma$ 和 $\beta$ 是可以跟随<strong>backpropagation</strong>进行学习更新的参数，类似神经网络的中的权重$\omega$, 更新公式如下：</p><script type="math/tex; mode=display">\omega^{[l]} := {\omega}^{[l]} - \alpha·d{\omega}^{[l]}</script><script type="math/tex; mode=display">\gamma^{[l]} := {\gamma}^{[l]} - \alpha·d{\gamma}^{[l]}</script><script type="math/tex; mode=display">\beta^{[l]} := {\beta}^{[l]} - \alpha·d{\beta}^{[l]}</script><p>其中$\gamma$以及$\beta$也是可以再反向传播的时候进行<strong>training</strong>的参数，需要注意的是：当 $\gamma = \sqrt{\sigma^2 + \epsilon}$ 以及 $\beta = \mu$ 的时候，相当于是恢复到了<strong>batch_norm</strong>之前的状态.</p><blockquote><hr><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558356602000.png" alt="Alt text"><br><strong>Feature Scaling:</strong><br>下面这个图中，假设$x_1$是$1,2,…$，$x_2$是$100,200,….$，假设$x_1,x_2$是一样重要的<strong>feature</strong>，对结果的影响力是一样的，这意味着$w_1$的<strong>scale</strong>会比较大，然后把$w_1,w_2$与$loss$的关系拿出来作图，就会发现在$w_1$这个方向的变化的斜率是比较小的，相应的<strong>gradient</strong>是比较小的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558357052634.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558358133270.png" alt="Alt text"><br>对于深度学习，在<strong>training</strong>的过程中每一层的参数都是不断的变化，则每一层的<strong>output</strong>都会在变化，相应的每一层的<strong>mean</strong>和<strong>variance</strong>也就在不断的变化。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558358847474.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558358946692.png" alt="Alt text"><br><strong>Batch_Norm中的Batch不能太小了，不然算均值和方差会不准。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558359272868.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558359358028.png" alt="Alt text"><br>需要注意的是<strong>Batch_Norm</strong>在做<strong>backpropogation</strong>的时候是需要通过$\mu, \sigma$的，不能把它们两个当成是常量给排除掉，因为往上返回的时候$\mu, \sigma$是会改动的，所以不能当成常量。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558359744159.png" alt="Alt text"><br>$\gamma$和$\beta$不同于$\mu$和$\sigma$的，他俩是独立于数据的，他俩是<strong>network</strong>自己的学习得到的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558360603736.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558360856175.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558360899077.png" alt="Alt text"></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;记录深度学习理论课程相关内容.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="理论推导" scheme="http://yoursite.com/tags/%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/"/>
    
  </entry>
  
  <entry>
    <title>迁移学习中预训练模型的保存文件</title>
    <link href="http://yoursite.com/2019/06/06/model-pd/"/>
    <id>http://yoursite.com/2019/06/06/model-pd/</id>
    <published>2019-06-06T03:58:39.000Z</published>
    <updated>2020-07-25T14:08:26.248Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>分析不同的预训练模型的保存文件的特性.</strong><br><a id="more"></a></p></blockquote><h2 id="1-pb文件"><a href="#1-pb文件" class="headerlink" title="1. pb文件"></a>1. pb文件</h2><blockquote><p>在使用<strong>迁移学习</strong>代码的时候发现，对于存在着不同的模型保存文件，例如： <strong>.npy文件、.npz文件、.mat文件、.ckpt文件以及.pb文件</strong>。其中我在使用<strong>VGG16</strong>的是使用的模型保存文件是<strong>.npy文件</strong>，在使用<strong>Inception-v3</strong>的时候使用的是<strong>.pb文件</strong>。</p><p>对于<strong>.npy文件、.npz文件、.mat文件</strong>而言，这些文件仅仅保存着网络的权重，因此要想使用就得在自己模型的代码中重建网络，根据网络结构一步步搭建，将相应的权重赋予其中的$w、b$，然后再需要修改的层那里对新的变量进行初始化。需要明确的是：<strong>tf.Optimizer</strong>只优化<strong>tf.GraphKeys.TRAINABLE_VARIABLES</strong>中的变量，也就是说在进行<strong>Backpropagation</strong>参数更新的，只会回传梯度到<strong>Variable</strong>(变量)上去，常量是被固定住了，不会被更新的，因此在迁移学习中对于很多预训练了的框架，想要在迁移的时候对部分参数进行固定以避免训练的话，可以通过使用<strong>tf.constant</strong>或者<strong>python</strong>变量的形式来规避常量被训练。</p><p>除了以上的几个模型保存格式之外，通常我们使用<strong>TensorFlow</strong>时保存模型都使用 <strong>ckpt</strong> 格式的模型文件(<code>tf.train.Saver()</code>)，但是这种方式有几个缺点：</p><blockquote><ol><li>首先这种模型文件是依赖 <strong>TensorFlow</strong> 的，只能在其框架下使用；</li><li>其次，在恢复模型之前还需要再定义一遍网络结构，然后才能把变量的值恢复到网络中。</li></ol></blockquote><p>也就是说：通过<strong>tf.saver</strong>保存形成的<strong>ckpt</strong>文件其变量数据和图是分开的。我们知道<strong>TensorFlow</strong>是先画图，然后通过<strong>placeholde</strong>往图里面喂数据。这种解耦形式存在的方法对以后的迁移学习以及对程序进行微小的改动提供了极大的便利性。但是对于一些模型来说，如果训练好了之后不会再发生改变的话，那么<strong>ckpt</strong>的方式就没有必要了。一方面，<strong>ckpt</strong>文件储存的数据都是变量，既然我们不再改动，就应当让其变成常量，直接<strong>“烧”</strong>到图里面(也就是是将模型参数固化到图文件中)。另一方面，谷歌推荐的保存模型的方式是保存模型为<strong>pb</strong>文件，<strong>它具有语言独立性，可独立运行，封闭的序列化格式，任何语言都可以解析它，它允许其他语言和深度学习框架读取、继续训练和迁移 TensorFlow 的模型</strong>。它的主要使用场景是<strong>实现创建模型与使用模型的解耦， 使得前向推导 inference的代码统一</strong>。另外的好处是保存为<strong>pb</strong>文件时候，模型的变量都会变成固定的，导致模型的大小会大大减小，适合在手机端运行。对于线上的模型，我们一般是通过<strong>C++</strong>或者<strong>C</strong>语言编写的程序进行调用。所以一般模型最终形式都是应该写成<strong>pb</strong>文件的形式。</p></blockquote><h2 id="2-相关网页链接"><a href="#2-相关网页链接" class="headerlink" title="2. 相关网页链接"></a>2. 相关网页链接</h2><blockquote><ol><li><a href="https://blog.csdn.net/wc781708249/article/details/78043099?locationNum=10&amp;fps=1" target="_blank" rel="noopener">Tensorflow-pb保存与导入</a></li><li><a href="https://blog.csdn.net/guvcolie/article/details/77478973" target="_blank" rel="noopener">将tensorflow网络模型（图+权值）保存为.pb文件，并从.pb文件中还原网络模型</a></li><li><a href="https://zhuanlan.zhihu.com/p/31417693" target="_blank" rel="noopener">知乎——tensorflow保存和恢复模型的两种方法介绍</a></li><li><a href="https://cloud.tencent.com/developer/article/1357573" target="_blank" rel="noopener">Tensorflow MobileNet移植到Android</a></li><li><a href="https://juejin.im/entry/5be3b34c6fb9a049fc030303" target="_blank" rel="noopener">从Tensorflow模型文件中解析并显示网络结构图（pb模型篇）</a></li><li><a href="https://zhuanlan.zhihu.com/p/64099452" target="_blank" rel="noopener">知乎——[深度学习] TensorFlow中模型的freeze_graph</a></li><li><a href="https://blog.csdn.net/zyc121561/article/details/82222417" target="_blank" rel="noopener">基于Inception-v3的CNN迁移学习框架训练实例</a></li><li><a href="https://epleone.github.io/2018/03/19/tensorflow-guide/" target="_blank" rel="noopener">TensorFlow 学习心得</a></li><li><a href="https://blog.csdn.net/wc781708249/article/details/78043099?locationNum=10&amp;fps=1" target="_blank" rel="noopener">Tensorflow-pb保存与导入</a></li><li><a href="http://www.yanglajiao.com/article/qq_36356761/79505153" target="_blank" rel="noopener">TensorFlow 预训练模型导入</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;分析不同的预训练模型的保存文件的特性.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="迁移学习" scheme="http://yoursite.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>近期迁移学习相关网页</title>
    <link href="http://yoursite.com/2019/06/05/transfer-learning-Web-page/"/>
    <id>http://yoursite.com/2019/06/05/transfer-learning-Web-page/</id>
    <published>2019-06-05T02:10:04.000Z</published>
    <updated>2020-07-25T14:05:19.760Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>近期有关迁移学习的一些重要的网页.</strong><br><a id="more"></a></p><ol><li><a href="https://blog.csdn.net/qq_33431368/article/details/79344848" target="_blank" rel="noopener">【TensorFlow实战笔记】 迁移学习实战—卷积神经网络CNN-Inception-v3模型(非常重要)</a></li><li><a href="https://blog.csdn.net/White_Idiot/article/details/78816850" target="_blank" rel="noopener">【TensorFlow】迁移学习（使用Inception-v3）(非常重要)</a></li><li><a href="https://blog.csdn.net/gaoyueace/article/details/79724359" target="_blank" rel="noopener">使用TensorFlow-Slim实现Inception模块</a></li><li><a href="https://www.jianshu.com/p/54a96089fdac" target="_blank" rel="noopener">TensorFlow将checkpoint文件转成.pd文件</a></li><li><a href="https://blog.csdn.net/oYouHuo/article/details/81111833" target="_blank" rel="noopener">TensorFlow保存训练模型为pd文件并恢复(重要)</a></li><li><a href="https://blog.csdn.net/qq26983255/article/details/82846614" target="_blank" rel="noopener">TensorFlow：将ckpt文件固化成pb文件(非常重要)</a></li><li><a href="https://blog.csdn.net/guyuealian/article/details/82218092" target="_blank" rel="noopener">tensorflow实现将ckpt转pb文件(非常重要)</a></li><li><a href="https://blog.csdn.net/yjl9122/article/details/78341689" target="_blank" rel="noopener">tensorflow—使用freeze_graph.py将ckpt转为pb文件(重要)</a></li><li><a href="https://blog.csdn.net/mao_xiao_feng/article/details/73409975" target="_blank" rel="noopener">【Tensorflow】辅助工具篇——tensorflow slim(TF-Slim)介绍(重要)</a></li><li><a href="https://tensorlayercn.readthedocs.io/zh/latest/" target="_blank" rel="noopener">TensorLayer 中文文档</a></li><li><a href="https://www.cnblogs.com/zengfanlin/p/8970868.html" target="_blank" rel="noopener">用tensorlayer导入Slim模型迁移学习</a></li><li><a href="https://niuyuanyuanna.github.io/2018/12/06/tensorflow/4.1%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%86%E5%88%AB%E8%8A%B1/#inceptipn-v3%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">Inception-V3迁移学习(重要)</a></li><li><a href="http://czx.im/2018/04/18/Tensorflow6/" target="_blank" rel="noopener">使用 Google Inception V3模型进行迁移学习之——牛津大学花朵分类(重要)</a></li><li><a href="https://blog.csdn.net/u013709270/article/details/78778538" target="_blank" rel="noopener">应用TF-Slim快速实现迁移学习</a></li><li><a href="https://blog.csdn.net/chaipp0607/article/details/74139895" target="_blank" rel="noopener">TensorFlow-Slim图像分类库(重要)</a></li><li><a href="https://blog.csdn.net/Mr_health/article/details/81285119" target="_blank" rel="noopener">tensorflow利用slim进行迁移学习(重要)</a></li><li><a href="https://blog.csdn.net/weixin_42001089/article/details/81055745" target="_blank" rel="noopener">tensorflow-Inception迁移学习(非常重要)</a></li><li><a href="https://blog.csdn.net/qq_38343111/article/details/82256878" target="_blank" rel="noopener">Tensorflow Inception-v3模型迁移学习(重要)</a></li><li><a href="https://www.cnblogs.com/hellcat/p/6909269.html" target="_blank" rel="noopener">『TensorFlow』迁移学习</a></li><li><a href="https://blog.csdn.net/u014061630/article/details/80557028" target="_blank" rel="noopener">Tensorflow之pb文件分析(重要)</a></li><li><a href="https://blog.csdn.net/qq_36653505/article/details/86526310" target="_blank" rel="noopener">查看TensorFlow的pb模型文件的ops和tensor并使用TensorBoard可视化(重要)</a></li><li><a href="https://blog.csdn.net/fu6543210/article/details/80343345" target="_blank" rel="noopener">tensorflow保存数据为.ｐｂ格式和加载ｐｂ文件</a></li><li><a href="https://zhuanlan.zhihu.com/p/32887066" target="_blank" rel="noopener">知乎——TensorFlow 保存模型为 PB 文件(重要)</a></li><li><a href="https://www.zhihu.com/question/49637656/answer/142035602" target="_blank" rel="noopener">知乎——tensorflow如何从pb or pbtxt中训练一个model？(非常重要)</a></li><li><a href="https://zhuanlan.zhihu.com/p/47649285" target="_blank" rel="noopener">知乎——TensorFlow应用.pb文件保存和加载模型方法及相关注意事项</a></li><li><a href="https://www.zhihu.com/question/58287577?sort=created" target="_blank" rel="noopener">知乎——tensorflow训练好的模型怎么调用？</a></li><li><a href="http://czx.im/2018/04/18/Tensorflow6/" target="_blank" rel="noopener">Tensorflow学习笔记6(Transfer Learning迁移学习）(重要)</a></li><li><a href="https://www.jianshu.com/p/613c3b08faea" target="_blank" rel="noopener">TensorFlow学习笔记：Retrain Inception_v3（一）(重要)</a></li><li><a href="https://www.jianshu.com/p/4e5b3e652639" target="_blank" rel="noopener">Inception-v2/v3结构解析（原创）(重要)</a></li><li><a href="http://wenda.chinahadoop.cn/question/5890" target="_blank" rel="noopener">Inception V3要避免bottleneck，而ResNet使用了bottleneck，这该如何理解？</a></li><li><a href="https://blog.csdn.net/zziahgf/article/details/82801098" target="_blank" rel="noopener">网络结构之 Inception V3(重要)</a></li><li><a href="https://tensorflow.juejin.im/tutorials/image_retraining.html" target="_blank" rel="noopener">重新训练 Inception 最后一层并识别新的分类(非常重要)</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;近期有关迁移学习的一些重要的网页.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="迁移学习" scheme="http://yoursite.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>VGG16迁移学习分析</title>
    <link href="http://yoursite.com/2019/06/04/transfer-learning/"/>
    <id>http://yoursite.com/2019/06/04/transfer-learning/</id>
    <published>2019-06-04T05:42:10.000Z</published>
    <updated>2020-07-25T14:45:07.420Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>整理使用VGG16框架进行迁移学习的方法步骤.</strong><br><a id="more"></a></p></blockquote><h2 id="1-迁移学习简介"><a href="#1-迁移学习简介" class="headerlink" title="1. 迁移学习简介"></a>1. 迁移学习简介</h2><blockquote><p>深度学习中最强大的理念之一就是有时候神经网络可以从一个任务中学习到知识，并将这些知识应用到另一个独立的任务中。比如现在你训练好了一个神经网络能够识别像猫这样的对象，然后使用这些学习到的知识去帮助你更好的阅读<strong>X射线扫描图</strong>，这就是所谓的<strong>迁移学习</strong>。</p></blockquote><h2 id="2-VGG-Network介绍"><a href="#2-VGG-Network介绍" class="headerlink" title="2. VGG Network介绍"></a>2. VGG Network介绍</h2><blockquote><p><strong>「VGG Network」</strong>，牛津大学<strong>VGG</strong>实验室设计的架构，将<strong>AlexNet</strong>的$8$层提高到了$19$层，真正让深度这个词得以充分体现。从<strong>VGG</strong>开始，人们不再使用太大的卷积核，取而代之的是若干个小卷积核的组合。比如， $3$个步长为$1$的$3 \times 3$卷积核，可以拥有同$1$个$7 \times 7$的卷积核一样的感受野，但是，它可以使整个网络变得更深，并且更具有非线性。同时，还能够进一步减少训练的参数量。</p></blockquote><p>下图是各个著名网络的对比相关信息以及<strong>「VGG Network」</strong>的信息：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559623117786.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559623163942.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559623226922.png" alt="Alt text"></p><h2 id="3-迁移学习实际操作步骤"><a href="#3-迁移学习实际操作步骤" class="headerlink" title="3. 迁移学习实际操作步骤"></a>3. 迁移学习实际操作步骤</h2><p>首先，整个操作过程借鉴一下文章：</p><blockquote><ol><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-16-transfer-learning/" target="_blank" rel="noopener">迁移学习 Transfer Learning</a></li><li><a href="https://www.cnblogs.com/zengfanlin/p/8886701.html" target="_blank" rel="noopener">用tensorflow迁移学习猫狗分类</a></li></ol></blockquote><p>其中这里的得到的<strong>VGG pre-trained model</strong>是使用<a href="https://github.com/machrisaa/tensorflow-vgg" target="_blank" rel="noopener">machrisaa/tensorflow-vgg</a>改写的<strong>VGG16的代码</strong>以及在他的<strong>GitHub</strong>下提供的训练好了的<strong>model parameters</strong>，也就是<strong>VGG16 NPY</strong>文件，如果下载不了的话，可以通过一下网页以百度云的方式下载：<a href="https://www.jianshu.com/p/c9d06e794393" target="_blank" rel="noopener">简书——vgg16.npy 和vgg19.npy</a>。</p><p>权重文件下载完毕之后，建立相应的项目，在项目下建立一个文件夹来存放<strong>VGG16.npy</strong>文件：<img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559624035247.png" alt="Alt text"></p><p>数据文件在上面的网页<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-16-transfer-learning/" target="_blank" rel="noopener">迁移学习 Transfer Learning</a>中可以找到下载，<strong>VGG16</strong>是针对<strong>ImageNet</strong>的$1000$个类别的众多图片做<strong>分类</strong>任务进行训练的，在本次的迁移学习任务中，我们利用了图片分类的网络，在最后迁移到利用$CNN$做回归的任务中去，对于前面的卷积层(<strong>CNN Layers</strong>)依旧保留，而后面的<strong>Fully Connected Layers</strong>，对其进行修改，将用于分类$1000$个<strong>softmax</strong>层删掉，在网上下载那$1000$个分类数据中的猫和老虎的图片, 然后伪造一些猫和老虎长度的数据。最后做到让迁移后的网络分辨出猫和老虎的长度 (<strong>regressor</strong>)的<strong>回归任务</strong>。</p><p>而项目目录下的<strong>transfer_learning.py</strong>文件就是迁移学习的具体代码，我们这里对几个重要的点一一作出分析。</p><h3 id="3-1-固定卷积层-训练全连接"><a href="#3-1-固定卷积层-训练全连接" class="headerlink" title="3.1 固定卷积层 + 训练全连接"></a>3.1 固定卷积层 + 训练全连接</h3><blockquote><p><strong>首先，对于迁移学习需要注意几点</strong>：</p><ol><li>如果你的数据集很小，你可能只需要<strong>重新训练最后一层的权重</strong>，并<strong>保持前面的参数不动</strong>，经验规则就是如果你的新的数据集比较小，那么就只训练输出层的最后一两层，而数据很多的时候，也许你可以重新训练神经网络中所有的参数；</li><li>如果放射科的数据足够多，那么你可以重新训练神经网络剩下的所有层；</li><li>当你新的数据足够多的时候，你重新训练神经网络中所有的参数的时候，那么在图像识别数据进行的初期训练阶段被称之为<strong>预训练(pre-training)</strong>，因为你使用图像识别数据去训练的时候，是以数据量多的那个任务预训练好的参数初始化神经网络的权重，然后如果你以后更新所有权重然后在另一个数据量少的任务上进行训练，这个过程被称之为<strong>微调(fine-tuning)</strong>。</li></ol></blockquote><p>在这个将$1000$个类别进行分类的任务迁移到动物体长预测的回归任务中，回归任务的数据量不是很多，因此，我们在这里将最后一层修改为回归神经元，在训练的时候只训练最后一层的参数。</p><p>那么迁移学习实际在代码中对应着什么意思呢？通过分析代码我发现，整体是这样的：</p><blockquote><ol><li>首先对于下图中用红色矩形框出的代码：<img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559625253556.png" alt="Alt text"><br>红色框的代码是一个非常重要的点，需要明确的是，<strong>self.data_dict</strong>用于存放<strong>VGG16</strong>预训练过后得到的参数，它是一个字典类型，存放着一共有16个键值对。<strong>data_dict</strong>的<strong>keys</strong>就是对应的每一层的名字(从<strong>conv1</strong>一直到<strong>fc8</strong>)，相应的，<strong>value</strong>则是一个巨型列表(每个元素又是一个<strong>numpy</strong>数组)，每一个<strong>key</strong>对应的<strong>value</strong>都包含两个元素(包含那一层的<strong>weights</strong>和<strong>bias</strong>)，因此迁移就体现在前面整个卷积层的参数是采取固定不动的状态，从<strong>VGG16</strong>文件中读取的，不参与训练；而后面如果将全连接层进行修改，则可训练参数是后面的全连接层。<strong>需要注意的是，最后在训练完毕之后，需要进行预测的时候，不单单需要将预训练的VGG16的参数读取出来，还需要将后面修改了的，进行训练了的全连接层(依你具体改了的层而定)的参数一起导入才能够进行预测。(反正改了什么层去训练，预测的时候就要相应的导入什么层的训练参数)</strong>。</li><li>将<strong>VGG16</strong>预训练的参数导出存放在一个字典里之后，该如何将其分发到每一层呢？具体如下：<blockquote><p>2.1. 首先需要根据前面图片中<strong>VGG16</strong>的框架结构，在代码中将其框架结构复现出来(这里暂时不算池化层，对框架中的全连接层将其改为一层全联机层用于回归，卷积层不改变)；<br>2.2. 复现前面卷积层的框架之后，定义下面的封装好的卷积层和池化层代码：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559626059290.png" alt="Alt text"><br>2.3. 在复现框架的过程中，对于权重(<strong>Weights</strong>)和偏置(<strong>bias</strong>)，使用下面方法获取预训练完毕的结果：<img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559626162296.png" alt="Alt text">，也就是说，从前面保存参数的字典中按照每一层对应的层名，以字典方式获取预训练完毕的参数结果。<br>2.4. 而最后几层的全连接层，由于需要修改以进行迁移，因此对于修改了的层就不需要导入预训练好了的参数值了，而是以相应的初始化方式进行初始化，然后训练修改了的那几层的参数，<strong>由此可得，在迁移学习中根据数据量的大小，对想要的层进行修改之后，未修改的层的参数就带入预训练好了的参数，而修改了的层的参数就按照某种方式进行初始化，然后仅仅训练想要的那几层好了。</strong></p></blockquote></li><li>最后训练完毕之后，准备进行预测的时候，前面卷积层的参数依旧是按照上面的方式从存放所有预训练参数的字典中获取，而修改了的进行训练了的层则需要从训练完毕的参数文件中进行读取，这样才能进行预测：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559626588927.png" alt="Alt text"></li></ol></blockquote><h3 id="3-2-VGG16各层参数的shape以及数目"><a href="#3-2-VGG16各层参数的shape以及数目" class="headerlink" title="3.2 VGG16各层参数的shape以及数目"></a>3.2 VGG16各层参数的shape以及数目</h3><pre><code>————————————————————————————————————————————————————————————Layer                   Output Shape               Param #============================================================conv1_1_weights         (3, 3, 3, 64)              1728conv1_1_bias            (64,)                      64————————————————————————————————————————————————————————————conv1_2_weights         (3, 3, 64, 64)             36864conv1_2_bias            (64,)                      64————————————————————————————————————————————————————————————conv2_1_weights         (3, 3, 64, 128)            73728conv2_1_bias            (128,)                     128————————————————————————————————————————————————————————————conv2_2_weights         (3, 3, 128, 128)           147456conv2_2_bias            (128,)                     128————————————————————————————————————————————————————————————conv3_1_weights         (3, 3, 128, 256)           294912conv3_1_bias            (256,)                     256————————————————————————————————————————————————————————————conv3_2_weights         (3, 3, 256, 256)           589824conv3_2_bias            (256,)                     256————————————————————————————————————————————————————————————conv3_3_weights         (3, 3, 256, 256)           589824conv3_3_bias            (256,)                     256————————————————————————————————————————————————————————————conv4_1_weights         (3, 3, 256, 512)           1179648conv4_1_bias            (512,)                     512————————————————————————————————————————————————————————————conv4_2_weights         (3, 3, 512, 512)           2359296conv4_2_bias            (512,)                     512————————————————————————————————————————————————————————————conv4_3_weights         (3, 3, 512, 512)           2359296conv4_3_bias            (512,)                     512————————————————————————————————————————————————————————————conv5_1_weights         (3, 3, 512, 512)           2359296conv5_1_bias            (512,)                     512————————————————————————————————————————————————————————————conv5_2_weights         (3, 3, 512, 512)           2359296conv5_2_bias            (512,)                     512————————————————————————————————————————————————————————————conv5_3_weights         (3, 3, 512, 512)           2359296conv5_3_bias            (512,)                     512————————————————————————————————————————————————————————————fc6_weights             (25088, 4096)              102760448fc6_bias                (4096,)                    4096————————————————————————————————————————————————————————————fc7_weights             (4096, 4096)               16777216fc7_bias                (4096,)                    4096————————————————————————————————————————————————————————————fc8_weights             (4096, 1000)               4096000fc8_bias                (1000,)                    1000————————————————————————————————————————————————————————————============================================================Total params: 138357544</code></pre><p>由此可见，<strong>vgg16.npy</strong>文件很大不是没有原因的。</p><h2 id="4-如何固定住参数"><a href="#4-如何固定住参数" class="headerlink" title="4. 如何固定住参数"></a>4. 如何固定住参数</h2><blockquote><p>首先需要明确一点的是，<strong>tf.Optimizer</strong>只优化<strong>tf.GraphKeys.TRAINABLE_VARIABLES</strong>中的变量，也就是说在进行<strong>Backpropagation</strong>参数更新的，只会回传梯度到<strong>Variable(变量)</strong>上去，常量是被固定住了，不会被更新的(<a href="https://blog.csdn.net/touch_dream/article/details/78446863" target="_blank" rel="noopener">tf常用集合及其获取方式</a>)。<br>因此在<strong>迁移学习</strong>中对于很多预训练了的框架，想要在迁移的时候对部分参数进行固定以避免训练的话，可以通过使用<strong>tf.constant</strong>或者<strong>python</strong>变量的形式来规避常量被训练。例如：<a href="https://blog.csdn.net/xys430381_1/article/details/88885580" target="_blank" rel="noopener">tensorflow —-迁移学习中如何只更新部分参数</a>，在这篇博客里详细讨论了如何固定部分参数，只训练另一部分参数的方法技巧。</p><hr><p>这里再附上一些相关的参考博客：</p><ol><li><a href="https://blog.csdn.net/grllery/article/details/80380332" target="_blank" rel="noopener">迁移学习之—tensorflow选择性加载权重</a></li><li><a href="https://blog.csdn.net/u014381600/article/details/71511794" target="_blank" rel="noopener">迁移学习技巧以及如何更好的finetune 模型(重点)</a></li><li><a href="https://www.jianshu.com/p/07426ba049b6" target="_blank" rel="noopener">简书——迁移学习总结</a></li><li><a href="https://blog.csdn.net/LoseInVain/article/details/78935157" target="_blank" rel="noopener">利用numpy数组保存TensorFlow模型的参数</a></li><li><a href="https://blog.csdn.net/qq_40108803/article/details/83627532" target="_blank" rel="noopener">读取VGG16网络生成的.npy文件的参数</a></li><li><a href="https://blog.csdn.net/qq_40614981/article/details/81001971" target="_blank" rel="noopener">模型保存文件.npy</a></li><li><a href="https://codeday.me/bug/20190401/856668.html" target="_blank" rel="noopener">python – Tensorflow：tf.get_collection不返回范围中的变量</a></li><li><a href="https://blog.csdn.net/shwan_ma/article/details/78879620" target="_blank" rel="noopener">[tensorflow]打印Tensorflow graph中的所有需要训练的变量—tf.trainable_variables()</a></li><li><a href="http://www.voidcn.com/article/p-mepbcmhk-bgt.html" target="_blank" rel="noopener">tensorflow常用函数及概念</a></li></ol></blockquote><p>在本代码中，由于卷积层是直接使用的<strong>VGG16</strong>的模型框架，注入的参数也是直接使用的是<strong>.npy</strong>文件导出的数据进行初始化的，而不是使用的<strong>TensorFlow</strong>中<strong>Varibale的方式初始化的</strong>。因此在之后训练的时候只会更新后改了的全连接层的参数(修改了的全连接层的参数通过<strong>Varibale的方式初始化的</strong>)，而前面的都是<strong>常量(constant)</strong>。这一点可以通过<code>tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))</code>的方式看出：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559675934438.png" alt="Alt text"><br>打印出来的结果如下：</p><pre><code>[ &lt; tf.Variable &#39;fc6/kernel:0&#39; shape = (25088, 256) dtype = float32_ref &gt;,  &lt; tf.Variable &#39;fc6/bias:0&#39; shape = (256,) dtype = float32_ref &gt;,  &lt; tf.Variable &#39;out/kernel:0&#39; shape = (256, 1) dtype = float32_ref &gt;,  &lt; tf.Variable &#39;out/bias:0&#39; shape = (1,) dtype = float32_ref &gt;]</code></pre><p>可以看出<strong>Variable</strong>变量只有后面修改了的全连接层的参数，前面的卷积层的参数都是常量，不参与到参数更新，因此也就被固定住了，不参与到训练过程中。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;整理使用VGG16框架进行迁移学习的方法步骤.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="迁移学习" scheme="http://yoursite.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="VGG16" scheme="http://yoursite.com/tags/VGG16/"/>
    
  </entry>
  
  <entry>
    <title>预测代码中遇到的问题</title>
    <link href="http://yoursite.com/2019/05/30/tensorflow-save-and-restore/"/>
    <id>http://yoursite.com/2019/05/30/tensorflow-save-and-restore/</id>
    <published>2019-05-30T00:45:54.000Z</published>
    <updated>2020-07-25T14:48:42.839Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>记录预测代码模块化过程中遇到的问题.</strong><br><a id="more"></a></p></blockquote><h2 id="1-TensorFlow模型的保存和提取"><a href="#1-TensorFlow模型的保存和提取" class="headerlink" title="1. TensorFlow模型的保存和提取"></a>1. TensorFlow模型的保存和提取</h2><h3 id="1-1-模型保存"><a href="#1-1-模型保存" class="headerlink" title="1.1 模型保存"></a>1.1 模型保存</h3><blockquote><pre><code>saver = tf.train.Saver(max_to_keep=num, var_list=var_list)saver.save(sess, model_save_path, global_step=i)</code></pre><p>需要注意的是<strong>max_to_keep</strong>用于指定需要保存的模型个数，<strong>var_list</strong>指定要保存和恢复的变量，如果<code>saver=tf.train.Saver()</code>里面不传入参数，默认保存全部变量。在模型运行运行结束后会生成<strong>4</strong>个文件，分别是：</p><ol><li><strong>checkpoint</strong>文件保存了一个目录下所有的模型文件列表，这个文件是<code>tf.train.Saver</code>类自动生成且自动维护的，它是一个二进制文件，包含了weights, biases, gradients和其他variables的值。但是0.11版本后的都修改了，用.data和.index保存值，用checkpoint记录最新的记录。；</li><li><strong>meta</strong>文件保存了<strong>TensorFlow</strong>计算图的结构，可以理解为神经网络的网络结构，<strong>TensorFlow</strong>通过元图（<strong>MetaGraph</strong>）来记录计算图中节点的信息以及运行计算图中节点所需要的元数据，它一个协议缓冲，保存<strong>tensorflow</strong>中完整的<strong>graph</strong>、<strong>variables</strong>、<strong>operation</strong>、<strong>collection</strong>。；</li><li><strong>data-00000-of-00001</strong>文件保存了<strong>TensorFlow</strong>程序中每一个变量的取值，这个文件是通过<strong>SSTable</strong>格式存储的，可以大致理解为就是一个（<strong>key，value</strong>）列表；</li><li><strong>index</strong>是对应模型的索引文件；</li><li>如果想设置每多长时间保存一次，可以设置<code>saver = tf.train.Saver(keep_checkpoint_every_n_hours=2)</code>，这个是每<strong>2</strong>个小时保存一次；</li><li>如果不想保存所有变量，可以在创建<strong>saver</strong>实例时，指定保存的变量，可以以<strong>list</strong>或者<strong>dict</strong>的类型保存，如下：<pre><code>w1 = tf.Variable(tf.random_normal(shape=[2]), name=&#39;w1&#39;)w2 = tf.Variable(tf.random_normal(shape=[5]), name=&#39;w2&#39;)saver = tf.train.Saver([w1,w2])</code></pre><h3 id="1-2-模型提取"><a href="#1-2-模型提取" class="headerlink" title="1.2 模型提取"></a>1.2 模型提取</h3></li></ol></blockquote><pre><code>&#39;&#39;&#39;在一个新的python脚本文件中&#39;&#39;&#39;import tensorflow as tf&#39;&#39;&#39;导入其他库&#39;&#39;&#39;pass&#39;&#39;&#39;其他数据准备工作&#39;&#39;&#39;&#39;&#39;&#39;这里不需要重新搭建模型&#39;&#39;&#39;&#39;&#39;&#39;提取模型，首先提取计算图，这一步相当于搭建模型&#39;&#39;&#39;saver = tf.train.import_meta_graph(&quot;model/mnist.ann-10000.meta&quot;)with tf.Session() as sess:    &#39;&#39;&#39;提取保存好的模型参数&#39;&#39;&#39;    &#39;&#39;&#39;这里注意模型参数文件名要丢弃后缀.data-00000-of-00001&#39;&#39;&#39;    saver.restore(sess, &quot;model/mnist.ann-10000&quot;)    &#39;&#39;&#39;通过张量名获取张量&#39;&#39;&#39;    &#39;&#39;&#39;这里按张量名获取了我保存的一个模型的三个张量，并换上新的名字&#39;&#39;&#39;    new_x = tf.get_default_graph().get_tensor_by_name(&quot;x:0&quot;)    new_y = tf.get_default_graph().get_tensor_by_name(&quot;y:0&quot;)    new_y_ = tf.get_default_graph().get_tensor_by_name(&quot;y_:0&quot;)    &#39;&#39;&#39;现在可以进行计算了&#39;&#39;&#39;    y_1 = sess.run(new_y_, feed_dict={new_x: new_x_data, new_y: new_y_data})print(y_1)</code></pre><h3 id="1-3-从断点处继续训练模型"><a href="#1-3-从断点处继续训练模型" class="headerlink" title="1.3 从断点处继续训练模型"></a>1.3 从断点处继续训练模型</h3><pre><code>with tf.Session() as sess:sess.run(tf.global_variables_initializer())saver = tf.train.Saver()ckpt = tf.train.get_checkpoint_state(os.path.dirname(&#39;checkpoints_path&#39;))# 如果checkpoint存在则加载断点之前的训练模型if ckpt and ckpt.model_checkpoint_path:    saver.restore(sess, ckpt.model_checkpoint_path)</code></pre><h3 id="1-4-模型结构是否需要重新定义"><a href="#1-4-模型结构是否需要重新定义" class="headerlink" title="1.4 模型结构是否需要重新定义"></a>1.4 模型结构是否需要重新定义</h3><blockquote><p>当我们将模型的预测部分与训练部分进行分裂，各自模块化之后，如果想要进行预测的时候，有两种方式：</p><ol><li><strong>重新定义网络结构: </strong> <pre><code>saver.restore(sess,&quot;save/model.ckpt&quot;)</code></pre>这种方法不方便的在于，在使用模型进行预测的时候，必须把模型的网络结构重新定义一遍，然后载入对应名字的变量的值。但是很多时候我们都更希望能够读取一个文件然后就直接使用模型，而不是还要把模型的网络结构(<strong>Inference</strong>)重新定义一遍。所以就需要使用另一种方法。</li><li><strong>不需重新定义网络结构的方法: </strong><pre><code>def load_model(): with tf.Session() as sess:     saver = tf.train.import_meta_graph(&#39;model/my-model-290.meta&#39;)     saver.restore(sess, tf.train.latest_checkpoint(&quot;model/&quot;))</code></pre>首先<strong>import_meta_graph</strong>，这里填的名字是<strong>meta</strong>文件的名字。然后<strong>restore</strong>时，是检查<strong>checkpoint</strong>，所以只填到<strong>checkpoint</strong>所在的路径下即可，不需要填<strong>checkpoint</strong>，不然会报错<strong>“ValueError: Can’t load save_path when it is None.”</strong>。这个方法可以从文件中将保存的<strong>graph</strong>的所有节点加载到当前的<strong>default graph</strong>中，并返回一个<strong>saver</strong>。也就是说，我们在保存的时候，除了将变量的值保存下来，其实还有将对应<strong>graph</strong>中的各种节点保存下来，所以模型的结构也同样被保存下来了。</li></ol></blockquote><h3 id="1-5-相关网址"><a href="#1-5-相关网址" class="headerlink" title="1.5 相关网址"></a>1.5 相关网址</h3><blockquote><ol><li><a href="https://blog.csdn.net/liuxiao214/article/details/79048136" target="_blank" rel="noopener">【tensorflow】保存模型、再次加载模型等操作</a></li><li><a href="https://www.jianshu.com/p/52e7ae04cecf" target="_blank" rel="noopener">TensorFlow模型保存/载入的两种方法</a></li><li><a href="https://blog.csdn.net/shu15121856/article/details/85063043" target="_blank" rel="noopener">【TensorFlow学习笔记】5：variable_scope和name_scope,图的基本操作</a></li><li><a href="https://www.jianshu.com/p/c3a7f5c47b83" target="_blank" rel="noopener">TensorFlow：保存和提取模型</a></li><li><a href="https://www.jianshu.com/p/cb48e8e91d96" target="_blank" rel="noopener">2018-06-25《TensorFlow模型保存、提取、预测》</a></li><li><a href="https://blog.csdn.net/leo_xu06/article/details/79200634" target="_blank" rel="noopener">Tensorflow在不同训练场景下读取和使用不同格式pretrained model的方法</a></li><li><a href="https://blog.csdn.net/spylyt/article/details/71601174" target="_blank" rel="noopener">tensorflow 模型保存与加载</a></li><li><a href="https://blog.csdn.net/mieleizhi0522/article/details/80535189" target="_blank" rel="noopener">Tensorflow中保存与恢复模型tf.train.Saver类讲解（恢复部分模型参数的方法）</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;记录预测代码模块化过程中遇到的问题.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>目标检测算法(2)</title>
    <link href="http://yoursite.com/2019/05/27/object-detection-1/"/>
    <id>http://yoursite.com/2019/05/27/object-detection-1/</id>
    <published>2019-05-27T09:29:37.000Z</published>
    <updated>2020-07-25T14:50:03.268Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>详细记录目标检测算法整个过程的原理.</strong><br><a id="more"></a></p></blockquote><h3 id="1-5-Bounding-Box-预测"><a href="#1-5-Bounding-Box-预测" class="headerlink" title="1.5 Bounding Box 预测"></a>1.5 Bounding Box 预测</h3><blockquote><p>卷积的滑动窗口实现效率很高，但是存在输出边界框不精确的问题。</p></blockquote><p>在滑动窗算法中，我们取这些离散的位置集合，然后在它们上面跑分类器。在这种情况下很有可能没有一个能完美匹配汽车位置的边界框，而真正完美的边界框甚至都是有点长方形的。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558920463424.png" alt="Alt text"></p><p>其中一个能够得到更加精确的边界框的算法是$YOLO$，它的做法如下：</p><blockquote><p>首先，假设你的输入图片为$100 \times 100$，然后在图片上放入网格(这里假设使用的是$3 \times 3$的网格，实际在使用的时候会用更加精细的网格)。基本思路是<strong>使用图像分类和定位算法</strong>，然后将相应的算法逐一应用到这$9$个格子上。<br>更加具体一点的就是，你需要定义训练标签，所以对于$9$个格子中的每一个，指定一个标签$y$，其中$y$是和以前一样的$8$维向量：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} p_c \\ b_x \\ b_y \\ b_h \\b_w \\c_1 \\ c_2 \\ c_3\end{bmatrix}</script><p>其中$(b_x, b_y, b_h, b_w)$代表如果那个格子有对象，那么就给出边界框(红色)的坐标。因此对于下面的图像有$9$个格子，则会有$9$个长度一样的标签向量。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558921343655.png" alt="Alt text"><br>以上图中左上角的紫色包围的格子为例，由于里面什么都没有，因此：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} 0 \\ ? \\ ? \\ ? \\ ? \\ ? \\ ? \\ ? \end{bmatrix}</script><hr><p>上图中第二行的第二个格子同时包含两量车的一部分，对于$YOLO$算法来说，要做的就是，取两个对象的重点，然后将各自的对象分配给包含对象中点的格子(这里这个格子指的是<strong>YOLO</strong>算法中将图像划分成很多个格子的那个格子，而不是指的边界框)。因此右边那辆车就分配给第二行第三个格子了。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558921836899.png" alt="Alt text"></p></blockquote><p>对于上图中绿色框定的格子的标签值为:</p><script type="math/tex; mode=display">y =  \begin{bmatrix} 1\\ b_x \\ b_y \\ b_h \\b_w \\0\\ 1\\ 0\end{bmatrix}</script><p>因此对于每一个格子都是一个$8$维的输出，而整个图片是$3 \times 3$，因此，最后总的输出为$3 \times 3 \times 8$。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558922138471.png" alt="Alt text"></p><p>如果你现在要训练一个输入为$100 \times 100 \times 3$的神经网络，然后通过一个普通的卷积网络，和选定的各个池化层等等… …最后映射到一个$3 \times 3 \times 8$的输出尺寸(这个应该是根据你一开始就将输入图片选定要生成多少网格就可以得到)，当你使用反向传播进行训练的时候，将任意输入$x$映射到这类输出向量$y$，<strong>这个算法的好处在于神经网络可以输出精确的边界框</strong>。只是需要注意的是只要每个格子的对象数目没有超过$1$个，那这个算法就是$ok$的，至于超过一个需要怎么处理，后面再讲。</p><p>另一个需要注意的是，例子中我们采用的是$3 \times 3$的网格，而在实践中我们可能会采用更精细的$19 \times 19$，所以得到的输出为$19 \times 19 \times 8$，那个时候同一个格子被分配多个对象的概率就会小得多。</p><p>再次强调的是: $YOLO$算法把对象分配到一个格子的过程是，观察对象的重点，然后将这个对象分配到其中点所在的格子，所以一个对象即使横跨了多个格子，也只会被分配到$9$个格子中的一个，而在越精细的格子数目划分中，两个对象的中点处于同一个格子的概率就会更低。$YOLO$算法能够让神经网络输出边界框可以具有任意的宽高比，并且能够输出更加精确的坐标，而又避免了滑动窗口分类器的步长大小限制；其次，这是一个卷积过程，算法并没有在$3 \times 3$的网格上跑$9$次算法，相反，这是单次卷积实现的，因为使用了卷积，使得很多过程共享计算步骤，所以这个算法效率很高，实际上它由于是卷积过程，它的运行速度非常快，可以达到事实识别。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558923754768.png" alt="Alt text"></p><hr><p>如果编码标签中的边界框的位置$(b_x, b_y, b_h, b_w)$？<br>还是以上面的图为例，网格的大小设定为$3 \times 3$，我们以第二行第三个格子(面包车)为例，其标签为：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} 1\\ b_x \\ b_y \\ b_h \\b_w \\0\\ 1\\ 0\end{bmatrix}</script><p>在$YOLO$算法中对于黄色的框，我们取它左上角的蓝色点为原点$(0, 0)$，而去黄色框的右下角的蓝色点为$(1,1)$(这与滑动窗口卷积中取整个图像左上角为原点$(0, 0)$，整个图片左下角为$(1, 1)$的方式是不同的)，所以要指定黄色中点的位置，对于$(b_x, b_y)$大体是$(0.4, 0.3)$，而对于$(b_h, b_w)$则红色框相较于格子的比例大小，这个大概为$(0.9, 0.5)$。换句话说，$(b_x, b_y, b_h, b_w)$是相对于格子尺度的比例大小。所以$(b_x, b_y)$必须在$(0,1)$之间，都在它就该被分配到别的格子了，而边界框的大小时可能是大于$1$的。</p><hr><p>所以最后总结一下:</p><blockquote><ol><li><p>滑动窗口的方式是选取固定大小的窗口进行滑动检测，因此会存在边界框不准确的情况，因为物体有大有小， 所以这种方式无法的到准确的位置；</p></li><li><p>而$YOLO$算法则不同，采用的是<strong>图像分类+目标定位+划分网格</strong>的方式去做的，而目标定位的特点就是得到的输出结果的标签中是包含物体的边界框参数的，这个东西会根据物体的大小和形状，得到不同的长方形或者正方形，所以说$YOLO$算法是能够得到物体精确的位置的，除此之外，$YOLO$通过划分网格的方式，直接一次卷积就可以得到相应的结果，而不是重复的让自己设定的滑动窗按照给定的步长滑过图像，这样计算就会非常的快速了，而相较于滑动窗口的卷积处理而言，虽然滑动窗口的卷积处理而言也是很快的，避免了大量重复计算，但是实际上的结果也是相当于取固定大小的边界框和固定大小的步长，只不过相比较于滑动窗口不断滑动的多次重复计算而言，滑动窗口的卷积处理通过全连接改动为卷积的神操作，一次卷积就达到了滑动窗口算法的效果<strong>(滑动窗口在整幅图片上进行滑动卷积的操作过程，就等同于在该图片上直接进行卷积运算的过程)</strong>，只不过还是那个致命问题：不管是固定的边界框和固定的步长还是滑动窗口的卷积操作中最后效果中相当于固定的边界框和固定的步长，都出在固定上了，这就造成了它们的输出结果不能得到物体的精确边界框。</p></li><li>因此实际上$YOLO$算法的网格划分是类似滑动窗口的卷积处理的，这也就使得$YOLO$一次卷积就$ok$了，而不是依据网格多少就跑多少次卷积的方式，那样就和滑动窗口一样了，另一点，需要注意的是，$YOLO$的网格划分其实类似于滑动窗口中的固定边界框大小，但是这种固定边界框大小的做法得到的结果是比较不精确的，所以$YOLO$再结合<strong>目标定位算法</strong>，这样再在固定大小的边界框(网格)中获得精确定位的边界框参数，所以才能有好的效果。</li><li><strong>YOLO</strong>显式地输出边界框，使得其可以具有任意宽高比，并且能输出更精确的坐标，不受滑动窗口算法滑动步幅大小的限制；</li><li><strong>YOLO</strong>是一次卷积实现，并不是在$n\times n $网格上进行$n^{2}$次运算，而是单次卷积实现，算法实现效率高，运行速度快，可以实现实时识别。</li></ol></blockquote><h3 id="1-6-交并比-Intersection-over-union"><a href="#1-6-交并比-Intersection-over-union" class="headerlink" title="1.6 交并比(Intersection over union)"></a>1.6 交并比(Intersection over union)</h3><blockquote><p><strong>交并比(Intersection over union):</strong> 用于评价对象算法，从而进一步改善对象检测算法，它通过计算两个边界框的交集和并集得到。</p></blockquote><p>在下图中，包围汽车的红色边界框代表实际的边界框，而结果你的算法给出了下面的紫色边框，那么这个结果是好呢还是坏呢？</p><blockquote><p><strong>Intersection over union: </strong></p><script type="math/tex; mode=display">IOU = \frac{intersection}{union}</script><p>对于上面的计算，一般约定，在计算机检测任务中：</p><script type="math/tex; mode=display">f(n)= \begin{cases} correct, & \text {if IOU $\geq$ 0.5} \\  \end{cases}</script><p>因此$IOU \geq 0.5$是一个阈值，用来判断预测的边界框是否正确。</p></blockquote><h3 id="1-7-非极大值抑制-Non-max-suppression"><a href="#1-7-非极大值抑制-Non-max-suppression" class="headerlink" title="1.7 非极大值抑制(Non-max suppression)"></a>1.7 非极大值抑制(Non-max suppression)</h3><blockquote><p>对象检测算法的一个可能的问题就是同一个对象被算法做出多次检测。而非极大值抑制可能帮助你的算法对每个对象只检测一次。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558929523282.png" alt="Alt text"><br>假设现在你要在这张图片中检测行人和汽车，而现在你在图片上放$19 \times 19$的网格，那么对于不同的格子都是用<strong>图像分类+目标定位算法</strong>时，对于一辆车所占领的不同格子，都可能会觉得它们的格子里有车，<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558929740177.png" alt="Alt text"><br>因为划分为$n \times n$个网格，每个网格都会使用<strong>图像分类+目标定位算法</strong>，因此，很多格子可能都会举手说，我这个格子里有车的概率很高。因此当算法运行完毕之后，最后可能会对同一个对象做出多次检测，所以<strong>非极大值抑制</strong>要做的就是清理这些检测结果，使得每个物体仅被检测一次。</p></blockquote><p>首先这个算法每次会查看每个检查结果相关的概率$p_c \times c_1/c_2/c_3$，在下图中首先看右边那个最大的概率$0.9$，然后<strong>非极大值</strong>抑制就会逐一审视剩下的矩形：<strong>所有和这个最大的边界框有很高交并比、高度重叠的其他边界框</strong>，然后这些输出就会被抑制，所以概率显示为$0.6$和$0.7$的两个框就被抑制了，然后颜色变暗。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558937531478.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558937549748.png" alt="Alt text"></p><p>接下来，逐一审视剩下的矩形，找出概率最高的$p_c$最高的一个，这种情况下是$0.8$，我们就认为这里检测出了一辆车，然后<strong>非最大值抑制算法</strong>就会去掉其他$IOU$值很高的矩形(也就是其他和$0.8$重叠程度很高的矩阵)。所以现在每个矩形都会被高亮显示或者变暗，所以如果直接抛弃变暗的矩形，那就剩下高亮显示的那些矩形，这就是最后得到的两个预测结果：<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558938603264.png" alt="Alt text"><br>所以<strong>非最大值抑制</strong>意味着你只输出概率最大的分类结果，而抑制很接近，但是不是最大的其他预测结果。</p><hr><p><strong>非极大值抑制</strong>算法的细节如下：</p><blockquote><ol><li>首先假设网格为$19 \times 19$，然后在各个网格上跑一下算法，然后得到尺寸大小为$19 \times 19 \times 8$的输出结果。</li><li>要实现<strong>非极大值抑制</strong>，第一件事是去掉所有预测概率低于某个阈值的边界框，这里假设$p_c \leq 0.6$的框被去掉。</li><li>接下来剩下的边界框(没有被抛弃、没有被处理过的)，你就<strong>一直选择</strong>概率$p_c$最高的边界框，然后把它输出成预测结果。</li><li>接下来去掉所有剩下的边界框：把这些和输出边界框(概率最高的边界框)有很高的重叠面积，也就是和上一步输出边界框有很高的交并比的边界框全部抛弃。</li><li>循环上面的步骤，直到每个边界框都判断过了，它们有的作为输出结果，剩下的会被抛弃。</li><li>上面仅是介绍图片中只有一个类别物体时的情况，如果图片中有多个不同类别的物体时，则相应的独立进行多次非最大值抑制，每次对每个输出类别做一次。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558939655151.png" alt="Alt text"></li></ol></blockquote><h3 id="1-8-Anchor-boxes"><a href="#1-8-Anchor-boxes" class="headerlink" title="1.8 Anchor boxes"></a>1.8 Anchor boxes</h3><blockquote><p>到目前为止，对象检测中存在的一个问题是：每个格子只能检测一个对象，如果你想让一个格子检测出多个对象，则可以使用<strong>Anchor Boxes</strong>。</p></blockquote><p><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558940177091.png" alt="Alt text"><br>以上图中的例子为例，在图中我们依旧使用的网格大小为$3 \times 3$，从图中我们能够发现一个非常明显的特点：<strong>行人的中点和汽车的中点基本是落在了同一点，也就是行人和汽车的中点都落在了同一个格子里。</strong>又由于我们的算法中涉及图像分类的算法，此时两者落与同一个格子，那么在那个格子的标签值$y$将无法输出检测结果(我们前面都是设定的<strong>多类单标签</strong>预测)。</p><p>而<strong>anchor boxes</strong>的思路是这样的：</p><blockquote><ol><li>预先定义两个不同形状的<strong>anchor boxes</strong>:<img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558940149829.png" alt="Alt text"></li><li>把预测结果和这两个<strong>anchor boxes</strong>关联起来，一般来说，你可能会用到更多的<strong>anchor boxes</strong>(可能$5$个甚至更多)。</li><li>定义类别标签：<script type="math/tex; mode=display">y =  \begin{bmatrix} p_{c1}\\ b_{x1} \\ b_{y1} \\ ...\\p_{c2} \\b_{x2}\\ ...\\ c_3\end{bmatrix}</script>此时的输出维度为$16$维。上面的$y$的各个分量是对应<strong>anchor box</strong>的分量。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558940557063.png" alt="Alt text"></li></ol></blockquote><p>由于存在<strong>anchor boxes</strong>这个概念，现在需要做的是：现在每个对象都和以前一样分配到同一个格子中，分配到对象中点所在的格子中。除此之外，它还被分配到一个和对象形状(应该就是边界框)交并比最高的<strong>anchor boxes</strong>中：假设你的对象形状如下(应该指的就是对象边界框)，然后你就看看你设定的两个<strong>anchor boxes</strong>哪个和实际的边界框的交并比最高，当然 ，不管分配哪一个<strong>anchor boxes</strong>，每个对象都不只分配到一个格子，而是分配到<strong>(网格，anchor boxes)</strong>。除此之外，标签$y$的维度也会变化，如果对象很多，那么$y$的维度也会很多。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558941300847.png" alt="Alt text"></p><p>具体例子如下：<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558941608992.png" alt="Alt text"></p><p>需要注意的是：</p><blockquote><ol><li>假设你现在有两个<strong>anchor boxes</strong>，但是在同一个格子中有三个对象出现，这种情况算法处理不好，此时并没有很好的处理办法；</li><li>当两个对象分配到一个格子中，但是它们的<strong>anchor boxes</strong>形状一样，这是算法处理不好的另一种情况；除非此时引入一些打破僵局的默认手段用于专门处理这些情况。</li></ol></blockquote><hr><p><strong>Summary：</strong></p><blockquote><p>我们建立<strong>anchor boxes</strong>这个概念，是为了处理两个对象出现同一个格子的情况，实践中这种情况很少发生，特别是当你使用的网格大小是$19 \times 19$，两个对象中心处于$361$个格子中同一个格子的概率很低。也许设立<strong>anchor boxes</strong>的好处在于<strong>anchor boxes</strong>能够使你的学习算法，能够更有针对性，特别是如果你的数据集，有一些很高很瘦的对象，比如说：行人。这样让你的算法能够更有针对性。</p></blockquote><p><strong>Anchor box</strong> 的选择：</p><blockquote><ol><li>一般人工指定<strong>Anchor box</strong> 的形状，选择<strong>5~10</strong>个以覆盖到多种不同的形状，可以涵盖我们想要检测的对象的形状；</li><li>高级方法：<strong>K-means</strong> 算法：将不同对象形状进行聚类，用聚类后的结果来选择一组最具代表性的<strong>Anchor box</strong>，以此来代表我们想要检测对象的形状。</li></ol></blockquote><h3 id="1-9-YOLO算法"><a href="#1-9-YOLO算法" class="headerlink" title="1.9 YOLO算法"></a>1.9 YOLO算法</h3><blockquote><ol><li><strong>构造训练集：假设你有两个anchor boxes</strong><br>1.1 标签类别: 行人、汽车、摩托、背景<br>1.2 标签维度：$3 \times 3 \times 2 \times 8$，其中$3 \times 3$是因为选择的网格大小如此，然后$\times 2$代表有两个<strong>anchor boxes</strong>，$\times 8$是因为一个<strong>anchor boxes</strong>所对应的参数量。<br>要构造训练集，需要遍历$9$个格子，然后构成对应的目标向量$y$。对于下面的图可以看出，红色框是边界框，<strong>anchor boxes2</strong>和边界框的交并比要更大一下，那么车子就和向量的下半部分有关了。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558946053778.png" alt="Alt text"></li><li><strong>模型预测：</strong><br>输入与训练集中相同大小的图片，同时得到每个格子中不同的输出结果：$3\times3\times2\times8$ 。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558946335028.png" alt="Alt text"></li><li><strong>运行非极大值抑制(NMS)</strong>：<br>3.1 假设使用了<strong>2</strong>个<strong>Anchor box</strong>，那么对于每一个网格，我们都会得到预测输出的<strong>2</strong>个<strong>bounding boxes</strong>，其中一个 $P_{c}$比较高；<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558946471037.png" alt="Alt text"><br>3.2 需要注意的是：有些边界框是可以超过所在格子的高度和宽度的；<br>3.3 接着，抛弃概率$P_{c}$值低的预测<strong>bounding boxes</strong>；<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558946807709.png" alt="Alt text"><br>3.4 最后，如果你有三个对象检测类别，那么对于每一个类别，单独运行<strong>非最大值抑制</strong>去处理预测结果是对应类别的边界框。<br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558946818419.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/05/27/object-detection-1/1558946885008.png" alt="Alt text"></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;详细记录目标检测算法整个过程的原理.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>目标检测算法(1)</title>
    <link href="http://yoursite.com/2019/05/26/object-detection/"/>
    <id>http://yoursite.com/2019/05/26/object-detection/</id>
    <published>2019-05-26T15:43:58.000Z</published>
    <updated>2020-07-25T14:50:44.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>详细记录目标检测算法整个过程的原理.</strong><br><a id="more"></a></p></blockquote><h3 id="1-1-目标定位-Object-Localization"><a href="#1-1-目标定位-Object-Localization" class="headerlink" title="1.1 目标定位(Object Localization)"></a>1.1 目标定位(Object Localization)</h3><blockquote><ol><li><strong>Image Classification(图像分类)</strong></li><li><strong>Classification with Localization(定位分类): </strong>判断图像中是否为某一物体(<strong>分类</strong>)，还要在图片中标记处它的位置(<strong>定位</strong>)，用边框把物体圈出来。<strong>定位分类</strong>问题通常是<strong>图像中只有一个较大的对象，且对象位于图片中间位置。</strong></li><li><strong>Detection(检测):</strong>当图片中有多个对象的时候(<strong>例如自动驾驶中需要检测出汽车、行人以及交通灯等多个对象</strong>)，需要将它们的类别检测出来，还要分别都确定其位置。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558873247390.png" alt="Alt text"></li></ol></blockquote><hr><p>对于<strong>定位分类</strong>问题来说，首先可以明确地是它是<strong>多类单标签</strong>问题，也就是说在图像中它仅有一个物体，这个物体的标签仅有一个，最后的<strong>softmax</strong>层输出最有可能的一个类别概率。</p><p>那么在<strong>分类</strong>基础之上，想要通过网络最后输出除了物体的<strong>类别</strong>之外，还包含物体的<strong>位置(定位)</strong>应该怎么办呢？方法是让神经网络除了仅有的一个<strong>softmax</strong>输出单元之外，再多输出几个单元，这几个单元就是物体的<strong>边界框(bounding box)</strong>。这里具体的就是让神经网络再多输出$4$个数字，分别标记为$(b_x, b_y, b_h, b_w)$，这四个数字就是<strong>bounding box</strong>的参数化。</p><p>这里需要先说明一下基本的设定，对于图像的坐标，我们设定图像的左上角为$(0, 0)$，而右下角的坐标为$(1, 1)$。想要确定<strong>边界框(bounding box)</strong>的具体位置，则需要指定框住了物体的边界框的中心点的坐标为$(b_x, b_y)$，而边界框的高度为$b_h$，宽度为$b_w$。</p><p>因此有上面可知，训练集中不仅包含对象的分类标签，还需要包含表示边界框的这四个数字$(b_x, b_y, b_h, b_w)$。然后将数据输入网络，训练完毕，给出被检测物体的<strong>类别标签</strong>以及想要的<strong>边界框位置</strong>。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558874270553.png" alt="Alt text"></p><hr><p>这里假设我们的类别标签如下：</p><blockquote><ol><li><strong>pedestrain</strong></li><li><strong>car</strong></li><li><strong>motorcycle</strong></li><li><strong>background</strong></li></ol></blockquote><p>因此标签如下：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} p_c \\ b_x \\ b_y \\ b_h \\b_w \\c_1 \\ c_2 \\ c_3\end{bmatrix}</script><blockquote><ol><li>其中$p_c$代表图片中是否有对象，如果对象属于上面类别标签中的前三类，则$p_c = 1$，如果是第$4$类，也就是背景，则代表没有要检测的物体，$p_c = 0$，因此$p_c$代表被检测对象属于某一类别的概率(背景除外)；</li><li>如果检测到对象，则输出被检测对象的边界框参数$(b_x, b_y, b_h, b_w)$；</li><li>当$p_c = 1$时，则$(c_1, c_2, c_3)$输出该对象属于$1-3$中的哪一类，需要注意的是针对分类定位问题，图片中最多只会出现其中一个对象类别。</li></ol></blockquote><p>以下图中有车的图为例，下图的标签输出为：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} 1 \\ b_x \\ b_y \\ b_h \\b_w \\0 \\ 1 \\ 0\end{bmatrix}</script><p>$(c_1, c_2, c_3)$最多只有一个为$1$.</p><p>以下图中只有背景的图为例，下图的标签输出为：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} 0 \\ ?\\ ?\\ ?\\?\\? \\ ? \\ ?\end{bmatrix}</script><p>其中$?$代表无意义的参数.</p><p>最后定义一下训练神经网络的<strong>损失函数</strong>(如果采用<strong>平方误差</strong>策略)</p><script type="math/tex; mode=display">\cal{L}(\hat{y},y)= \begin{cases} (\hat{y}_1 - y_1)^2 + (\hat{y}_2 - y_2)^2 + ... + (\hat{y}_8 - y_8)^2, & \text {if $p_c = 1$} \\ (\hat{y}_1 - y_1)^2, & \text{if $p_c = 0$} \end{cases}</script><p>需要注意的是采用平方误差策略只是为了简化而已，实际一般是对于坐标$(b_x, b_y, b_h, b_w)$采用平方误差，而对$p_c$采用逻辑回归函数，当然，采用平方误差也是可以的。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558877137389.png" alt="Alt text"></p><h3 id="1-2-特征点检测-Landmark-Detection"><a href="#1-2-特征点检测-Landmark-Detection" class="headerlink" title="1.2 特征点检测(Landmark Detection)"></a>1.2 特征点检测(Landmark Detection)</h3><h4 id="1-2-1-人脸部特征定位"><a href="#1-2-1-人脸部特征定位" class="headerlink" title="1.2.1 人脸部特征定位"></a>1.2.1 人脸部特征定位</h4><p>假设你现在要对人的脸部各个特征点进行定位，那么你可以通过设定特征点的个数，假设设定脸部有$64$个特征点。选定特征点的个数之后，生成包含这些特征点的标签训练集(人为辛苦标注的)，然后利用神经网络输出脸部关键特征点的位置。</p><p>具体的做法是：准备一个卷积神经网络和一些特征集，将人脸图片输入卷积神经网络，然后输出对于的标签：</p><script type="math/tex; mode=display">y =  \begin{bmatrix} p_c \\ l_{x_1}\\ l_{y_1}\\ ...\\l_{64x}\\l_{64y} \\\end{bmatrix}</script><p>因此，对于这个例子来说$y$有$129$个输出单元，因此实现对图片的人脸进行检测和定位(现实中的例子：AR中在人的头上显示皇冠的效果)。</p><h4 id="1-2-2-人体姿态检测"><a href="#1-2-2-人体姿态检测" class="headerlink" title="1.2.2 人体姿态检测"></a>1.2.2 人体姿态检测</h4><p>通过神经网络去标注人物姿态的关键特征点，就相当于输出了人物的姿态动作。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558878193152.png" alt="Alt text"></p><h3 id="1-3-目标检测-Object-Detection"><a href="#1-3-目标检测-Object-Detection" class="headerlink" title="1.3 目标检测(Object Detection)"></a>1.3 目标检测(Object Detection)</h3><blockquote><p>基于滑动窗口的目标检测算法.</p></blockquote><p>假设你现在需要构建一个汽车检测算法，步骤是：</p><blockquote><ol><li>首先创建一个标签训练集，也就是说$(x, y)$代表适当剪切的汽车图片样本。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558878528054.png" alt="Alt text"></li><li>出于我们对这个训练集的期望，我们一开始可以使用适当剪切的图片：就是整张图片$x$几乎被汽车给占据，剪掉汽车以外的部分，使得汽车居于中心位置，并基本占据整张照片；</li><li>然后将这些适当剪切过的图像输入卷积神经网络中，然后让卷积神经网络输出标签$y = 0/1$，代表是否有车。</li></ol></blockquote><p>训练完这个卷积网络之后，就可以用它来实现滑动窗口目标检测了，具体步骤如下:</p><blockquote><ol><li>假设下图是一张测试图片，首先选定一个特定大小的窗口，比如图片下面的红色方框；</li><li>然后将这个红色小方块(<strong>所框定的图像</strong>)输入上面训练好了的物体检测的卷积网络，然后卷积网络开始进行预测，即判断红色方框中有没有汽车；</li><li>滑动窗口目标检测算法接下来会以一定步长稍向右边移动，得到第二个要处理的图像，然后输入卷积神经网络得到预测的标签；</li><li>然后继续<strong>以固定的步幅</strong>稍向右移，把框定后的图像输入卷积网络，然后继续按照上面进行处理，依次重复，直到这个窗口滑过图像的每一个角落，对每个位置按照$0/1$进行分类。</li><li>以上就是所谓的图像滑动窗口操作(<strong>sliding window through the image</strong>)。</li><li>接下来选择一个更大的窗口，截取更大的区域，输入给卷积网络处理，然后按照上面的重复，然后再一次选择更大一点的窗口，继续重复… …<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558879577145.png" alt="Alt text"></li></ol></blockquote><p>如果按照上面的思路，不管汽车在图像的哪个位置，总有一个窗口可以检测到它。不过<strong>滑动窗口目标检测算法</strong>有一个很明显的缺点就是<strong>计算成本</strong>，因为你在图像上剪出了很多的小块，而卷积要一个一个地处理；而如果你选用的窗口比较大，显然可以减少输入卷积网络的窗口数，但是粗粒度可能会影响到性能，可能会无法准确定位图片中的对象。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558879928003.png" alt="Alt text"></p><h3 id="1-4-卷积的滑动窗口实现"><a href="#1-4-卷积的滑动窗口实现" class="headerlink" title="1.4 卷积的滑动窗口实现"></a>1.4 卷积的滑动窗口实现</h3><p>为了构建滑动窗口的卷积应用，首先需要知道如何把神经网络的全连接层转化为卷积层：假设对象检测算法输入一个$14 \times 14 \times 3$的图像(这里的尺寸大小应该是指的上一节中红色框定的图像大小)，这里设定滤波器大小为$5 \times 5$，数量为$16$个。</p><p>图像在滤波器处理过后变成$10 \times 10 \times 16$，然后继续如下：<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558881465218.png" alt="Alt text"><br>其中<strong>softmax</strong>层为$4$个类别的输出: <strong>行人、汽车、摩托车和其他</strong>。</p><p>下面将展示如何将全连接层转化为卷积层：前面的卷积层一样，最后一层卷积完毕之后不是直接连接全连接层，而是通过$5 \times 5 \times 16$，个数为$400$个的滤波器将其转化为了$1 \times 1 \times 400$的卷积块，因此我们不再将其看做是一个含有$400$个节点的集合，而是一个$1 \times 1 \times 400$的输出层，从数学的角度来看，它和全连接层是一样的。然后在添加另一个卷积层，滤波器为$1 \times 1 \times 400$，个数为$400$个。最后在经由$1 \times 1$的滤波器处理，得到一个$softmax$激活值，大小为$1 \times 1 \times 4$。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558882030068.png" alt="Alt text"></p><p>接着我们再看看如果通过卷积实现滑动窗口对象检测算法：假设测试集图像为$16 \times 16 \times 3$(加上黄色条块的图像)，首先将测试图片集中蓝色区域输入卷积网络(也就是大小为$14 \times 14 \times 3$)生成$0/1$分类，接着滑动两个像素，将图中绿色框定的图像输入到卷积网络，得到另一个标签$0/1$，接着继续讲橘色框定的区域输入给卷积网络，得到另外的标签，最后将右下方紫色区域进行最后一次卷积操作，因此我们在这个$16 \times 16 \times 3$的图像上滑动窗口，如果运行$4$次，得到$4$个标签的话，会发现这$4$次卷积操作中本身很多的计算都是重复的。</p><p>但是使用滑动窗口的卷积应用则不同，它会使得上面$4$次的包含大量重复计算的卷积操作，在滑动窗口的卷积应用中把大量重复的计算进行共享：</p><blockquote><ol><li>通过$5 \times 5 \times 3$大小的滤波器核(卷积网络运行同样的参数)，得到$12 \times 12 \times 16$的输出层，然后跟$14 \times 14 \times 3$卷积过程一样的参数进行最大池化，最后得到$2 \times 2 \times 4$的输出层，最终在输出层这$4$个子方块中，左上角蓝色方块是最开始输入的测试图像$16 \times 16 \times 3$左上部分$14 \times 14$的输出；右上角的黄色块对应测试图片$16 \times 16 \times 3$右上部分$14 \times 14$的输出；左下角的黄色块对应测试图片$16 \times 16 \times 3$左下角部分$14 \times 14$的输出；右下角的黄色块对应测试图片$16 \times 16 \times 3$右下部分$14 \times 14$的输出；<strong>因此滑动窗口的卷积应用中，我们不需要把输入图片根据我们设定的固定大小的框把输入的测试图片分割成四个子集，分别4次执行卷积得到4个结果，而是直接一次卷积得到4个结果(因为4次分别卷积会有很多重复的计算，而滑动窗口的卷积应用一次就会将很多公有区域的计算结果进行共享，因此根据设定的参数可以准确的得到4个小块，对应4个类别)</strong>。</li><li>以另一个更大的预测图片为例($28 \times 28 \times 3$)，应用滑动窗口的卷积操作，以同样的方式进行卷积过程，最后得到$8 \times 8 \times 4$，其中相当于原始的不停的滑动窗口操作以步幅为<strong>2</strong>，不断滑动的结果，这里只需要<strong>1</strong>次，但是原始的话要得到<strong>8</strong>个结果需要<strong>8</strong>次卷积才能实现，这样就会导致超多的重复计算。<br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558885048931.png" alt="Alt text"></li><li>因此相比较于依靠连续滑动的卷积操作来识别图片中的车，滑动窗口的卷积应用根据定好的窗口大小，一次直接对整张图片进行卷积，一次得到所有的预测值，避免了超多的重复运算(只不过缺点是边界框的位置可能不够准确，下一节解决)。<strong>而滑动窗口的卷积能够有这样的效果是因为，由于一次能够得到所有预测值，而所有预测值是堆叠成块的，这是全连接不能够做到的，这也就是上面为什么使用卷积来替代全连接才能达到的效果。</strong><br><img src= "/img/loading.gif" data-src="/2019/05/26/object-detection/1558885256455.png" alt="Alt text"></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;详细记录目标检测算法整个过程的原理.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>YOLO_v3代码解析以及相关注意事项</title>
    <link href="http://yoursite.com/2019/05/25/YOLO/"/>
    <id>http://yoursite.com/2019/05/25/YOLO/</id>
    <published>2019-05-25T08:52:18.000Z</published>
    <updated>2020-07-25T14:51:47.878Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>对于目标检测算法YOLO在使用过程中遇到的问题以及知识点进行记录.</strong><br><a id="more"></a></p></blockquote><h2 id="1-项目介绍"><a href="#1-项目介绍" class="headerlink" title="1. 项目介绍"></a>1. 项目介绍</h2><p>&#160; &#160; &#160; &#160;本次YOLO_v3的项目来源于<a href="https://www.jiqizhixin.com/%5B%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83%5D" target="_blank" rel="noopener">机器之心</a>翻译的项目—-<a href="https://www.jiqizhixin.com/articles/2018-04-23-3" target="_blank" rel="noopener">从零开始PyTorch项目：YOLO v3目标检测实现</a>以及<a href="https://www.jiqizhixin.com/articles/042602" target="_blank" rel="noopener">从零开始 PyTorch 项目：YOLO v3 目标检测实现（下）</a>两部分组成，原版的博客在此<a href="https://blog.paperspace.com/tag/series-yolo/" target="_blank" rel="noopener">Series: YOLO object detector in PyTorch</a>，原始博客的GitHub地址为：<a href="https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch" target="_blank" rel="noopener">ayooshkathuria/pytorch-yolo-v3</a>，最后附上论文的地址：<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">YOLOv3: An Incremental Improvement</a>。</p><h2 id="2-项目需求及相关文件解释"><a href="#2-项目需求及相关文件解释" class="headerlink" title="2. 项目需求及相关文件解释"></a>2. 项目需求及相关文件解释</h2><p>&#160; &#160; &#160; &#160;YOLO_v3官方原始的代码是由C语言所写，真是佩服作者手撸代码能力啊，但是由于源码并没有相关的任何注释，阅读起来特别费事，所以参考了网上关于YOLO_v3迁移到$Tensorflow$和$Pytorch$的代码进行阅读，以便对代码有更深的认识和理解。</p><p>关于此YOLO_v3的$Pytorch$的代码，其$Pytorch$作者的<a href="https://github.com/ayooshkathuria/pytorch-yolo-v3" target="_blank" rel="noopener">GitHub</a>提到相应的需求如下：</p><ul><li>Python3.5</li><li>OpenCV</li><li>Pytorch 0.4(<strong>其中作者提了一句：$Using$ $PyTorch$ $0.3$ $will$ $break$ $the$ $detector.$</strong>)</li></ul><p>&#160; &#160; &#160; &#160;在准备好相应的东西之后将相应的项目$Clone$ $or$ $downoad$下来，除此之外你首先需要下载$weights$ $file$(权重文件)，地址如下：<a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener"> a weightsfile only for COCO</a>，大小有237M，下载下来的权重文件只需放在下载好的$project$根目录下即可，权重文件名为：<strong>yolov3.weights</strong></p><p>&#160; &#160; &#160; &#160;一切准备就绪就可以进行图片检测了，如果想要检测图片，可以在$Pycharm$的底部栏找到$Terminal$，也就是$CMD$，将地址$cd$到你下载的项目所在的地址，如：<strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\yolo_tensorflow-master&gt;</strong>，然后在$Terminal$中输入：<strong>python detect.py —images imgs —det det</strong>命令即可，其中 <strong>—images</strong>代表定义图片存储或加载的文件夹，而<strong>—det</strong>代表检测完图片之后将检测图片保存起来的文件夹，当然，如果需要查看其它标记含义(例如：<strong>—bs</strong>)，可以通过在命令行输入：<strong>python detect.py -h</strong> 或者直接代开$project$中<strong>detect.py</strong>文件查看<strong>arg_parse()</strong>函数下定义的参量进行查看。甚至直接运行<strong>detect.py</strong>文件也可以进行图片检测。</p><p>&#160; &#160; &#160; &#160;如果你想检测视频，可以运行<strong>video.py</strong>进行检测，要在视频或网络摄像头上运行这个检测器，代码基本可以保持不变，只是我们不会在 batch 上迭代，而是在视频的帧上迭代。</p><p>&#160; &#160; &#160; &#160;在视频上运行该检测器的代码可以在我们的 <strong>GitHub</strong> 中的 <strong>video.py</strong> 文件中找到。这个代码非常类似 <strong>detect.py</strong> 的代码，只有几处不太一样。这里需要注意的是，如果想要对视频进行检测需要将视频文件放入$projiect$的根目录中，而且在<strong>video.py</strong>文件中对视频定义的默认参数为：<strong>default = “video.avi”</strong>，所以，你需要更改你视频文件的名称或者更改参数的默认设定。</p><h2 id="3-源码详细解析"><a href="#3-源码详细解析" class="headerlink" title="3. 源码详细解析"></a>3. 源码详细解析</h2><p>&#160; &#160; &#160; &#160;<strong>PASS</strong></p><h2 id="4-源码中的问题"><a href="#4-源码中的问题" class="headerlink" title="4. 源码中的问题"></a>4. 源码中的问题</h2><blockquote><p>&#160; &#160; &#160; &#160;在项目下载下来之后发现了源码有一些比较坑的地方，以至于部分代码无法运行，这里举出暂时所遇到的坑以及解决办法。</p></blockquote><h3 id="4-1-视频检测中遇到的问题："><a href="#4-1-视频检测中遇到的问题：" class="headerlink" title="4.1 视频检测中遇到的问题："></a>4.1 视频检测中遇到的问题：</h3><p>&#160; &#160; &#160; &#160;此处非项目：<strong>YOLO_v3_tutorial_from_scratch-master</strong>中视频检测遇到的问题，而是项目<strong>yolo_tensorflow-master</strong>中视频检测遇到的问题，只是在此将所有和YOLO代码相关的问题都列举出来，在项目<strong>yolo_tensorflow-master</strong>的<strong>test.py</strong>中，第201-203行代码为摄像头检测的代码：<strong>detect from camera</strong><br>代码中：<strong>cap = cv2.VideoCapture(0)</strong>，原始的<a href="https://github.com/hizhangp/yolo_tensorflow" target="_blank" rel="noopener">GitHub：hizhangp/yolo_tensorflow</a>代码中为：<strong>cap = cv2.VideoCapture(-1)</strong>，运行时造成摄像头开启后显示的图像为满屏幕的雪花噪点，无法检测到任何东西，需要将括号中的参数<strong>-1</strong>改为<strong>0</strong>方可正常开始摄像头进行检测。此项目<strong>yolo_tensorflow-master</strong>的解读参照知乎相关专栏：<a href="https://zhuanlan.zhihu.com/p/25053311" target="_blank" rel="noopener">YOLO源码解析 作者：狗头山人七 </a></p><h3 id="4-2-图片检测中遇到的问题："><a href="#4-2-图片检测中遇到的问题：" class="headerlink" title="4.2 图片检测中遇到的问题："></a>4.2 图片检测中遇到的问题：</h3><blockquote><p>&#160; &#160; &#160; &#160;此处遇到的问题非常的多和杂，将代码修改正确以正常运行耗费了我很多精力，在此一一列举，以备以后遇到类似情况。<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791232225.png" alt="Alt text"></p></blockquote><h4 id="4-2-1-绝对路径与相对路径的问题"><a href="#4-2-1-绝对路径与相对路径的问题" class="headerlink" title="4.2.1 绝对路径与相对路径的问题"></a>4.2.1 绝对路径与相对路径的问题</h4><p>&#160; &#160; &#160; &#160;刚开始运行图片检测代码(项目：<strong>YOLO_v3_tutorial_from_scratch-master</strong>)的时候直接报错，报的错误为：</p><blockquote><p>&#160; &#160; &#160; &#160;<strong>AttributeError: ‘NoneType’ object has no attribute ‘shape’</strong><br>&#160; &#160; &#160; &#160;<strong>！！完全不知道错误的那种错！！</strong><br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791246507.png" alt="Alt text"></p></blockquote><p>&#160; &#160; &#160; &#160;后来一堆百度，Google查证之后发现是报了一个空的对象没有<strong>shape</strong>这个属性的错误，可能是因为路径设置不对，所以返回的类型是None。后来通过bug排除发现是代码第<strong>126</strong>行：<strong>imlist = [osp.join(osp.realpath(‘.’), images, img) for img in os.listdir(images)]</strong>，这一行代码主要是用于生成图片所在的绝对地址：</p><blockquote><p>&#160; &#160; &#160; &#160;<strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\dog.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\eagle.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\giraffe.jpg</strong><br>&#160; &#160; &#160; &#160; <strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\herd_of_horses.jpg</strong><br>&#160; &#160; &#160; &#160; <strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\img1.jpg</strong><br>&#160; &#160; &#160; &#160; <strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\img2.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\img3.jpg</strong><br>&#160; &#160; &#160; &#160; <strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\img4.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>D:\软件安装\pycharm\PyCharm Community Edition 2017.3.3\workplace\pytorch-yolo-v3-master\imgs\lisa.jpg</strong></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791339113.png" alt="Alt text"></p></blockquote><p>&#160; &#160; &#160; &#160;这里我意识到可能此处的错误并非作者源代码的错误，而是作者在用这一句<strong>osp.join(osp.realpath(‘.’)</strong>的时候获取的是绝对路径，而我的绝对路径包含了中文！！！！<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791360085.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;所以为了解决这个问题，我需要将绝对路径修改为相对路径，以避免出现中文这个脑壳疼的问题（话说好多与<strong>bug</strong>有关的问题是出现路径有中文上！），修改方法就是上面的代码修改为：<strong>osp.join(osp.relpath(‘.’)</strong>，其中<strong>rel</strong>指的就是<strong>relative</strong>。</p><hr><h4 id="4-2-2-斜杠’-‘与反斜杠’-’的问题"><a href="#4-2-2-斜杠’-‘与反斜杠’-’的问题" class="headerlink" title="4.2.2 斜杠’/‘与反斜杠’\\’的问题"></a>4.2.2 斜杠’/‘与反斜杠’\\’的问题</h4><p>&#160; &#160; &#160; &#160;在上面的问题解决以后运行代码发现代码运行正常，但是最后保存检测图片的文件夹<strong>—det</strong>并没有任何图片存放其中，后来定位到存放图片的代码：</p><pre><code>det_names = pd.Series(imlist).apply(lambda x: &quot;{}/det_{}&quot;.format(args.det,x.split(&quot;/&quot;)[-1]))list(map(cv2.imwrite, det_names, orig_ims))</code></pre><p>&#160; &#160; &#160; &#160;每张图像都以<strong>「det_」</strong>加上图像名称的方式保存。我们创建了一个地址列表，这是我们保存我们的检测结果图像的位置。最后，将带有检测结果的图像写入到 <strong>det_names</strong> 中的地址。</p><p>&#160; &#160; &#160; &#160;其中<strong>det_name</strong>打印出来结果如下：</p><blockquote><p>&#160; &#160; &#160;<strong>0                det/det_.\imgs\dog.jpg</strong><br>&#160; &#160; &#160;<strong>1              det/det_.\imgs\eagle.jpg</strong><br>&#160; &#160; &#160;<strong>2            det/det_.\imgs\giraffe.jpg</strong><br>&#160; &#160; &#160;<strong>3     det/det_.\imgs\herd_of_horses.jpg</strong><br>&#160; &#160; &#160;<strong>4               det/det_.\imgs\img1.jpg</strong><br>&#160; &#160; &#160;<strong>5               det/det_.\imgs\img2.jpg</strong><br>&#160; &#160; &#160;<strong>6               det/det_.\imgs\img3.jpg</strong><br>&#160; &#160; &#160;<strong>7               det/det_.\imgs\img4.jpg</strong><br>&#160; &#160; &#160;<strong>8               det/det_.\imgs\lisa.jpg</strong><br>&#160; &#160; &#160;<strong>9              det/det_.\imgs\messi.jpg</strong><br>&#160; &#160; &#160;<strong>10            det/det_.\imgs\person.jpg</strong><br>&#160; &#160; &#160;<strong>11            det/det_.\imgs\scream.jpg</strong><br>&#160; &#160; &#160;<strong>12             det/det_.\imgs\vagaa.jpg</strong></p></blockquote><p>&#160; &#160; &#160; &#160;而<strong>imlist</strong>打印出来的结果为：</p><blockquote><p>&#160; &#160; &#160; &#160; <strong>[‘.\\imgs\\dog.jpg’,’.\\imgs\\eagle.jpg’, ‘.\\imgs\\giraffe.jpg’,’.\\imgs\\herd_of_horses.jpg’, ‘.\\imgs\\img1.jpg’,’.\\imgs\\img2.jpg’, ‘.\\imgs\\img3.jpg’]</strong></p></blockquote><p>&#160; &#160; &#160; &#160;下一句代码：<strong>list(map(cv2.imwrite, det_names, orig_ims))</strong>中：</p><blockquote><p>&#160; &#160; &#160; &#160;1.<strong>cv2.imwrite()</strong>函数：保存图片的函数，共两个参数，第一个为保存文件名，第二个为读入图片。<br>&#160; &#160; &#160; &#160;2.<strong>orig_ims</strong>：为list所包裹这的array矩阵，类型为无符号8位<strong>dtype=uint8</strong>，所以为图片的编码矩阵。<br>&#160; &#160; &#160; &#160;3.<strong>map()</strong>函数：会根据提供的函数对指定序列做映射。第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。语法为：<strong>map(function, iterable, …)</strong>，其中function — 函数，有两个参数；iterable — 一个或多个序列，例如：<strong>map(square, [1,2,3,4,5])</strong>，打印：<strong>[1, 4, 9, 16, 25]</strong>，其中<strong>square(x)</strong>为一个函数，<strong>return x$^2$</strong>。</p></blockquote><p>&#160; &#160; &#160; &#160;所以从上可以看出<strong>map</strong>函数将<strong>cv2.imwrite, det_names, orig_ims</strong>之间建立映射，其中<strong>det_names, orig_ims</strong>作为<strong>cv2.imwrite()</strong>的参数，其中一个作为保存文件名，一个作为读入的图片。</p><p>&#160; &#160; &#160; &#160;运行此代码，显示：</p><blockquote><p>&#160; &#160; &#160; &#160;[False, False, False, False, False, False, False, False, False, False, False, False, False]</p></blockquote><p>&#160; &#160; &#160; &#160;说明映射失败，图片存储不成功。而由<strong>det_name</strong>所打印的显示表明失败是由于文件名中存在反斜杠’\’，反斜杠在<strong>python</strong>中有实际转义含义的，所以需要将反斜杠换为斜杠，不然运行的时候代码会将反斜杠一起运行。</p><p>&#160; &#160; &#160; &#160;修改方法如下：</p><pre><code>for i in range(len(imlist)):        imlist[i] = imlist[i].replace(&#39;\\&#39;, &#39;/&#39;)</code></pre><p>&#160; &#160; &#160; &#160;此处还需要注意的就是<strong>imlist[i].replace(‘\\’, ‘/‘)</strong>会生成一个列表存放修改后的值，而不是在原本的列表<strong>imlist[]</strong>上进行修改。</p><p>&#160; &#160; &#160; &#160;修改后的<strong>det_name</strong>为：</p><blockquote><p>&#160; &#160; &#160; &#160;<strong>0                det/det_dog.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>1              det/det_eagle.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>2            det/det_giraffe.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>3     det/det_herd_of_horses.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>4               det/det_img1.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>5               det/det_img2.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>6               det/det_img3.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>7               det/det_img4.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>8               det/det_lisa.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>9              det/det_messi.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>10            det/det_person.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>11            det/det_scream.jpg</strong><br>&#160; &#160; &#160; &#160;<strong>12             det/det_vagaa.jpg</strong></p></blockquote><p>&#160; &#160; &#160; &#160;此时，反斜杠已经改为了斜杠，代码运行正常，且<strong>det</strong>文件夹也正常存放了检测后的图片，如下：<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791410182.png" alt="Alt text"></p><h2 id="5-关于YOLO-v3作者"><a href="#5-关于YOLO-v3作者" class="headerlink" title="5. 关于YOLO_v3作者"></a>5. 关于YOLO_v3作者</h2><p>&#160; &#160; &#160; &#160;在知乎中有一个问题是关于<a href="https://www.zhihu.com/question/269909535?rf=269932466" target="_blank" rel="noopener">如何评价最新的YOLOv3？</a>，非常有意思，列举如下一些有趣的点：</p><p>&#160; &#160; &#160; &#160;1. 首先，YOLO_v3的论文根本就不算论文，更多的算是一片技术报告，读起来相当的欢乐。作者首先就诚实的表明自己去年没做什么研究，而是主要花在完Twitter上和玩了下GAN。</p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791419395.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791432929.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;2. 其次，yolo的作者编码能力真的强，darknet几乎不用啥依赖库，全部手撸，编译起来也是快到没朋友。而且，作业可能是个<strong>小马宝莉</strong>的迷吧，哈哈哈，蛮有意思，感觉像个小萌妹。看看他的resume、个人主页、git的commit message感受一下萌萌哒。<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791517510.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791523803.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791547788.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;而他本人长这样：<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791553065.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;3. 直接diss一众算法，例如他论文中出现的：<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791558772.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;这张图把自己放在第二象限，赤裸裸的表达你们这些都是辣鸡。。。。<br>&#160; &#160; &#160; &#160;这图应该作者从其他论文裁剪过来，再强行画上的;<br>&#160; &#160; &#160; &#160;实力嘲讽啊，取了Retina里面的图，然后diss一波。</p><p>&#160; &#160; &#160;4. 论文结尾亮点满满，挺佩服能把自己论文写的这么随意的，羡慕大佬：<br>&#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791566119.png" alt="Alt text"></p><p>&#160; &#160; &#160;效果好到论文可以随意写，反正我效果摆在这里，论文写的再随意，你们还是要引用的。</p><p>&#160; &#160; &#160;5. 而且有细心网友发现，模型一作在arXiv上发布研究论文时，脑回路清奇地将自己这篇论文自引自用了一下。<br>&#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791580144.png" alt="Alt text"></p><p>&#160; &#160; &#160;而在小哥自引自用后没多久，arXiv官方账号宣布服务器由于不明原因挂掉了……<br>&#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/YOLO/1558791585566.png" alt="Alt text"></p><h2 id="6-补完计划"><a href="#6-补完计划" class="headerlink" title="6. 补完计划"></a>6. 补完计划</h2><p>&#160; &#160; &#160;文章的第三部分代码详细解析，有空了会继续补完，待续。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;对于目标检测算法YOLO在使用过程中遇到的问题以及知识点进行记录.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="YOLO" scheme="http://yoursite.com/tags/YOLO/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle竞赛相关知识</title>
    <link href="http://yoursite.com/2019/05/25/Kaggle-2/"/>
    <id>http://yoursite.com/2019/05/25/Kaggle-2/</id>
    <published>2019-05-25T08:50:50.000Z</published>
    <updated>2020-07-26T13:12:34.354Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Kaggle相关的知识点.</strong><br><a id="more"></a></p></blockquote><h2 id="1-分类算法基础篇"><a href="#1-分类算法基础篇" class="headerlink" title="1. 分类算法基础篇"></a>1. 分类算法基础篇</h2><h3 id="1-1-相应内容"><a href="#1-1-相应内容" class="headerlink" title="1.1 相应内容"></a>1.1 相应内容</h3><blockquote><p>&#160; &#160; &#160; &#160;1. 线性分类器基本原理与案例实战<br>&#160; &#160; &#160; &#160;2. 支持向量机分类器原理与案例实战<br>&#160; &#160; &#160; &#160;3. 朴素贝叶斯分类器原理与案例实战<br>&#160; &#160; &#160; &#160;4. K近邻分类器原理与案例实战<br>&#160; &#160; &#160; &#160;5. 决策树分类器原理与案例实战<br>&#160; &#160; &#160; &#160;6. 集成模型分类器原理与案例实战</p></blockquote><h3 id="1-2-线性分类器（Logistic-Regression）"><a href="#1-2-线性分类器（Logistic-Regression）" class="headerlink" title="1.2 线性分类器（Logistic Regression）"></a>1.2 线性分类器（<strong>Logistic Regression</strong>）</h3><h4 id="1-2-1-监督学习基本架构和流程"><a href="#1-2-1-监督学习基本架构和流程" class="headerlink" title="1.2.1 监督学习基本架构和流程"></a>1.2.1 监督学习基本架构和流程</h4><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792273135.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792278297.png" alt="Alt text"></p><h4 id="1-2-2-监督学习之分类问题"><a href="#1-2-2-监督学习之分类问题" class="headerlink" title="1.2.2 监督学习之分类问题"></a>1.2.2 监督学习之分类问题</h4><p>&#160; &#160; &#160; &#160;监督学习的重点在于根据已有的经验知识对未知样本的目标、标记进行预测。根据目标预测变量的不同，监督学习分类可以分为两类：分类任务和回归任务。<br>&#160; &#160; &#160; &#160;其中分类问题可以分为：<strong>二分类问题</strong>和<strong>多分类问题</strong>，根据预测标签的个数还可以分为：<strong>单标签分类问题</strong>和<strong>多标签分类问题(实际问题中遇到的比较多)</strong>。所以上下两种是可以组合的，比方说：<strong>两类多标签分类问题</strong>。</p><h4 id="1-2-3-分类器模型之线性分类器"><a href="#1-2-3-分类器模型之线性分类器" class="headerlink" title="1.2.3 分类器模型之线性分类器"></a>1.2.3 分类器模型之线性分类器</h4><p>&#160; &#160; &#160; &#160;<strong>模型介绍</strong>：线性分类器(<strong>Linear classifiers</strong>)，顾名思义，是一种假设特征向量与分类结果存在线性关系的模型。这个模型通过累加计算每个维度的特征与对应的权重的乘积来进行分类决策。<br>&#160; &#160; &#160; &#160;比如：$n$维特征向量：$x$ = [$x_1$,$x_2$,$…….$,$x_n$]${^T}$<br>&#160; &#160; &#160; &#160;相应：$n$维权重向量：$w$ = [$w_1$,$w_2$,$…….$,$w_n$]${^T}$<br>&#160; &#160; &#160; &#160;线性模型：$f(w,x,b)$ = $w{^T}x$ + $b$，其中$f∈R$<br>&#160; &#160; &#160; &#160;在最简单的二分类问题中，我们希望$f∈${$0,1$}，因此我们需要一个函数把$f$从实数域映射到{$0,1$}，因此引入$Logistic$ $Function$(逻辑回归函数):<br>&#160; &#160; &#160; &#160;$g(z)=\frac{1}{1+e^{-z}}$，$z∈R$，$g(z)∈(0,1)$<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792314026.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;将变量$z$替换为$f(w,x,b)$，就可以得到一个典型的<strong>线性分类器</strong>:逻辑回归模型($Logistic$ $Regression$):<br>&#160; &#160; &#160; &#160;$h_{w,b}(x) = g(f(w,b,x)) = \frac{1}{1+e^{-(w^{T}x+b)}}$<br>&#160; &#160; &#160; &#160;如果$h_{w,b} &lt; 0.5$，就把$x$分为一类；<br>&#160; &#160; &#160; &#160;如果$h_{w,b} \geq 0.5$，就把$x$分为另一类。<br>&#160; &#160; &#160; &#160;如何在给定的样本集上估计模型参数呢？<br>&#160; &#160; &#160; &#160;$m$个样本特征向量构成的训练集：$X =$ {$x^1,x^2,….,x^m$}<br>&#160; &#160; &#160; &#160;$m$个样本特征向量的真实类标签：$Y =$ {$y^1,y^2,….,y^m$}<br>&#160; &#160; &#160; &#160;我们希望逻辑回归模型能够在上述数据集上取得最大似然估计($Maximum$ $Likelihood$)：<br>&#160; &#160; &#160; &#160;$\mathop{\arg \max}_{w,b} L(w,b) = \mathop{\arg \max}_{w,b} \prod_{i=1}^m (h_{w,b}(x^i))^{y^{i}}·(1-h_{w,b}(x^i))^{1-y^i}$<br>&#160; &#160; &#160; &#160;因为这里要求解一个最大化的优化问题，而且目标函数是解析的，所以我们使用随机梯度上升算法(<strong>Stochastic Gradient Ascend</strong>)来进行目标函数的优化进而寻找最优的模型参数。                </p><h4 id="1-2-4-分类器性能评估"><a href="#1-2-4-分类器性能评估" class="headerlink" title="1.2.4 分类器性能评估"></a>1.2.4 分类器性能评估</h4><p>&#160; &#160; &#160; &#160;混淆矩阵，下图所示：<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792324266.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792329492.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<strong>精确率(Precision)</strong>：<br>&#160; &#160; &#160; &#160;$Precision = \frac{tp}{tp + fp}$，其中$tp = True$ $positive$，$fp = True$ $negative$<br>&#160; &#160; &#160; &#160;<strong>召回率(Recall)</strong>：<br>&#160; &#160; &#160; &#160;$Precision = \frac{tp}{tp + fn}$，其中$tp = True$ $positive$，$fp = False$ $negative$<br>&#160; &#160; &#160; &#160;相关$wiki$链接：<a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">Confusion matrix</a><br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792337314.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792345311.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792357487.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;关于$Precision$和$Recall$的进一步解释：<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792414989.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;$Precision$：被分类器挑选($selected$)出来的正样本究竟有多少是真正的正样本！说白了，准确率就是<strong>找的对</strong>！<br>&#160; &#160; &#160; &#160;$Recall$：在所有的真正的正样本中分类器挑选了多少个，召回率就是<strong>找的全</strong>！<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792421997.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;参考书目及链接：<br>&#160; &#160; &#160; &#160;1. <a href="https://www.cnblogs.com/sddai/p/5696870.html" target="_blank" rel="noopener">博客园—赏月斋：准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure</a><br>&#160; &#160; &#160; &#160;2. 李航. 统计学习方法[M]. 北京:清华大学出版社,2012.<br>&#160; &#160; &#160; &#160;3. <a href="https://www.zhihu.com/question/19645541" target="_blank" rel="noopener">如何解释召回率与准确率？</a><br>&#160; &#160; &#160; &#160;4. <a href="https://blog.csdn.net/u010705209/article/details/53037481" target="_blank" rel="noopener">SK_Lavender的博客—机器学习模型评价指标 — 混淆矩阵</a><br>&#160; &#160; &#160; &#160;5. <a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">Markdown 公式指导手册</a><br>&#160; &#160; &#160; &#160;6. <a href="https://blog.csdn.net/zdk930519/article/details/54137476" target="_blank" rel="noopener">Markdown中数学公式整理</a><br>&#160; &#160; &#160; &#160;7. <a href="https://blog.argcv.com/articles/1036.c" target="_blank" rel="noopener">准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure</a></p><h4 id="1-2-5-线性分类器案例实战"><a href="#1-2-5-线性分类器案例实战" class="headerlink" title="1.2.5 线性分类器案例实战"></a>1.2.5 线性分类器案例实战</h4><h5 id="1-2-5-1-数据描述：良-恶性乳腺肿瘤预测"><a href="#1-2-5-1-数据描述：良-恶性乳腺肿瘤预测" class="headerlink" title="1.2.5.1 数据描述：良/恶性乳腺肿瘤预测"></a>1.2.5.1 数据描述：良/恶性乳腺肿瘤预测</h5><p>&#160; &#160; &#160; &#160;数据地址：<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/" target="_blank" rel="noopener">良/恶性肿瘤数据集</a><br>&#160; &#160; &#160; &#160;样本个数：699($as$ $of$ $15$ $July$ $1992$)<br>&#160; &#160; &#160; &#160;样本属性个数：10加上类别标签<br>&#160; &#160; &#160; &#160;特征属性描述：(类比标签已经被移到了最后一列上)</p><div class="table-container"><table><thead><tr><th style="text-align:left">#</th><th style="text-align:right">属性</th><th style="text-align:center">值域</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:right">样本编号</td><td style="text-align:center">id number</td></tr><tr><td style="text-align:left">2</td><td style="text-align:right">Clump Thickness</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">3</td><td style="text-align:right">Uniformity of Cell Size</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">4</td><td style="text-align:right">Uniformity of Cell shape</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">5</td><td style="text-align:right">Marginal Adhesion</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">6</td><td style="text-align:right">Singal Epithelial Cell Size</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">7</td><td style="text-align:right">Bare Nuclei</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">8</td><td style="text-align:right">Bland Chromatin</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">9</td><td style="text-align:right">Normal Nucleoli</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">10</td><td style="text-align:right">Mitoses</td><td style="text-align:center">1-10</td></tr><tr><td style="text-align:left">11</td><td style="text-align:right">类别标签</td><td style="text-align:center">2代表良性，4代表恶性</td></tr></tbody></table></div><p>&#160; &#160; &#160; &#160;缺失属性值的样本个数：16，对应的缺失属性值标记为’?’。<br>&#160; &#160; &#160; &#160;类分布：<br>&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;良性($Benign$)：458(65.5%)；<br>&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;恶性($Malignant$)：241(34.5%)</p><h5 id="1-2-5-2-实战步骤"><a href="#1-2-5-2-实战步骤" class="headerlink" title="1.2.5.2 实战步骤"></a>1.2.5.2 实战步骤</h5><blockquote><p>&#160; &#160; &#160; &#160;1. 数据预处理(由于缺失值的存在)<br>&#160; &#160; &#160; &#160;2. 划分训练集和测试集<br>&#160; &#160; &#160; &#160;3. 训练学习模型并在测试集上预测<br>&#160; &#160; &#160; &#160;4. 对学习器模型进行评估<br>&#160; &#160; &#160; &#160;5. 实战总结</p></blockquote><h5 id="1-2-5-3-实战代码"><a href="#1-2-5-3-实战代码" class="headerlink" title="1.2.5.3 实战代码"></a>1.2.5.3 实战代码</h5><p>&#160; &#160; &#160; &#160;这一部分见Pycharm。</p><h3 id="1-3-支持向量机原理-support-vector-machine"><a href="#1-3-支持向量机原理-support-vector-machine" class="headerlink" title="1.3 支持向量机原理(support vector machine)"></a>1.3 支持向量机原理(support vector machine)</h3><h4 id="1-3-1-主要内容"><a href="#1-3-1-主要内容" class="headerlink" title="1.3.1 主要内容"></a>1.3.1 主要内容</h4><blockquote><p>&#160; &#160; &#160; &#160;1. 支持向量机的基本原理<br>&#160; &#160; &#160; &#160;2. 支持向量机实战：手写数字分类</p></blockquote><h4 id="1-3-2-基本原理"><a href="#1-3-2-基本原理" class="headerlink" title="1.3.2 基本原理"></a>1.3.2 基本原理</h4><blockquote><p>&#160; &#160; &#160; &#160;1. <strong>线性分类器的间隔(margin):</strong>到超平面最近的样本与此超平面之间的距离。<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792440848.png" alt="Alt text"></p></blockquote><p>&#160; &#160; &#160; &#160;2. 具有最大间隔的线性分类器叫做<strong>最大间隔线性分类器</strong>。其中最简单的一种就是支持向量机(<strong>SVM</strong>)，成为<strong>线性支持向量机(LSVM)</strong><br>&#160; &#160; &#160; &#160;3. 支持向量就是那些距离超平面最近的点。</p><h4 id="1-3-3-为何需要寻找最大间隔"><a href="#1-3-3-为何需要寻找最大间隔" class="headerlink" title="1.3.3 为何需要寻找最大间隔"></a>1.3.3 为何需要寻找最大间隔</h4><p>&#160; &#160; &#160; &#160;1. 直观上感觉很好。<br>&#160; &#160; &#160; &#160;2. 学习得到的线性分类器其对未知样本的预测能力与分类器间隔如下关系：<br>&#160; &#160; &#160; &#160;$R{(w)} \leq R_{emp}(α) + \Phi(\frac{1}{margin})$<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792449659.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792458927.png" alt="Alt text"></p><p>&#160; &#160; &#160; &#160;3. 利用二次优化求解：<br>&#160; &#160; &#160; &#160;$\mathop{Minimize} : {\frac{1}{2}w^Tw}$<br>&#160; &#160; &#160; &#160;$suject$ $to：y_k(w·x_k + b) \geq 1; k = 1,2,…..,n$<br>&#160; &#160; &#160; &#160;用拉格朗日乘数发转换为无约束二次优化问题。</p><h4 id="1-3-4-实战案例"><a href="#1-3-4-实战案例" class="headerlink" title="1.3.4 实战案例"></a>1.3.4 实战案例</h4><h5 id="1-3-4-1-案例数据集"><a href="#1-3-4-1-案例数据集" class="headerlink" title="1.3.4.1 案例数据集"></a>1.3.4.1 案例数据集</h5><p>&#160; &#160; &#160; &#160;1.案例的数据集为手写数字数据集:<strong>load_digits()</strong><br>&#160; &#160; &#160; &#160;2.其为多分类任务的数据集，其构成如下:<br>&#160; &#160; &#160; &#160;3. <strong>Data set Characteristics:</strong> </p><div class="table-container"><table><thead><tr><th style="text-align:left">Number of Instances:</th></tr></thead><tbody><tr><td style="text-align:left">5620</td></tr><tr><td style="text-align:left"><strong>Number of Attributes:</strong></td></tr><tr><td style="text-align:left"></td></tr><tr><td style="text-align:left">64</td></tr><tr><td style="text-align:left"><strong>Attribute information:</strong></td></tr><tr><td style="text-align:left"></td></tr><tr><td style="text-align:left">8x8 image of integer pixels in the range 0:16</td></tr><tr><td style="text-align:left"><strong>Miss Attribute Values:</strong></td></tr><tr><td style="text-align:left"></td></tr><tr><td style="text-align:left">None</td></tr></tbody></table></div><p>&#160; &#160; &#160; &#160;4. 其中<strong>64</strong>代表有64个特征分量，它是一个<strong>8x8</strong>的图像，每个像素点的范围是<strong>0-16</strong>的灰度图像，且无缺失值。<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792468660.png" alt="Alt text"></p><h5 id="1-3-4-2-实战步骤"><a href="#1-3-4-2-实战步骤" class="headerlink" title="1.3.4.2 实战步骤"></a>1.3.4.2 实战步骤</h5><blockquote><p>&#160; &#160; &#160; &#160;1. 数据预处理<br>&#160; &#160; &#160; &#160;2. 划分训练集和测试集<br>&#160; &#160; &#160; &#160;3. 训练学习模型并在测试集上预测<br>&#160; &#160; &#160; &#160;4. 对学习器模型的性能进行评估<br>&#160; &#160; &#160; &#160;5. 实战总结</p></blockquote><h5 id="1-3-4-3-实战代码"><a href="#1-3-4-3-实战代码" class="headerlink" title="1.3.4.3 实战代码"></a>1.3.4.3 实战代码</h5><p>&#160; &#160; &#160; &#160;这一部分见Pycharm。</p><h3 id="1-4-朴素贝叶斯分类器基本原理"><a href="#1-4-朴素贝叶斯分类器基本原理" class="headerlink" title="1.4 朴素贝叶斯分类器基本原理"></a>1.4 朴素贝叶斯分类器基本原理</h3><h4 id="1-4-1-主要内容"><a href="#1-4-1-主要内容" class="headerlink" title="1.4.1 主要内容"></a>1.4.1 主要内容</h4><blockquote><p>&#160; &#160; &#160; &#160;1. 基本原理<br>&#160; &#160; &#160; &#160;2. 分类实战：新闻文本分类</p></blockquote><h4 id="1-4-2-基本原理"><a href="#1-4-2-基本原理" class="headerlink" title="1.4.2 基本原理"></a>1.4.2 基本原理</h4><p>&#160; &#160; &#160; &#160;朴素贝叶斯分类器是建立在贝叶斯概率论基础之上的。<br>&#160; &#160; &#160; &#160;它的基本假定是特征向量的各个分量(属性)是条件独立的，因为这个假设在大多数时候都是不成立的，这也是<strong>‘朴素(naive)’</strong>这个叫法的来源。<br>&#160; &#160; &#160; &#160;虽然上述关于特征分量之间条件独立的假定比较<strong>naive</strong>，但是朴素贝叶斯分类器却能比较好的工作。朴素贝叶斯分类器首先单独考察每一个特征分量下的类别分配概率，然后把每个特征分量的条件概率按照贝叶斯概率论综合起来得出最终的判决结果。<br>&#160; &#160; &#160; &#160;朴素贝叶斯分类器的训练过程其实就是估计所有类的类条件概率分布的过程。这个过程可以并行计算。</p><h4 id="1-4-3-数学推导过程"><a href="#1-4-3-数学推导过程" class="headerlink" title="1.4.3 数学推导过程"></a>1.4.3 数学推导过程</h4><p>&#160; &#160; &#160; &#160;1. $n$维特征向量：$x = <x_1,x_2,....x_n>$  类别标签集合：$y ∈${$c_1,c_2,….c_k$}$$</x_1,x_2,....x_n></p><p>&#160; &#160; &#160; &#160;2. 特征向量$x$属于类别$c_i$的概率：$P(y=c_i|x)$<br>&#160; &#160; &#160; &#160;3. 依据贝叶斯原理：</p><blockquote><p>&#160; &#160; &#160; &#160;$P(y=c_i|x) = \frac{P(x|y=c_i)P(y=c_i)}{P(x)}$，其中$P(y=c_i)$是先验概率$y=c_i$代表性别，而$P(y=c_i|x)$则代表后验概率。 </p></blockquote><p>&#160; &#160; &#160; &#160;4. 分类决策过程，在$k$个可能的类中找到一个类标签使得$P(y|x)$最大。由于$P(x)$在所有的类标签下都是相同的(也就是不管$c_i$是多少，我们的观测量$P({\bf x})$都是不依赖它的)，所以我们事实上要最大化是分子部分：</p><blockquote><p>&#160; &#160; &#160; &#160;$\mathop{\arg \max}_{y}P(x|y)P(y) = \mathop{\arg \max}_{y}P(x_1,x_2,…x_n|y)P(y)$</p></blockquote><p>&#160; &#160; &#160; &#160;5.要想最大化上面的式子，我们必须要首先知道先验概率分布$P(y)$以及类条件概率分布$P(x_1,x_2,….x_n|y)$<br>&#160; &#160; &#160; &#160;6. 先验概率分布的估计一般有两种办法：</p><blockquote><p>&#160; &#160; &#160; &#160;①$P(y=c_1) = P(y=c_2) = ···· = P(y=c_k) = \frac{1}{k}$    <strong>极大熵方法(也就是最大熵原理)，可参考：李航 《统计学习方法》 2012年清华大学出版社 第6章 逻辑斯啼回归与最大熵模型</strong>，这里考虑所有分布的概率均等，极大熵的意思就是我的估计要是我的熵最大，在<strong>均匀分布</strong>的时候概率分布的熵最大。<u>极大熵的原理是在没有任何实际观察的情况下，我们就按照最简单的逻辑进行推理估计其取值，我们不给它添加任何先验的信息</u>。<br>&#160; &#160; &#160; &#160;②$P(y=c_1) = \frac{N_1}{N}, P(y=c_2) = \frac{N_2}{N}, ….,P(y=c_k) = \frac{N_k}{N}$   <strong>极大似然方法(从样本中估计)</strong>，其中$N$是总的样本量，$N_i$是$c_i$类的样本量：$\sum^{k}_{i = 1}N_k = N$，这种方式依赖于你的样本量。</p></blockquote><p>&#160; &#160; &#160; &#160;7. 大多数时候使用的<strong>最大熵原理</strong>，除非你的总的样本是全部覆盖了所有可能的取值情况，而且是不偏不倚的，这时候你再用<strong>极大似然估计</strong>才能准确。<br>&#160; &#160; &#160; &#160;8. 类联合条件概率分布：</p><blockquote><p>&#160; &#160; &#160; &#160;$\mathop{\arg \max}_{y}P({\bf x}|y)P(y) = \mathop{\arg \max}_{y}P(x_1,x_2,…x_n|y)P(y)$<br>&#160; &#160; &#160; &#160;$P(x_1,x_2,….,x_n|y) = P(x_1|y)P(x_2,….,x_n|x_1,y)P(x_3,…..,x_n|x_1,x_2,y)······P(x_n|x_1,x_2,….,x_{n-1},y)$<br>&#160; &#160; &#160; &#160;在此发现展开还是很难估计，因为若每个特征分量只取$0$或者$1$这两个值，那么上面的联合概率分布就有$k*2^n$个，参数需要估计太多,不可行！其中$k$为类的数目，其中$y = c_1$我们要估计一遍上面的式子，$y=c_2$我们要估计一遍上面的式子，$y=c_k$我们要估计一遍上面的式子，所以上面的式子要估计$k$次，而每个特征分量可以取$0$或者$1$，所以会有$2^n$个排列数。有时候$x$的取值不仅仅两个的时候，那么取值就会非常的大。所以我们要假定特征分量之间相互条件独立($x_1$到$x_n$相互独立)$P(x_n|x_1,x_2,…..,x_{n-1},y) = P(x_n|y)$，这样，联合概率分布就可以用边缘调价概率分布的乘积来计算。</p><p>&#160; &#160; &#160; &#160;$P(x_1,x_2,….,x_n|y) = \mathop \prod^{n}_{i=1}P(x_i|y) = P(x_1|y)P(x_2|y)·····P(x_n|y)$<br>&#160; &#160; &#160; &#160;这种时候只需要估计$2n*k$个参数。<br>&#160; &#160; &#160; &#160;每个类的每个特征分类只有两个概率值：<br>&#160; &#160; &#160; &#160;$P(x_1=0|y=c_1)$和$P(x_1=1|y=c_1)$<br>&#160; &#160; &#160; &#160;类边缘条件概率分布的估计：对于任意一个类$c_i$，我们要估计它的每个特征分量取值的概率分布$P(x_1=v|y=c_i)$。这种时候，我们一般假定每个特征分量的取值是有限个离散的数值。如果原来是连续的，我们可以将其离散化为若干个有限的取值水平。<br>&#160; &#160; &#160; &#160;而且需要注意的是$x_1$和$x_2$的取值可以不一样，$x_1 ∈ V_1 = $ { $v_1,v_2,…..,v_m$ }，$x_2 ∈ V_2,….,x_n∈V_n$<br>&#160; &#160; &#160; &#160;用极大似然法估计每个特征分量在每个可能取值上的类条件概率：<br>&#160; &#160; &#160; &#160;$P(x_1=v_1|y=c_i) = \frac{N_{x_1=v_1,y=c_i}}{N_{y=c_i}}$，$P(x_1=v_2|y=c_i) = \frac{N_{x_1=v_2,y=c_i}}{N_{y=c_i}}$</p></blockquote><p>&#160; &#160; &#160; &#160;9.万一分子上的样本数$N_{x_1=v_1,y=c_i}$等于零，也就是某个类(比方说$c_i$类)在某个特征分量(比方说$x_1$这个特征分量)的某个取值上(比方说$v_1$这个取值)完全没有样本咋办？这时会造成联合概率分布为0。</p><blockquote><p>&#160; &#160; &#160; &#160;这时，我们假定所有类在所有特征分量上的每种取值上都至少落入了$q$个样本。这其实就是事先假定所有的类条件概率分布都是均匀分布。然后在训练过程中不断的根据样本的实际情况去更新类条件概率分布。改进如下：<br>&#160; &#160; &#160; &#160;$P(x_1=v_1|y=c_i) = \frac {N_{x_1=v_1,y=c_i} + q}{N_{y=c_i} + m<em>q}$，$P(x_1=v_2|y=c_i) = \frac {N_{x_1=v_2,y=c_i} + q}{N_{y=c_i} + m</em>q}$<br>&#160; &#160; &#160; &#160;其中$q = 1$的时候，叫做<strong>拉普拉斯平滑</strong>。</p></blockquote><h4 id="1-4-4-新闻文本分类实战"><a href="#1-4-4-新闻文本分类实战" class="headerlink" title="1.4.4 新闻文本分类实战"></a>1.4.4 新闻文本分类实战</h4><h5 id="1-4-4-1-数据描述"><a href="#1-4-4-1-数据描述" class="headerlink" title="1.4.4.1 数据描述"></a>1.4.4.1 数据描述</h5><blockquote><p>&#160; &#160; &#160; &#160;20类新闻文本数据集：<br>&#160; &#160; &#160; &#160;该数据集包含了关于20个话题(<strong>topic</strong>)的18000条新闻报道，这些数据被分为两个子集，训练集和测试集。<br>&#160; &#160; &#160; &#160;<img src= "/img/loading.gif" data-src="/2019/05/25/Kaggle-2/1558792487149.png" alt="Alt text"></p></blockquote><h4 id="1-4-5-贝叶斯实战"><a href="#1-4-5-贝叶斯实战" class="headerlink" title="1.4.5 贝叶斯实战"></a>1.4.5 贝叶斯实战</h4><h5 id="1-4-5-1-数据处理与实战步骤"><a href="#1-4-5-1-数据处理与实战步骤" class="headerlink" title="1.4.5.1 数据处理与实战步骤"></a>1.4.5.1 数据处理与实战步骤</h5><p>&#160; &#160; &#160; &#160;和上面一样。</p><h2 id="2-回归算法基础篇"><a href="#2-回归算法基础篇" class="headerlink" title="2. 回归算法基础篇"></a>2. 回归算法基础篇</h2><h3 id="2-1-相应内容"><a href="#2-1-相应内容" class="headerlink" title="2.1 相应内容"></a>2.1 相应内容</h3><blockquote><p>&#160; &#160; &#160; &#160;1. 线性回归器基本原理与案例实战<br>&#160; &#160; &#160; &#160;2. 支持向量机回归器原理与案例实战<br>&#160; &#160; &#160; &#160;3. K近邻回归器原理与案例实战<br>&#160; &#160; &#160; &#160;4. 决策树回归器原理与案例实战<br>&#160; &#160; &#160; &#160;5. 集成模型回归原理与案例实战</p></blockquote><h2 id="3-无监督学习基础篇"><a href="#3-无监督学习基础篇" class="headerlink" title="3. 无监督学习基础篇"></a>3. 无监督学习基础篇</h2><h3 id="3-1-相应内容"><a href="#3-1-相应内容" class="headerlink" title="3.1 相应内容"></a>3.1 相应内容</h3><blockquote><p>&#160; &#160; &#160; &#160;1. K均值聚类基本原理与案例实战<br>&#160; &#160; &#160; &#160;2. 主成分分析(<strong>PCA</strong>)原理与案例实战</p></blockquote><h2 id="4-进阶篇"><a href="#4-进阶篇" class="headerlink" title="4. 进阶篇"></a>4. 进阶篇</h2><h3 id="4-1-相应内容"><a href="#4-1-相应内容" class="headerlink" title="4.1 相应内容"></a>4.1 相应内容</h3><blockquote><p>&#160; &#160; &#160; &#160;1. 特征提取技法提升<br>&#160; &#160; &#160; &#160;2. 模型选择技法提升<br>&#160; &#160; &#160; &#160;3. 模型验证技法提升<br>&#160; &#160; &#160; &#160;4. 其他常用机器学习工具的用法</p></blockquote><h2 id="5-实战篇"><a href="#5-实战篇" class="headerlink" title="5. 实战篇"></a>5. 实战篇</h2><h3 id="5-1-相应内容"><a href="#5-1-相应内容" class="headerlink" title="5.1 相应内容"></a>5.1 相应内容</h3><blockquote><p>&#160; &#160; &#160; &#160;1. Kaggle平台简介<br>&#160; &#160; &#160; &#160;2. Titanic罹难乘客预测<br>&#160; &#160; &#160; &#160;3. IMDB影评得分估计<br>&#160; &#160; &#160; &#160;4. MNIST手写数字识别</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kaggle相关的知识点.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Kaggle" scheme="http://yoursite.com/tags/Kaggle/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理---数据清洗 &amp; 特征工程</title>
    <link href="http://yoursite.com/2019/05/25/data-preprocess/"/>
    <id>http://yoursite.com/2019/05/25/data-preprocess/</id>
    <published>2019-05-25T08:48:52.000Z</published>
    <updated>2020-07-25T14:54:37.708Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>收集的一些比较重要的特征工程相关的博客链接.</strong><br><a id="more"></a></p></blockquote><h1 id="1-相应参考链接"><a href="#1-相应参考链接" class="headerlink" title="1.相应参考链接"></a>1.相应参考链接</h1><blockquote><p>&#160; &#160; &#160; &#160;1. <a href="http://www.cnblogs.com/charlotte77/" target="_blank" rel="noopener">数学系的数据挖掘民工(公众号:CharlotteDataMining)</a><br>&#160; &#160; &#160; &#160;2. <a href="https://tech.meituan.com/machinelearning-data-feature-process.html" target="_blank" rel="noopener">机器学习中的数据清洗与特征处理综述</a><br>&#160; &#160; &#160; &#160;3. <a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a><br>&#160; &#160; &#160; &#160;4. <a href="https://www.cnblogs.com/jasonfreak/category/823064.html" target="_blank" rel="noopener">随笔分类 - 特征工程</a><br>&#160; &#160; &#160; &#160;5. <a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">weibao—特征工程</a><br>&#160; &#160; &#160; &#160;6. <a href="https://blog.csdn.net/power0405hf/article/details/49644041" target="_blank" rel="noopener">数据清洗—power0405hf的专栏</a><br>&#160; &#160; &#160; &#160;7. <a href="https://zhuanlan.zhihu.com/p/20571505" target="_blank" rel="noopener">知乎专栏—数据清洗的一些梳理</a><br>&#160; &#160; &#160; &#160;8. <a href="https://blog.csdn.net/yen_csdn/article/details/53445616" target="_blank" rel="noopener">利用Python Pandas进行数据预处理-数据清洗</a><br>&#160; &#160; &#160; &#160;9. <a href="http://www.cnblogs.com/wkslearner/default.html?page=3" target="_blank" rel="noopener">molearner—python数据预处理</a><br>&#160; &#160; &#160; &#160;10. <a href="https://www.cnblogs.com/nxld/p/6085605.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">python数据清洗</a><br>&#160; &#160; &#160; &#160;11.<a href="http://bluewhale.cc/2016-08-21/python-data-cleaning.html" target="_blank" rel="noopener">蓝鲸的网站分析笔记—-使用python进行数据清洗</a><br>&#160; &#160; &#160; &#160;12.<a href="https://github.com/ferventdesert/etlpy/" target="_blank" rel="noopener">GitHub—-etlpy是基于配置文件的数据采集和清洗工具</a><br>&#160; &#160; &#160; &#160;13. <a href="https://www.dataivy.cn/blog/3-1-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%9A%E7%BC%BA%E5%A4%B1%E5%80%BC%E3%80%81%E5%BC%82%E5%B8%B8%E5%80%BC%E5%92%8C%E9%87%8D%E5%A4%8D%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86-2%E4%BB%A3%E7%A0%81/" target="_blank" rel="noopener">数据常青藤</a><br>&#160; &#160; &#160; &#160;14. <a href="https://my.oschina.net/dfsj66011/blog/601546" target="_blank" rel="noopener">Python数据清洗实用小工具</a><br>&#160; &#160; &#160; &#160;15. <a href="https://blog.csdn.net/yehui_qy/article/details/53791006" target="_blank" rel="noopener">机器学习-常见的数据预处理</a><br>&#160; &#160; &#160; &#160;16. <a href="http://www.cnblogs.com/zhizhan/p/4870397.html" target="_blank" rel="noopener">数据预处理（完整步骤）</a><br>&#160; &#160; &#160; &#160;17.<a href="https://zhuanlan.zhihu.com/p/31885996" target="_blank" rel="noopener">从零构建机器学习模型(一)数据预处理初阶</a><br>&#160; &#160; &#160; &#160;18. <a href="https://ljalphabeta.gitbooks.io/python-/content/missingvalue.html" target="_blank" rel="noopener">重点：处理缺失值</a><br>&#160; &#160; &#160; &#160;19. <a href="https://docs.transwarp.io/5.0/goto?file=MidasManual__%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html" target="_blank" rel="noopener">重点：数据预处理</a><br>&#160; &#160; &#160; &#160;20. <a href="https://github.com/jacksu/machine-learning/blob/master/markdown/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.md" target="_blank" rel="noopener">github—数据预处理</a><br>&#160; &#160; &#160; &#160;21. <a href="https://ask.hellobi.com/blog/CharlotteDataMining/10958" target="_blank" rel="noopener">深度学习系列—-PaddlePaddle之数据预处理</a><br>&#160; &#160; &#160; &#160;22. <a href="https://zhuanlan.zhihu.com/p/33030631" target="_blank" rel="noopener">Python数据处理 II：数据的清洗（预处理）</a><br>&#160; &#160; &#160; &#160;23. <a href="http://www.cnblogs.com/BoyceYang/p/8182053.html" target="_blank" rel="noopener">[数据清洗]-Pandas 清洗“脏”数据（一）</a><br>&#160; &#160; &#160; &#160;23. <a href="https://blog.csdn.net/ggwcr/article/details/78168623" target="_blank" rel="noopener">tensorflow图像数据预处理</a><br>&#160; &#160; &#160; &#160;23. <a href="https://blog.csdn.net/xinyu3307/article/details/74643019" target="_blank" rel="noopener">TensorFlow——训练自己的数据（一）数据处理</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;收集的一些比较重要的特征工程相关的博客链接.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数据清洗" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
    
      <category term="特征工程" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
</feed>
