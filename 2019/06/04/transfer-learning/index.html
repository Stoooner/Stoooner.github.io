<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>VGG16迁移学习分析 | Stoner的博客</title><meta name="description" content="整理使用VGG16框架进行迁移学习的方法步骤.  1. 迁移学习简介 深度学习中最强大的理念之一就是有时候神经网络可以从一个任务中学习到知识，并将这些知识应用到另一个独立的任务中。比如现在你训练好了一个神经网络能够识别像猫这样的对象，然后使用这些学习到的知识去帮助你更好的阅读X射线扫描图，这就是所谓的迁移学习。  2. VGG Network介绍 「VGG Network」，牛津大学VGG实验室"><meta name="keywords" content="卷积神经网络,迁移学习,VGG16"><meta name="author" content="Stoner"><meta name="copyright" content="Stoner"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2019/06/04/transfer-learning/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="VGG16迁移学习分析"><meta property="og:url" content="http://yoursite.com/2019/06/04/transfer-learning/"><meta property="og:site_name" content="Stoner的博客"><meta property="og:description" content="整理使用VGG16框架进行迁移学习的方法步骤.  1. 迁移学习简介 深度学习中最强大的理念之一就是有时候神经网络可以从一个任务中学习到知识，并将这些知识应用到另一个独立的任务中。比如现在你训练好了一个神经网络能够识别像猫这样的对象，然后使用这些学习到的知识去帮助你更好的阅读X射线扫描图，这就是所谓的迁移学习。  2. VGG Network介绍 「VGG Network」，牛津大学VGG实验室"><meta property="og:image" content="http://qe0z9wdl5.bkt.clouddn.com/20200725224325.png"><meta property="article:published_time" content="2019-06-04T05:42:10.000Z"><meta property="article:modified_time" content="2020-07-25T14:45:07.420Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="近期迁移学习相关网页" href="http://yoursite.com/2019/06/05/transfer-learning-Web-page/"><link rel="next" title="预测代码中遇到的问题" href="http://yoursite.com/2019/05/30/tensorflow-save-and-restore/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Stoner的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">48</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">62</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content toc-div-class" style="display:none"></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://qe0z9wdl5.bkt.clouddn.com/20200725224325.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Stoner的博客</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">VGG16迁移学习分析</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-06-04 13:42:10"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2019-06-04</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 22:45:07"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><blockquote>
<p><strong>整理使用VGG16框架进行迁移学习的方法步骤.</strong><br><a id="more"></a></p>
</blockquote>
<h2 id="1-迁移学习简介"><a href="#1-迁移学习简介" class="headerlink" title="1. 迁移学习简介"></a>1. 迁移学习简介</h2><blockquote>
<p>深度学习中最强大的理念之一就是有时候神经网络可以从一个任务中学习到知识，并将这些知识应用到另一个独立的任务中。比如现在你训练好了一个神经网络能够识别像猫这样的对象，然后使用这些学习到的知识去帮助你更好的阅读<strong>X射线扫描图</strong>，这就是所谓的<strong>迁移学习</strong>。</p>
</blockquote>
<h2 id="2-VGG-Network介绍"><a href="#2-VGG-Network介绍" class="headerlink" title="2. VGG Network介绍"></a>2. VGG Network介绍</h2><blockquote>
<p><strong>「VGG Network」</strong>，牛津大学<strong>VGG</strong>实验室设计的架构，将<strong>AlexNet</strong>的$8$层提高到了$19$层，真正让深度这个词得以充分体现。从<strong>VGG</strong>开始，人们不再使用太大的卷积核，取而代之的是若干个小卷积核的组合。比如， $3$个步长为$1$的$3 \times 3$卷积核，可以拥有同$1$个$7 \times 7$的卷积核一样的感受野，但是，它可以使整个网络变得更深，并且更具有非线性。同时，还能够进一步减少训练的参数量。</p>
</blockquote>
<p>下图是各个著名网络的对比相关信息以及<strong>「VGG Network」</strong>的信息：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559623117786.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559623163942.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559623226922.png" alt="Alt text"></p>
<h2 id="3-迁移学习实际操作步骤"><a href="#3-迁移学习实际操作步骤" class="headerlink" title="3. 迁移学习实际操作步骤"></a>3. 迁移学习实际操作步骤</h2><p>首先，整个操作过程借鉴一下文章：</p>
<blockquote>
<ol>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-16-transfer-learning/" target="_blank" rel="noopener">迁移学习 Transfer Learning</a></li>
<li><a href="https://www.cnblogs.com/zengfanlin/p/8886701.html" target="_blank" rel="noopener">用tensorflow迁移学习猫狗分类</a></li>
</ol>
</blockquote>
<p>其中这里的得到的<strong>VGG pre-trained model</strong>是使用<a href="https://github.com/machrisaa/tensorflow-vgg" target="_blank" rel="noopener">machrisaa/tensorflow-vgg</a>改写的<strong>VGG16的代码</strong>以及在他的<strong>GitHub</strong>下提供的训练好了的<strong>model parameters</strong>，也就是<strong>VGG16 NPY</strong>文件，如果下载不了的话，可以通过一下网页以百度云的方式下载：<a href="https://www.jianshu.com/p/c9d06e794393" target="_blank" rel="noopener">简书——vgg16.npy 和vgg19.npy</a>。</p>
<p>权重文件下载完毕之后，建立相应的项目，在项目下建立一个文件夹来存放<strong>VGG16.npy</strong>文件：<img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559624035247.png" alt="Alt text"></p>
<p>数据文件在上面的网页<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-16-transfer-learning/" target="_blank" rel="noopener">迁移学习 Transfer Learning</a>中可以找到下载，<strong>VGG16</strong>是针对<strong>ImageNet</strong>的$1000$个类别的众多图片做<strong>分类</strong>任务进行训练的，在本次的迁移学习任务中，我们利用了图片分类的网络，在最后迁移到利用$CNN$做回归的任务中去，对于前面的卷积层(<strong>CNN Layers</strong>)依旧保留，而后面的<strong>Fully Connected Layers</strong>，对其进行修改，将用于分类$1000$个<strong>softmax</strong>层删掉，在网上下载那$1000$个分类数据中的猫和老虎的图片, 然后伪造一些猫和老虎长度的数据。最后做到让迁移后的网络分辨出猫和老虎的长度 (<strong>regressor</strong>)的<strong>回归任务</strong>。</p>
<p>而项目目录下的<strong>transfer_learning.py</strong>文件就是迁移学习的具体代码，我们这里对几个重要的点一一作出分析。</p>
<h3 id="3-1-固定卷积层-训练全连接"><a href="#3-1-固定卷积层-训练全连接" class="headerlink" title="3.1 固定卷积层 + 训练全连接"></a>3.1 固定卷积层 + 训练全连接</h3><blockquote>
<p><strong>首先，对于迁移学习需要注意几点</strong>：</p>
<ol>
<li>如果你的数据集很小，你可能只需要<strong>重新训练最后一层的权重</strong>，并<strong>保持前面的参数不动</strong>，经验规则就是如果你的新的数据集比较小，那么就只训练输出层的最后一两层，而数据很多的时候，也许你可以重新训练神经网络中所有的参数；</li>
<li>如果放射科的数据足够多，那么你可以重新训练神经网络剩下的所有层；</li>
<li>当你新的数据足够多的时候，你重新训练神经网络中所有的参数的时候，那么在图像识别数据进行的初期训练阶段被称之为<strong>预训练(pre-training)</strong>，因为你使用图像识别数据去训练的时候，是以数据量多的那个任务预训练好的参数初始化神经网络的权重，然后如果你以后更新所有权重然后在另一个数据量少的任务上进行训练，这个过程被称之为<strong>微调(fine-tuning)</strong>。</li>
</ol>
</blockquote>
<p>在这个将$1000$个类别进行分类的任务迁移到动物体长预测的回归任务中，回归任务的数据量不是很多，因此，我们在这里将最后一层修改为回归神经元，在训练的时候只训练最后一层的参数。</p>
<p>那么迁移学习实际在代码中对应着什么意思呢？通过分析代码我发现，整体是这样的：</p>
<blockquote>
<ol>
<li>首先对于下图中用红色矩形框出的代码：<img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559625253556.png" alt="Alt text"><br>红色框的代码是一个非常重要的点，需要明确的是，<strong>self.data_dict</strong>用于存放<strong>VGG16</strong>预训练过后得到的参数，它是一个字典类型，存放着一共有16个键值对。<strong>data_dict</strong>的<strong>keys</strong>就是对应的每一层的名字(从<strong>conv1</strong>一直到<strong>fc8</strong>)，相应的，<strong>value</strong>则是一个巨型列表(每个元素又是一个<strong>numpy</strong>数组)，每一个<strong>key</strong>对应的<strong>value</strong>都包含两个元素(包含那一层的<strong>weights</strong>和<strong>bias</strong>)，因此迁移就体现在前面整个卷积层的参数是采取固定不动的状态，从<strong>VGG16</strong>文件中读取的，不参与训练；而后面如果将全连接层进行修改，则可训练参数是后面的全连接层。<strong>需要注意的是，最后在训练完毕之后，需要进行预测的时候，不单单需要将预训练的VGG16的参数读取出来，还需要将后面修改了的，进行训练了的全连接层(依你具体改了的层而定)的参数一起导入才能够进行预测。(反正改了什么层去训练，预测的时候就要相应的导入什么层的训练参数)</strong>。</li>
<li>将<strong>VGG16</strong>预训练的参数导出存放在一个字典里之后，该如何将其分发到每一层呢？具体如下：<blockquote>
<p>2.1. 首先需要根据前面图片中<strong>VGG16</strong>的框架结构，在代码中将其框架结构复现出来(这里暂时不算池化层，对框架中的全连接层将其改为一层全联机层用于回归，卷积层不改变)；<br>2.2. 复现前面卷积层的框架之后，定义下面的封装好的卷积层和池化层代码：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559626059290.png" alt="Alt text"><br>2.3. 在复现框架的过程中，对于权重(<strong>Weights</strong>)和偏置(<strong>bias</strong>)，使用下面方法获取预训练完毕的结果：<img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559626162296.png" alt="Alt text">，也就是说，从前面保存参数的字典中按照每一层对应的层名，以字典方式获取预训练完毕的参数结果。<br>2.4. 而最后几层的全连接层，由于需要修改以进行迁移，因此对于修改了的层就不需要导入预训练好了的参数值了，而是以相应的初始化方式进行初始化，然后训练修改了的那几层的参数，<strong>由此可得，在迁移学习中根据数据量的大小，对想要的层进行修改之后，未修改的层的参数就带入预训练好了的参数，而修改了的层的参数就按照某种方式进行初始化，然后仅仅训练想要的那几层好了。</strong></p>
</blockquote>
</li>
<li>最后训练完毕之后，准备进行预测的时候，前面卷积层的参数依旧是按照上面的方式从存放所有预训练参数的字典中获取，而修改了的进行训练了的层则需要从训练完毕的参数文件中进行读取，这样才能进行预测：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559626588927.png" alt="Alt text"></li>
</ol>
</blockquote>
<h3 id="3-2-VGG16各层参数的shape以及数目"><a href="#3-2-VGG16各层参数的shape以及数目" class="headerlink" title="3.2 VGG16各层参数的shape以及数目"></a>3.2 VGG16各层参数的shape以及数目</h3><pre><code>————————————————————————————————————————————————————————————
Layer                   Output Shape               Param #
============================================================
conv1_1_weights         (3, 3, 3, 64)              1728
conv1_1_bias            (64,)                      64
————————————————————————————————————————————————————————————
conv1_2_weights         (3, 3, 64, 64)             36864
conv1_2_bias            (64,)                      64
————————————————————————————————————————————————————————————
conv2_1_weights         (3, 3, 64, 128)            73728
conv2_1_bias            (128,)                     128
————————————————————————————————————————————————————————————
conv2_2_weights         (3, 3, 128, 128)           147456
conv2_2_bias            (128,)                     128
————————————————————————————————————————————————————————————
conv3_1_weights         (3, 3, 128, 256)           294912
conv3_1_bias            (256,)                     256
————————————————————————————————————————————————————————————
conv3_2_weights         (3, 3, 256, 256)           589824
conv3_2_bias            (256,)                     256
————————————————————————————————————————————————————————————
conv3_3_weights         (3, 3, 256, 256)           589824
conv3_3_bias            (256,)                     256
————————————————————————————————————————————————————————————
conv4_1_weights         (3, 3, 256, 512)           1179648
conv4_1_bias            (512,)                     512
————————————————————————————————————————————————————————————
conv4_2_weights         (3, 3, 512, 512)           2359296
conv4_2_bias            (512,)                     512
————————————————————————————————————————————————————————————
conv4_3_weights         (3, 3, 512, 512)           2359296
conv4_3_bias            (512,)                     512
————————————————————————————————————————————————————————————
conv5_1_weights         (3, 3, 512, 512)           2359296
conv5_1_bias            (512,)                     512
————————————————————————————————————————————————————————————
conv5_2_weights         (3, 3, 512, 512)           2359296
conv5_2_bias            (512,)                     512
————————————————————————————————————————————————————————————
conv5_3_weights         (3, 3, 512, 512)           2359296
conv5_3_bias            (512,)                     512
————————————————————————————————————————————————————————————
fc6_weights             (25088, 4096)              102760448
fc6_bias                (4096,)                    4096
————————————————————————————————————————————————————————————
fc7_weights             (4096, 4096)               16777216
fc7_bias                (4096,)                    4096
————————————————————————————————————————————————————————————
fc8_weights             (4096, 1000)               4096000
fc8_bias                (1000,)                    1000
————————————————————————————————————————————————————————————
============================================================
Total params: 138357544
</code></pre><p>由此可见，<strong>vgg16.npy</strong>文件很大不是没有原因的。</p>
<h2 id="4-如何固定住参数"><a href="#4-如何固定住参数" class="headerlink" title="4. 如何固定住参数"></a>4. 如何固定住参数</h2><blockquote>
<p>首先需要明确一点的是，<strong>tf.Optimizer</strong>只优化<strong>tf.GraphKeys.TRAINABLE_VARIABLES</strong>中的变量，也就是说在进行<strong>Backpropagation</strong>参数更新的，只会回传梯度到<strong>Variable(变量)</strong>上去，常量是被固定住了，不会被更新的(<a href="https://blog.csdn.net/touch_dream/article/details/78446863" target="_blank" rel="noopener">tf常用集合及其获取方式</a>)。<br>因此在<strong>迁移学习</strong>中对于很多预训练了的框架，想要在迁移的时候对部分参数进行固定以避免训练的话，可以通过使用<strong>tf.constant</strong>或者<strong>python</strong>变量的形式来规避常量被训练。例如：<a href="https://blog.csdn.net/xys430381_1/article/details/88885580" target="_blank" rel="noopener">tensorflow —-迁移学习中如何只更新部分参数</a>，在这篇博客里详细讨论了如何固定部分参数，只训练另一部分参数的方法技巧。</p>
<hr>
<p>这里再附上一些相关的参考博客：</p>
<ol>
<li><a href="https://blog.csdn.net/grllery/article/details/80380332" target="_blank" rel="noopener">迁移学习之—tensorflow选择性加载权重</a></li>
<li><a href="https://blog.csdn.net/u014381600/article/details/71511794" target="_blank" rel="noopener">迁移学习技巧以及如何更好的finetune 模型(重点)</a></li>
<li><a href="https://www.jianshu.com/p/07426ba049b6" target="_blank" rel="noopener">简书——迁移学习总结</a></li>
<li><a href="https://blog.csdn.net/LoseInVain/article/details/78935157" target="_blank" rel="noopener">利用numpy数组保存TensorFlow模型的参数</a></li>
<li><a href="https://blog.csdn.net/qq_40108803/article/details/83627532" target="_blank" rel="noopener">读取VGG16网络生成的.npy文件的参数</a></li>
<li><a href="https://blog.csdn.net/qq_40614981/article/details/81001971" target="_blank" rel="noopener">模型保存文件.npy</a></li>
<li><a href="https://codeday.me/bug/20190401/856668.html" target="_blank" rel="noopener">python – Tensorflow：tf.get_collection不返回范围中的变量</a></li>
<li><a href="https://blog.csdn.net/shwan_ma/article/details/78879620" target="_blank" rel="noopener">[tensorflow]打印Tensorflow graph中的所有需要训练的变量—tf.trainable_variables()</a></li>
<li><a href="http://www.voidcn.com/article/p-mepbcmhk-bgt.html" target="_blank" rel="noopener">tensorflow常用函数及概念</a></li>
</ol>
</blockquote>
<p>在本代码中，由于卷积层是直接使用的<strong>VGG16</strong>的模型框架，注入的参数也是直接使用的是<strong>.npy</strong>文件导出的数据进行初始化的，而不是使用的<strong>TensorFlow</strong>中<strong>Varibale的方式初始化的</strong>。因此在之后训练的时候只会更新后改了的全连接层的参数(修改了的全连接层的参数通过<strong>Varibale的方式初始化的</strong>)，而前面的都是<strong>常量(constant)</strong>。这一点可以通过<code>tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))</code>的方式看出：<br><img src= "/img/loading.gif" data-src="/2019/06/04/transfer-learning/1559675934438.png" alt="Alt text"><br>打印出来的结果如下：</p>
<pre><code>[ &lt; tf.Variable &#39;fc6/kernel:0&#39; shape = (25088, 256) dtype = float32_ref &gt;,
  &lt; tf.Variable &#39;fc6/bias:0&#39; shape = (256,) dtype = float32_ref &gt;,
  &lt; tf.Variable &#39;out/kernel:0&#39; shape = (256, 1) dtype = float32_ref &gt;,
  &lt; tf.Variable &#39;out/bias:0&#39; shape = (1,) dtype = float32_ref &gt;]
</code></pre><p>可以看出<strong>Variable</strong>变量只有后面修改了的全连接层的参数，前面的卷积层的参数都是常量，不参与到参数更新，因此也就被固定住了，不参与到训练过程中。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Stoner</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/06/04/transfer-learning/">http://yoursite.com/2019/06/04/transfer-learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">Stoner的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a><a class="post-meta__tags" href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">迁移学习</a><a class="post-meta__tags" href="/tags/VGG16/">VGG16</a></div><div class="post_share"><div class="social-share" data-image="http://qe0z9wdl5.bkt.clouddn.com/20200725214526.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/06/05/transfer-learning-Web-page/"><img class="prev-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725220507.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">近期迁移学习相关网页</div></div></a></div><div class="next-post pull-right"><a href="/2019/05/30/tensorflow-save-and-restore/"><img class="next-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725224620.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">预测代码中遇到的问题</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/05/22/Weights-Pred-of-Pigs/" title="Weights of Predictions of Pigs"><img class="relatedPosts_cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725232147.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2019-05-22</div><div class="relatedPosts_title">Weights of Predictions of Pigs</div></div></a></div><div class="relatedPosts_item"><a href="/2019/06/06/model-pd/" title="迁移学习中预训练模型的保存文件"><img class="relatedPosts_cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725220814.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-06</div><div class="relatedPosts_title">迁移学习中预训练模型的保存文件</div></div></a></div><div class="relatedPosts_item"><a href="/2019/06/05/transfer-learning-Web-page/" title="近期迁移学习相关网页"><img class="relatedPosts_cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725220507.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-05</div><div class="relatedPosts_title">近期迁移学习相关网页</div></div></a></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Stoner</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/search/local-search.js"></script><script>if (document.getElementsByClassName('mermaid').length) {
  loadScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js',function () {
    mermaid.initialize({
      theme: 'default',
  })
})
}</script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>