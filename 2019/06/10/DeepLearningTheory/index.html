<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>深度学习理论 | Stoner的博客</title><meta name="description" content="记录深度学习理论课程相关内容.  1.1 Can shallow network fit any function  上图是说：给定一个network，以及相应的parameter(weights and biases)，那么这个network代表了一个function，而同样structure的network，在填入不同的parameter(weights and biases)的时候就对应不"><meta name="keywords" content="理论推导"><meta name="author" content="Stoner"><meta name="copyright" content="Stoner"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2019/06/10/DeepLearningTheory/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="深度学习理论"><meta property="og:url" content="http://yoursite.com/2019/06/10/DeepLearningTheory/"><meta property="og:site_name" content="Stoner的博客"><meta property="og:description" content="记录深度学习理论课程相关内容.  1.1 Can shallow network fit any function  上图是说：给定一个network，以及相应的parameter(weights and biases)，那么这个network代表了一个function，而同样structure的network，在填入不同的parameter(weights and biases)的时候就对应不"><meta property="og:image" content="http://qe0z9wdl5.bkt.clouddn.com/20200725220312.png"><meta property="article:published_time" content="2019-06-10T05:10:38.000Z"><meta property="article:modified_time" content="2020-07-25T14:03:28.010Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="重要卷积神经网络结构分析" href="http://yoursite.com/2019/06/10/Import-CNN-Framework/"><link rel="next" title="迁移学习中预训练模型的保存文件" href="http://yoursite.com/2019/06/06/model-pd/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Stoner的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="http://qe0z9wdl5.bkt.clouddn.com/20200726202347.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">48</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">62</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content toc-div-class" style="display:none"></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://qe0z9wdl5.bkt.clouddn.com/20200725220312.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Stoner的博客</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">深度学习理论</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-06-10 13:10:38"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2019-06-10</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 22:03:28"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><blockquote>
<p><strong>记录深度学习理论课程相关内容.</strong><br><a id="more"></a></p>
</blockquote>
<h2 id="1-1-Can-shallow-network-fit-any-function"><a href="#1-1-Can-shallow-network-fit-any-function" class="headerlink" title="1.1 Can shallow network fit any function"></a>1.1 Can shallow network fit any function</h2><blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558080065257.png" alt="Alt text"></p>
<p>上图是说：给定一个<strong>network</strong>，以及相应的<strong>parameter(weights and biases)</strong>，那么这个<strong>network</strong>代表了一个<strong>function</strong>，而同样<strong>structure</strong>的<strong>network</strong>，在填入不同的<strong>parameter(weights and biases)</strong>的时候就对应不同的<strong>function</strong>。对于一个给定了的<strong>network</strong>的架构(<strong>structure</strong>)的时候，在给定各式各样的<strong>parameter</strong>的情况之下，其实就是在所有可能的<strong>function</strong>里面划定一个范围(<strong>network space</strong>)。</p>
<hr>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558080739654.png" alt="Alt text"></p>
<p>上图是说需要一个<strong>network</strong>去拟合图上的函数，在某一要求下，比方说要求<strong>network</strong>拟合函数达到某个精度的要求下，<strong>deep</strong>的<strong>network</strong>要比<strong>shallow</strong>的<strong>network</strong>的参数量少。</p>
<hr>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558080994943.png" alt="Alt text"></p>
<hr>
<p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558081221451.png" alt="Alt text"></p>
<p> 上图是说如果一开始一个很<strong>shallow(small)</strong>的<strong>network</strong>不能覆盖到<strong>target function</strong>所在的<strong>function space</strong>，那么随着它的<strong>units</strong>的数目增长的越来越多，最终肯定是能<strong>fit</strong>到我们给定的<strong>target function</strong>的(后面讲到).</p>
<p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558081479151.png" alt="Alt text"></p>
<hr>
<p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558081540719.png" alt="Alt text"></p>
<p> <img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082005754.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082208813.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082496627.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558082840572.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558083173657.png" alt="Alt text"><br>上图中<strong>l</strong>的两个绿色点直接连线的距离，而<strong>L</strong>是<strong>L-Lipschitz</strong>中的那个<strong>L</strong>。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558084419662.png" alt="Alt text"></p>
<p>$error &lt; \Vert{f(x_1) - f(x_2)}\Vert \leq L\Vert{x_1 - x_2}\Vert &lt;l \times L \leq \xi    \Longrightarrow  l \leq \frac{\xi}{L}$<br>其中$ \Vert{f(x_1) - f(x_2)}\Vert \leq L\Vert{x_1 - x_2}\Vert$，且$\max_{0 \leq x \leq 1} \vert{f(x) - f^*(x)}\vert  \leq \xi$</p>
<p>因此，给定一个<strong>L-Lipschitz Function</strong>$f^<em>(x)$，需要找到一个<strong>piece-wise linear Function</strong>，让两者之间的<strong>error</strong>小于等于$\xi$，只需要做到在取线段的时候，相互之间的距离都取$\frac{\xi}{L}$就可以了，这样<strong>error</strong>就是小于等于$\xi$<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558086718838.png" alt="Alt text"><br>由此还可以得出，假设区间范围是$[0, 1]$之间，每个<strong>segment</strong>都是$\frac{\xi}{L}$，则<strong>segment</strong>的个数为$\frac{L}{\xi}$，也就是有$\frac{L}{\xi}$个<strong>piece-wise linear Function</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087000210.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087049947.png" alt="Alt text"><br>因此，按照证明，只要做出上面那个<strong>piece-wise linear function</strong>，我们就能<strong>fit</strong>我们的<strong>target function</strong>(<strong>L-Lipschitz function</strong>)，让它们的<strong>error</strong>小于等于$\xi$，那么有没有办法通过只有一个<strong>hidden layer</strong>，且<strong>activation function</strong>都是<strong>relu</strong>的<strong>network</strong>制造出上图中绿线那样的<em>*piece-wise linear function</em></em>呢？答案是可以的。方法之一：</p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087441622.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087711894.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558087994599.png" alt="Alt text"><br>上面两张图就是在解释使用两个<strong>relu</strong>可以组合成<strong>1</strong>个<strong>segment</strong>，而$\frac{L}{\xi}$个<strong>segment</strong>(<strong>piece-wise linear Function</strong>)，需要$\frac{2L}{\xi}$个<strong>relu neurons</strong>，就可以近似一个<strong>L-Lipschitz function</strong>，但是需要声明的是这个只是说可以做到，并没有说是效率最高的做法。这里有一个比较俗语的解释是上面成立的一个原因是因为，<strong>L</strong>越大，代表<strong>function</strong>的变化越快，则是一个越复杂的函数，那越复杂的函数就需要越多的神经元；相应的，$\xi$越小，则说明要求的精准度越高，则$\xi$越小在需要的神经元个数就越多。</p>
</blockquote>
<h2 id="1-2-Potential-of-Deep"><a href="#1-2-Potential-of-Deep" class="headerlink" title="1.2 Potential of Deep"></a>1.2 Potential of Deep</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097021232.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097421561.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097519589.png" alt="Alt text"></p>
<blockquote>
<p>类似逻辑电路一样(类比)，层数更多的逻辑电路需要的<strong>gate</strong>更少，而层数更多的神经网络在拟合同一个函数的时候，需要的神经元更少<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097545982.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558097751934.png" alt="Alt text"><br>上图所说的是：在参数量相同的情况下，<strong>deep network</strong>产生的线性分段要比<strong>shallow network</strong>更多。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558098139481.png" alt="Alt text"><br>上图中，<strong>relu</strong>是分段的，一部分的输出是<strong>0</strong>，还有一部分的输出和输入相同，对于采用<strong>relu</strong>作为激活函数的<strong>network</strong>，输出为<strong>0</strong>的那些可以拿掉，当做不存在一样，剩下的部分就好像是一个<strong>linear function</strong>，不过需要注意的是不能因此说<strong>relu</strong>是<strong>linear function</strong>，它是<strong>piece-wise linear function</strong>，当输入值达到某个阈值的时候<strong>relu</strong>就会从一种<strong>linear</strong>的输出状态到达另一种<strong>linear</strong>的输出状态。由于每个神经元在<strong>relu</strong>的左右下都有<strong>两种</strong>激活模式<strong>（ activation pattern）</strong>，因此如果有<strong>N</strong>个神经元，则有$2^N$种<strong>activation pattern</strong>，每一个<strong>activation pattern</strong>都制造了一个<strong>linear piece</strong>，所以总的也就是有$2^N$个<strong>linear pieces</strong>，但是需要注意的是这个是一个<strong>upper bound(最佳上限)</strong>，而这个上限可能永远都没有办法实现，有些<strong>pattern</strong>可能是永远也不会出现的，比方说：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099092748.png" alt="Alt text"></p>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099266392.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099615364.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099694294.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558099865914.png" alt="Alt text"></p>
<blockquote>
<p>从上面的图可以看出，当我们用<strong>deep structure</strong>的时候，每多加一层，线段的数目都变成<strong>double</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558100023891.png" alt="Alt text"><br>在上面的图中，每个<strong>abs activation function</strong>都是由两个<strong>relu activation function</strong>组成，有前面所讲的，每次两个神经元(<strong>每个神经元的激活都是relu</strong>)组成一个线段，那么对于一个<strong>shallow network</strong>，如果需要产生<strong>100</strong>个线性分段，则需要<strong>200</strong>个神经元；但是如果使用的是<strong>deep network</strong>，每层有两个神经元，每次多加一层，线段的数目就会增加两倍，所以对于同样的<strong>100</strong>个线性分段，则只需要<strong>7</strong>层就可以实现了。因此需要产生比较多的线性片段的时候，使用<strong>deep</strong>是比较有效率的。深层的网络把之前的<strong>pattern</strong>重复的组合起来，有规律的重复出来。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558101610973.png" alt="Alt text"><br>由上面的宽度为<strong>2</strong>，深度为<strong>N</strong>可以得出有$2^N$个线性分段的推论可以得出：宽度为$K$，深度为$H$的网络可以得出$K^H$个线性分段，以上就是<strong>network</strong>架构可以产生的线性分段的<strong>lower bound</strong>。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558102008385.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558102213746.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558102275286.png" alt="Alt text"><br>上面两图是验证<strong>network</strong>的<strong>low layer</strong>的参数是比较重要的：左边的是在<strong>CIFAR 10</strong>上面做的实验，其中往不同层的参数上加上噪音，如果噪音加在最后一层的话，对结果几乎没有什么影响，但是一样的噪声加在第一层，整个结果就坏掉了。右边的是在<strong>MNIST</strong>数据集上的，然后分别对第一层进行训练，其他都是<strong>random</strong>的，以及分别往后，一直到只对最后一层进行训练，其他都是<strong>random</strong>的，最上面的紫色就是只训练第一层的结果，也就是只训练第一层，别的不管的情况下，结果都是很不错的，然后，只训练最后一层，结果就会坏掉。</p>
<hr>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103014973.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103428049.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103746080.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558103919220.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104057330.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104144772.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104349911.png" alt="Alt text"></p>
<h2 id="1-3-Is-Deep-better-than-Shallow"><a href="#1-3-Is-Deep-better-than-Shallow" class="headerlink" title="1.3 Is Deep better than Shallow"></a>1.3 Is Deep better than Shallow</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104397074.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104528234.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558104600075.png" alt="Alt text"><br>需要注意的是，上图中右边是实际<strong>relu</strong>达不到的一种状态，因为是不连续的，只是这里为了后面的证明，假设在梦幻状态下可以达到。</p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558142786551.png" alt="Alt text"><br>现在假设在横轴上取两点$x_0$和$x_0 + l$，两点之间的间隔为$l$，然后我们假设要取一条直线$ax + b$取拟合这条红色的线($y = x^2$)，这条直线不需要它的头和尾在<strong>target function</strong>上面，那么这条直线在给定间距$l$的情况下能够拟合的多好呢？那么实际的也就是求解 $error^2 = \int_{x_0}^{x_0+l} (x^2 - (ax + b))^2 \, {\rm d}x \Longleftrightarrow \sqrt{\int_0^1 \vert f(x) - f^*(x) \vert ^2 \, {\rm d}x} \leq \xi$，然后找出一个$a$和$b$使得$error$最小，其最小结果为：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558143543203.png" alt="Alt text"><br>其中的数学推导提醒：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558143795151.png" alt="Alt text"><br>那么上面是对于一条直线进行拟合的过程，那么如果现在在区间$[0, 1]$之间有$n$条线段($\sum_{i=1}^{n} l_i = 1$)进行拟合，那如何分配这$n$条线段使得$error$的值最小：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558144193000.png" alt="Alt text"><br>由于对于一条线段的时候$e^2 = \frac{l^5}{180}$，那么对于多个分段的线段来说，每个的$(e_i)^2 = \frac{(l_i)^5}{180}$，因此总的$error^2$为：</p>
<script type="math/tex; mode=display">E^2 = \sum_{i=1}^{n} (e_i)^2 = \sum_{i=1}^{n} \frac{(l_i)^5}{180}</script><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558144599224.png" alt="Alt text"><br>所以现在需要考虑的是如何分配 $l_1$ 到 $l_n$ 使得 $E^2$ 最小呢？正确的答案是间隔$l_i$平均分配(感觉有点像是最大熵模型)：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558144869756.png" alt="Alt text"><br>由于采用间隔平均分配，因此：</p>
<script type="math/tex; mode=display">l_i = \frac{1}{n}</script><script type="math/tex; mode=display">E^2 = \sum_{i=1}^{n} \frac{(1/n)^5}{180} = \frac{1}{180}·\frac{1}{n^4}</script><p>平均分配的原因：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558145611525.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558145771413.png" alt="Alt text"><br>因此，当$n$取上面的值的时候(也就是有上面那么多的<strong>linear pieces</strong>)才能让$Error \leq \xi$，而我们知道，每一个<strong>piece</strong>在<strong>shallow</strong>的状况下都需要一个<strong>neuron</strong>来制造，因此在<strong>shallow</strong>状况下我们仍然需要$O(\frac{1}{\sqrt \xi})$个<strong>neurons</strong>才能够去拟合$y = x^2$<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558146848316.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558146860041.png" alt="Alt text"><br>因此在给了<strong>shallow</strong>梦幻状态的情况之下，结果<strong>shallow</strong>的最佳状态依旧是$O(\frac{1}{\sqrt \xi})$，因此可以得出<strong>deep</strong>是比<strong>shallow</strong>好的(虽然没有去证实$O(\log_2(\frac{1}{\sqrt \xi})))$是否是<strong>deep</strong>最好的状态).<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147073605.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147082431.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147223285.png" alt="Alt text"><br><strong>需要注意的是，上面说的是存在某一个function(文章中的应该？)，而不是说所有的function都不行。</strong></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147459349.png" alt="Alt text"><br>上面这张图就是给出再上面的两张图中的某一个函数，这里给出的是一个球状的函数，球外面全是<strong>0</strong>，球里面全是<strong>1</strong>，结果是不管浅层的网络的宽度如何增加，损失依旧没有办法降下来，但是深层的网络，宽度只在<strong>100</strong>的时候就能降下来了。</p>
<p>但是需要主要的是不是所有的<strong>function</strong>都是<strong>deep</strong>比<strong>shallow</strong>的好(比方说<strong>linear</strong>)，因此下面的文章给出结论：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147593069.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558147845171.png" alt="Alt text"><br>因此结论是：比$x^2$还简单的(比如$y = x$)是不符合<strong>deep</strong>比<strong>shallow</strong>更好的，但是比$x^2$复杂的就是深得比浅的好，所以深度学习是有用的。</p>
<h2 id="2-1-When-Gradient-is-Zero"><a href="#2-1-When-Gradient-is-Zero" class="headerlink" title="2.1 When Gradient is Zero"></a>2.1 When Gradient is Zero</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558148468485.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558148984222.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558149644026.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162352495.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162375183.png" alt="Alt text"></p>
<h3 id="2-1-1-Hessian-Matrix-When-Gradient-is-Zero"><a href="#2-1-1-Hessian-Matrix-When-Gradient-is-Zero" class="headerlink" title="2.1.1 Hessian Matrix: When Gradient is Zero"></a>2.1.1 Hessian Matrix: When Gradient is Zero</h3><blockquote>
<p><strong>Critical Point: </strong>驻点；<strong>stuck: </strong>卡住，无法移动；<strong>curvature: </strong>曲率；<strong>invertible：</strong>可逆的；<br><strong>dominant：</strong>主导<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162442951.png" alt="Alt text"></p>
<p>需要注意的是训练到一定程度的时候，出现损失基本不再下降，梯度趋近到<strong>0</strong>的时候，不要随随便便就说走到了<strong>local minima/global minima</strong>，因为还有可能是<strong>saddle point(鞍点: 并非极值，但是梯度依旧为0)</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558162665301.png" alt="Alt text"><br>那么要区分是上面哪一种的时候，就需要使用<strong>Hessian Matrix: </strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558163693705.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558163703389.png" alt="Alt text"></p>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558165032965.png" alt="Alt text"></p>
<blockquote>
<p>对于上图的解释是: 假设$f(x)$实际是图中蓝色的曲线，如果现在排除掉包含$H$的那一项，只考虑前面的两项，则实际得到的结果是只能得到绿色的虚线。因此蓝色实线和绿色虚线的差异就是在于<strong>H</strong>所在的那一项。</p>
</blockquote>
<p>因此有了<strong>Hessian Matrix</strong>之后就可以定义新的<strong>Optimize</strong>的方法(也就是<strong>Newton’s method</strong>)。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558166465037.png" alt="Alt text"><br>上图中，黑色是$f(x)$，前两项是绿色的实现，梯度无变化；而三项都考虑则变成红色的曲线，梯度则有了变化，我们期望通过某种方法找到红色曲线的最小值(梯度为<strong>0</strong>)的地方，方法就是对整个式子求一下导数为<strong>0</strong>的地方。</p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558167309487.png" alt="Alt text"><br>上图中，相较于梯度下降仅能靠梯度指引方向的方式，牛顿法能够通过$H^{-1}$($H^{-1}$自动决定了<strong>learning_rate</strong>要有多大)一步直达最低点。</p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558168321883.png" alt="Alt text"><br>上图中$f(\theta)$为黑色的曲线，而后面三项的组合为红色的曲线，以某个点$x_0$为初始点，当$\theta$和$\theta_0$非常接近的时候，红色的线才和黑色的线很贴近，那从初始点，初始的红线开始，牛顿法会一步走到初始红线的最低点，得到$x_1$，然后在$x_1$这个地方会重新计算$g$和$H$，然后得到新的第二条红线，然后牛顿法又是一步到第二条红色的二次曲线最低点，得到$x_2$，然后继续。。。所以相较于<strong>Gradient descent</strong>，牛顿法每次可以走一大步。当$f(x)$是二次函数的时候(<strong>quadratic function</strong>)，牛顿法直接一步到位。</p>
<p>但是为何有这么强方法，却不使用呢？因为牛顿法在深度学习中不好用，如下：</p>
<blockquote>
<ol>
<li>需要计算$H^{-1}$，因为在深度学习中参数的量动则就是几千万甚至上亿，由于<strong>H</strong>是$n \times n$的，所以要算它的$H^{-1}$基本是不可能的。</li>
<li>牛顿法是寻找梯度为<strong>0</strong>的点，但是梯度为<strong>0</strong>的点不一定是最值点，有可能是鞍点，因为在深度学习所定义出来的<strong>Loss function</strong>里，有很多<strong>gradient</strong>为<strong>0</strong>的点，不是<strong>local minima</strong>，而是<strong>saddle point</strong>.</li>
<li><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558168505545.png" alt="Alt text"></li>
</ol>
</blockquote>
<p>那么上面讲解<strong>Hessian Matrix</strong>的意义何在呢？<br>假设在用梯度下降的时候<strong>updata</strong>参数的时候，到了某一步，梯度很小的时候，这个时候$f(\theta)$的特性就会变成由<strong>Hessian</strong>主导，通过<strong>H</strong>告诉我们<strong>critical point</strong>到底属于那种情况.<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558168940567.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558169051514.png" alt="Alt text"></p>
<blockquote>
<p><strong>positive/negative definite: </strong>正定/负定<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558169310295.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558170532234.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558170569338.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558171129367.png" alt="Alt text"><br>上图是说，我们从$\theta_0$这个地方移动一个$H$的特征向量$v$，我们会增加特征值大小的值(假设特征值为正的时候).<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558171518613.png" alt="Alt text"><br>上图是说，使用<strong>Hessian</strong>的特征值和特征向量分析<strong>critical point</strong>的时候：<br>① 当$u$是$H$的特征向量的线性组合(这个随意的方向$u$一定是$H$的特征向量的线性组合，对称矩阵的性质)，且所有的特征值都是正的，那么往$u$的方向走过去的时候，$f(\theta)$会增加对应特征值大小的值，则现在的状态是对于<strong>local minima</strong>;<br>② 当$u$是$H$的特征向量的线性组合，且所有的特征值都是负的，那么往$u$的方向走过去的时候，$f(\theta)$会减小对应特征值大小的值，则现在的状态是对于<strong>local maxima</strong>;<br>③ 当$u$是$H$的特征向量的线性组合，且所有的特征值有正有负，那么往某些方向的时候，$f(\theta)$会增加，相反，$f(\theta)$会减少，则现在的状态是对于<strong>saddle point</strong>;</p>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558172883403.png" alt="Alt text"></p>
<blockquote>
<p>上面的例子中如何计算<strong>Hessian Matrix</strong>的种类：<br>假设特征向量如下：<script type="math/tex">v = \begin{bmatrix} a & b \\  \end{bmatrix}</script><br>因此：</p>
<script type="math/tex; mode=display">v^\mathrm T H v</script><script type="math/tex; mode=display">= \begin{bmatrix} a & b \\  \end{bmatrix} ^\mathrm T· \begin{bmatrix} 2 & 0 \\ 0 & 6 \\  \end{bmatrix} · \begin{bmatrix} a & b \\  \end{bmatrix}</script><script type="math/tex; mode=display">= \begin{bmatrix} 2a & 6b \\  \end{bmatrix}·\begin{bmatrix} a  \\ b \\  \end{bmatrix}</script><script type="math/tex; mode=display">= 2a^2 + 6b^2 > 0</script><script type="math/tex; mode=display">\because (a,b \neq 0)</script><script type="math/tex; mode=display">\therefore H \implies Positive-definite</script><script type="math/tex; mode=display">\implies local-minima</script></blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558172939358.png" alt="Alt text"></p>
<hr>
<blockquote>
<p><strong>Degenerate: </strong>衰退，退化<br>退化矩阵(奇异矩阵)：奇异阵是行列式为0的方阵，是特征值（奇异值）含有0的方阵。<br><strong>Degenarate Hessian has at least one zero eigen value代表的意思是往相应的那个方向的特征向量走，不增也不减.</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558183857197.png" alt="Alt text"><br>④因此当我们发现$H$有$zero vecror$的时候，我们没有办法判断在$zero eigen value$的方向是增加还是减少。当$Hessian$那项为<strong>0</strong>的时候，其他被抹去的项就起作用了。所以到$H$有特征值为<strong>0</strong>的时候，这个时候用<strong>Hessian</strong>判断是<strong>local minima、local maxima、saddle point</strong>就不适用了(无法判断了，都有可能，这个时候其中用去决定是哪一种的就是被抹去的更高次的项了)。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558184814846.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558184921722.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185042995.png" alt="Alt text"></p>
</blockquote>
<hr>
<blockquote>
<p>当你在训练的时候，<strong>loss</strong>不再下降了，上面的那些讨论都是假设是出于<strong>critical point(gradient为0的情况)</strong>，然后基于<strong>Hessian</strong>来讨论是出于<strong>local minima/local maxima/saddle point</strong>哪种；但是实际上还有可能<strong>training</strong>停下来了，并不代表你一定走到了<strong>critical point(gradient为0的情况)</strong>，因为这个时候你的<strong>gradient</strong>还有可能不为<strong>0</strong>。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185156166.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185608066.png" alt="Alt text"><br>上面也是这样的情况，并非到了<strong>critical point</strong>的地方，分析可能是更加复杂的地方，<strong>gradient</strong>变小的区域里，可能逐渐靠近一个<strong>saddle point</strong>，增大则可能是逃出了这个<strong>saddle point</strong>，在变小则可能是靠近了另一个<strong>saddle point</strong>… …</p>
</blockquote>
<h2 id="2-2-Deep-Linear-Network"><a href="#2-2-Deep-Linear-Network" class="headerlink" title="2.2 Deep Linear Network"></a>2.2 Deep Linear Network</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558185790834.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558186237347.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558186630036.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558187390194.png" alt="Alt text"></p>
<blockquote>
<p>上面计算出来在$w_1、w_2 = 0$的原点处是一个<strong>saddle point</strong>.而在$w_1·w_2 = 1$处则是<strong>critical point</strong>(在上面的热力图中可以看出，而且是<strong>global minima</strong>)。<br><strong>利用Hessian 计算critical point的性质.</strong><br><strong>hessian 有0的特征值的时候是，那个方向是分析是哪种情况是不好分析的，除非以某些具体数据去算算看。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558188314135.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558188466702.png" alt="Alt text"></p>
</blockquote>
<hr>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189160490.png" alt="Alt text"></p>
<blockquote>
<p>对于上面的深度线性网络，可以证明，只要满足一些非常宽松的条件，这个线性网络不管叠加多少层，它的所有局部极小值都是全局最小值。比方说一些文章给出的宽松条件是<strong>Hidden layer size &gt;= input dim, output dim</strong>, 比如输入和输出的维度是5维，那么中间每一个隐层的输出维度都要大于或者等于5维即可。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189596902.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189619568.png" alt="Alt text"></p>
</blockquote>
<h2 id="2-3-Does-Deep-Network-have-Local-Minima"><a href="#2-3-Does-Deep-Network-have-Local-Minima" class="headerlink" title="2.3 Does Deep Network have Local Minima"></a>2.3 Does Deep Network have Local Minima</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558189862012.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558190399092.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558190572757.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558190855626.png" alt="Alt text"><br><strong>上图说明初始化很重要，否则容易陷入盲点，造成训练落入local_minima,最后崩坏。</strong></p>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191303644.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191457103.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191502066.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558191556193.png" alt="Alt text"><br><strong>这一节就是说Deep Non-linear Network</strong>是存在<strong>local minima</strong>的，上面那些文献就是在设计各种实验去找出什么情况下容易遇到<strong>local minima</strong>，可能以后会有新的理论告诉我们应该如何设计网络(如何初始化，什么样的结构，什么样的数据…)，使用什么样的条件，就能有效的避开<strong>local minial</strong>，从而找到<strong>global minima</strong></p>
<h2 id="2-4-Geometry-of-Loss-Surface-Conjecture"><a href="#2-4-Geometry-of-Loss-Surface-Conjecture" class="headerlink" title="2.4 Geometry of Loss Surface(Conjecture)"></a>2.4 Geometry of Loss Surface(Conjecture)</h2><blockquote>
<ol>
<li><strong>Geometry: </strong>n.    几何(学); 几何形状; 几何图形; 几何结构;</li>
<li><strong>Conjecture: </strong>n.    猜测; 推测; 揣测; 臆测; v.    猜测; 推测;</li>
<li><strong>Empirical: </strong>adj.    以实验(或经验)为依据的; 经验主义的;</li>
</ol>
</blockquote>
<p>现在在<strong>Deep Learning</strong>的领域有这么一个推论：几乎所有的<strong>local minimum</strong>它的<strong>loss</strong>，跟<strong>global optinum</strong>都是差不多的，因此如果卡在了<strong>local minimum</strong>也不要惊慌，因它跟<strong>global minimum</strong>的<strong>loss</strong>是差不多的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230298561.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230627503.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230781646.png" alt="Alt text"></p>
<blockquote>
<p>上图是基于再上面一张图的假设(假设特征值一半为正，一半为负，至于为什么？不管，文章这么假设的)说明我们的网络越大，遇见<strong>saddle point</strong>的可能性越大。</p>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558230976183.png" alt="Alt text"></p>
<blockquote>
<p>现在假设几率不再是$\frac{1}{2}$, 而是跟现在的<strong>loss</strong>有关了，现在假设几率为$p$(上图中的<strong>error</strong>指的就是<strong>loss</strong>).<br>需要注意的是：<strong>Larger error，larger p</strong>是一个假设($p$是负的<strong>eigenvalue</strong>出现的几率，而<strong>负的eigenvalue</strong>代表<strong>loss</strong>有一条路往下降)。其实这个假设也是蛮合理的，想想看，刚开始的时候，在<strong>loss</strong>还比较高的地方，应该会有很多条路让你往<strong>loss</strong>更低的地方走，当<strong>loss</strong>很大的时候是比较有可能出现<strong>negative eigenvalue</strong>，后面训练到后来，<strong>loss</strong>很低时候，出现<strong>negative eigenvalue</strong>的几率$p$就变得很小了。因此上图的图中<strong>loss</strong>随着大小的不同，走到<strong>critical point</strong>，对应的<strong>eigenvalue</strong>的<strong>分布</strong>的变化。<br>从上图还能看出：在<strong>loss</strong>比较大的时候，<strong>eigenvalue</strong>为正的概率和为负的概率基本一半；<strong>loss</strong>比较小的时候，<strong>eigenvalue</strong>基本为正；因此<strong>saddle point</strong>比较容易出现在<strong>loss</strong>比较大的时候，而<strong>local minimum</strong>比较容易出现在<strong>loss</strong>比较低的地方(因为此时<strong>eigenvalue</strong>基本都是为正的)。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232413671.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232523313.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232630425.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232645925.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232744765.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232952444.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558232970689.png" alt="Alt text"></p>
</blockquote>
<h2 id="2-5-Geometry-of-Loss-Surface-Empirical"><a href="#2-5-Geometry-of-Loss-Surface-Empirical" class="headerlink" title="2.5 Geometry of Loss Surface(Empirical)"></a>2.5 Geometry of Loss Surface(Empirical)</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233086374.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233094775.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233469124.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233650824.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558233775339.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234122697.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234269079.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234347827.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234639087.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234762963.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234803475.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558234907367.png" alt="Alt text"><br><strong>不同初始化，造就的模型不同。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235070770.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235215231.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235350683.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235486657.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235587901.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235644335.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235807379.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558235877541.png" alt="Alt text"></p>
<h2 id="3-1-Generalization-Capability-of-Deep-Learning"><a href="#3-1-Generalization-Capability-of-Deep-Learning" class="headerlink" title="3.1 Generalization Capability of Deep Learning"></a>3.1 Generalization Capability of Deep Learning</h2><blockquote>
<p>需要注意的是，不管你的数据的分布是什么样子的，都会有$1-\delta$概率($\delta$是你自己定的一个值)出现下面的式子：</p>
<script type="math/tex; mode=display">E_{test} \leq E_{train} + \Omega(R, M, \delta)</script><p>因此有：</p>
<script type="math/tex; mode=display">E_{train} \leq E_{test} \leq E_{train} + \Omega(R, M, \delta)</script><p>其中$\Omega(R, M, \delta)$与$\delta$有关，至于这一项是什么后面再说。<br>因此$\delta$越小，上面式子发生的概率越大，则中间的$E_{test}$所在的区间就得越大(这样才有很大概率出现在这个区间之内)，因此相应的$\Omega(R, M, \delta)$也就越大。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558270653247.png" alt="Alt text"></p>
<hr>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558270867907.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558270971276.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558271086884.png" alt="Alt text"><br><strong>capacity of model</strong>的含义是对于错误标注的数据进行拟合，达到百分百正确时使用的数据的量就是模型的$VC dimension$。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558271290720.png" alt="Alt text"></p>
<hr>
<p><strong>Overparameterized: </strong>过参数化，参数过多；<br>一个比较反直觉的结论：在深度学习中，如果训练集的错误率很小了，而测试集的错误率比较高的情况下，如果这个时候再增加模型中的参数量，测试集的结果反而会变好。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558271774929.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558272967476.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273007272.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273269484.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273613610.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273738169.png" alt="Alt text"><br>上面的都是在验证模型大的情况下，泛化反而会相比较之前的较差结果要好一些。至于原因暂时还没有定论结果。</p>
<hr>
<p>下面这个实验是说：神经网络是自带正则化的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558273984950.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274308341.png" alt="Alt text"></p>
</blockquote>
<h2 id="3-2-Indicator-of-Generalization"><a href="#3-2-Indicator-of-Generalization" class="headerlink" title="3.2 Indicator of Generalization"></a>3.2 Indicator of Generalization</h2><p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274443913.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274626635.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558274741946.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275064641.png" alt="Alt text"></p>
<blockquote>
<p>上图中左图最下面的虚线正确率先增加后减少，说明模型在初期是先从$20 %$的真实标注数据中学习到了有用的东西的，而后遇到<strong>80 %</strong>错误标注的时候的正确率开始下降。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275402082.png" alt="Alt text"></p>
<hr>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275431578.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275669761.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558275998776.png" alt="Alt text"><br>其实$x$的<strong>sensitivity</strong>还是很直觉的，也就是说现在输入的数据如果有一个小量的变化的话，那对我的输出到底是有多大的变化。<strong>sensitivity</strong>和泛化之间的关系就是如果某个网络对某一份数据非常的<strong>sensitive</strong>，那么意味着这个<strong>Network</strong>不够<strong>roboost</strong>。<br>而<strong>Regularization</strong>也可以看做是<strong>minimize sensitivity</strong>，因此正则化的时候<strong>weight</strong>越接近0越好，其实就是让<strong>output</strong>越平滑，<strong>output</strong>越平滑其实就是<strong>sensitivity</strong>越小，因为<strong>input</strong>有变化的时候，<strong>output</strong>变化是比较小的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558276492716.png" alt="Alt text"><br>那么对于一个没有标签的测试集，就可以通过计算<strong>sensitivity</strong>的值去判断<strong>test</strong>的表现结果如何。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558276779000.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558276927319.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277096050.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277114755.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277165544.png" alt="Alt text"></p>
</blockquote>
<hr>
<p>下面是说不同锋利程度的<strong>local minimum</strong>与泛化程度的关系(的一种可能猜想)：<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277403045.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277607275.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277786862.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558277908659.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558278039524.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558278075345.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558278095267.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558356388010.png" alt="Alt text"></p>
<h2 id="4-1-1-Batch-Normalization"><a href="#4-1-1-Batch-Normalization" class="headerlink" title="4.1.1  Batch_Normalization"></a>4.1.1  Batch_Normalization</h2><p><strong>Batch_Norm:</strong></p>
<script type="math/tex; mode=display">\mu = \frac{1}{m} \sum_{i} z^{(i)}</script><script type="math/tex; mode=display">{\sigma}^2 = \frac{1}{m} (z^{(i)} - \mu)^2</script><script type="math/tex; mode=display">z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><script type="math/tex; mode=display">\tilde{z}^{(i)} = \gamma·z_{norm}^{(i)} + \beta</script><p>其中 $\epsilon$ 是为了保证数值稳定. $\gamma$ 和 $\beta$ 是可以跟随<strong>backpropagation</strong>进行学习更新的参数，类似神经网络的中的权重$\omega$, 更新公式如下：</p>
<script type="math/tex; mode=display">\omega^{[l]} := {\omega}^{[l]} - \alpha·d{\omega}^{[l]}</script><script type="math/tex; mode=display">\gamma^{[l]} := {\gamma}^{[l]} - \alpha·d{\gamma}^{[l]}</script><script type="math/tex; mode=display">\beta^{[l]} := {\beta}^{[l]} - \alpha·d{\beta}^{[l]}</script><p>其中$\gamma$以及$\beta$也是可以再反向传播的时候进行<strong>training</strong>的参数，需要注意的是：当 $\gamma = \sqrt{\sigma^2 + \epsilon}$ 以及 $\beta = \mu$ 的时候，相当于是恢复到了<strong>batch_norm</strong>之前的状态.</p>
<blockquote>
<hr>
<p><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558356602000.png" alt="Alt text"><br><strong>Feature Scaling:</strong><br>下面这个图中，假设$x_1$是$1,2,…$，$x_2$是$100,200,….$，假设$x_1,x_2$是一样重要的<strong>feature</strong>，对结果的影响力是一样的，这意味着$w_1$的<strong>scale</strong>会比较大，然后把$w_1,w_2$与$loss$的关系拿出来作图，就会发现在$w_1$这个方向的变化的斜率是比较小的，相应的<strong>gradient</strong>是比较小的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558357052634.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558358133270.png" alt="Alt text"><br>对于深度学习，在<strong>training</strong>的过程中每一层的参数都是不断的变化，则每一层的<strong>output</strong>都会在变化，相应的每一层的<strong>mean</strong>和<strong>variance</strong>也就在不断的变化。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558358847474.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558358946692.png" alt="Alt text"><br><strong>Batch_Norm中的Batch不能太小了，不然算均值和方差会不准。</strong><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558359272868.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558359358028.png" alt="Alt text"><br>需要注意的是<strong>Batch_Norm</strong>在做<strong>backpropogation</strong>的时候是需要通过$\mu, \sigma$的，不能把它们两个当成是常量给排除掉，因为往上返回的时候$\mu, \sigma$是会改动的，所以不能当成常量。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558359744159.png" alt="Alt text"><br>$\gamma$和$\beta$不同于$\mu$和$\sigma$的，他俩是独立于数据的，他俩是<strong>network</strong>自己的学习得到的。<br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558360603736.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558360856175.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/06/10/DeepLearningTheory/1558360899077.png" alt="Alt text"></p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Stoner</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/06/10/DeepLearningTheory/">http://yoursite.com/2019/06/10/DeepLearningTheory/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">Stoner的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/">理论推导</a></div><div class="post_share"><div class="social-share" data-image="http://qe0z9wdl5.bkt.clouddn.com/20200725214526.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/06/10/Import-CNN-Framework/"><img class="prev-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725220150.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">重要卷积神经网络结构分析</div></div></a></div><div class="next-post pull-right"><a href="/2019/06/06/model-pd/"><img class="next-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725220814.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">迁移学习中预训练模型的保存文件</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Stoner</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/search/local-search.js"></script><script>if (document.getElementsByClassName('mermaid').length) {
  loadScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js',function () {
    mermaid.initialize({
      theme: 'default',
  })
})
}</script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>