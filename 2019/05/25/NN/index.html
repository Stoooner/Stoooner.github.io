<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>神经网络实战之前的注意事项 | Stoner的博客</title><meta name="description" content="创建神经网络需要注意的各个要点. 1. 变量初始化1.1 零初始化 W &#x3D; np.zero((n_in,n_out))  结果：零初始化会导致训练过程无效，所有的神经元节点做的都是同样的计算,相当于重复劳动了。通常来说，零初始化都会导致神经网络无法打破对称性，最终导致的结构就是无论网络有多少层，最终只能得到和Logistic函数相同的效果。  1.2 标准高斯分布初始化W &#x3D; np.random"><meta name="keywords" content="神经网络"><meta name="author" content="Stoner"><meta name="copyright" content="Stoner"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2019/05/25/NN/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="神经网络实战之前的注意事项"><meta property="og:url" content="http://yoursite.com/2019/05/25/NN/"><meta property="og:site_name" content="Stoner的博客"><meta property="og:description" content="创建神经网络需要注意的各个要点. 1. 变量初始化1.1 零初始化 W &#x3D; np.zero((n_in,n_out))  结果：零初始化会导致训练过程无效，所有的神经元节点做的都是同样的计算,相当于重复劳动了。通常来说，零初始化都会导致神经网络无法打破对称性，最终导致的结构就是无论网络有多少层，最终只能得到和Logistic函数相同的效果。  1.2 标准高斯分布初始化W &#x3D; np.random"><meta property="og:image" content="http://qe0z9wdl5.bkt.clouddn.com/20200725230120.png"><meta property="article:published_time" content="2019-05-25T08:33:57.000Z"><meta property="article:modified_time" content="2020-07-25T15:01:50.741Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="原始数据划分以及TFrecords实战" href="http://yoursite.com/2019/05/25/tfrecords-1/"><link rel="next" title="PIL缩放切图,抗锯齿" href="http://yoursite.com/2019/05/25/PIL/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Stoner的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">48</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">62</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content toc-div-class" style="display:none"></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://qe0z9wdl5.bkt.clouddn.com/20200725230120.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Stoner的博客</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">神经网络实战之前的注意事项</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-05-25 16:33:57"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2019-05-25</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 23:01:50"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><blockquote>
<p><strong>创建神经网络需要注意的各个要点.</strong><br><a id="more"></a></p>
<h2 id="1-变量初始化"><a href="#1-变量初始化" class="headerlink" title="1. 变量初始化"></a>1. 变量初始化</h2><h3 id="1-1-零初始化"><a href="#1-1-零初始化" class="headerlink" title="1.1 零初始化"></a>1.1 零初始化</h3></blockquote>
<pre><code>W = np.zero((n_in,n_out))
</code></pre><blockquote>
<p>结果：<br>零初始化会导致训练过程无效，所有的神经元节点做的都是同样的计算,相当于重复劳动了。通常来说，零初始化都会导致神经网络无法打破对称性，最终导致的结构就是无论网络有多少层，最终只能得到和Logistic函数相同的效果。</p>
</blockquote>
<h3 id="1-2-标准高斯分布初始化"><a href="#1-2-标准高斯分布初始化" class="headerlink" title="1.2 标准高斯分布初始化"></a>1.2 标准高斯分布初始化</h3><pre><code>W = np.random.randn((n_in,n_out))
</code></pre><blockquote>
<p>结果：<br>高斯分布：$ f(x)=\frac{1}{\sqrt{2\pi }\sigma }exp(-\frac{(x-\mu )^{2}}{2\sigma ^{2}})$<br>根据独立标准高斯分布变量来选择权重和偏置， $μ=0$ ， $σ=1$<br>由于隐藏神经元的输出 $σ(z)$($sigmoid$函数)会接近0或1，使隐藏层神经元达到饱和．此时对神经元权重进行微小调整只能给神经元的激活值带来非常微弱的变化．结果，在进行梯度下降算法时会导致神经网络学习缓慢。</p>
</blockquote>
<h3 id="1-3-改进高斯分布初始化"><a href="#1-3-改进高斯分布初始化" class="headerlink" title="1.3 改进高斯分布初始化"></a>1.3 改进高斯分布初始化</h3><pre><code>W = np.random.randn((n_in,n_out))/np.sqrt(n_in)
</code></pre><p>使用 $μ=0$,$\sigma=\frac{1}{ \sqrt{n_{in} }}$ 高斯分布的随机值,对权重和偏向进行初始化</p>
<blockquote>
<p>结果：<br>在-1到1区间上 σ(z)的斜率比较大，对神经元权重进行微小调整能给神经元的激活值带来非常明显的变化，解决了神经网络学习缓慢的问题．</p>
</blockquote>
<h3 id="1-4-Xavier初始化"><a href="#1-4-Xavier初始化" class="headerlink" title="1.4 Xavier初始化"></a>1.4 Xavier初始化</h3><p>$W\sim U\left[ -\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}},\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}} \right]$</p>
<p>其中$U[−a,a]$是区间$(−a,a)$上的均匀分布</p>
<pre><code>def xavier_init(fan_in, fan_out, constant = 1):
    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))
    high = constant * np.sqrt(6.0 / (fan_in + fan_out))
    return tf.random_uniform((fan_in, fan_out),minval=low, maxval=high, dtype=tf.float32)
</code></pre><blockquote>
<p>论文来源：<br>$Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In International Conference on Artificial Intelligence and Statistics, pages 315–323, 2011$</p>
</blockquote>
<h3 id="1-5-He初始化"><a href="#1-5-He初始化" class="headerlink" title="1.5 He初始化"></a>1.5 He初始化</h3><p>He初始化中，系数为<strong>sqrt(2./layers_dims[l-1])</strong></p>
<pre><code>W = np.random.randn(layers-dims[l], layers-dims[l-1]) * np.sqrt(2./layers-dims[l-1])
</code></pre><blockquote>
<p>He初始化搭配ReLU激活函数常常可以得到不错的效果。</p>
</blockquote>
<h2 id="2-正则化"><a href="#2-正则化" class="headerlink" title="2. 正则化"></a>2. 正则化</h2><blockquote>
<p>在深度学习中，如果数据集没有足够大的话，可能会导致一些过拟合的问题。过拟合导致的结果就是在训练集上有着很高的精确度，但是在遇到新的样本时，精确度下降严重。为了避免过拟合的问题，需要使用正则化。</p>
</blockquote>
<h3 id="2-1-L2正则化"><a href="#2-1-L2正则化" class="headerlink" title="2.1 L2正则化"></a>2.1 L2正则化</h3><p>$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)}) + \frac{\lambda}{m}||w||_2^2$</p>
<p>其中，$||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw$</p>
<h3 id="2-2-L1正则化"><a href="#2-2-L1正则化" class="headerlink" title="2.2 L1正则化"></a>2.2 L1正则化</h3><p>$J(w; X, y) = L_{emp}(w; X, y) + \alpha|w|_1$</p>
<blockquote>
<p>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为$||w||_1$<br>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。<br>稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。<br><strong>注意：</strong><br>the L1 regularizer is punished uniformly for low and high values, and<br>has an incentive to decrease all the non-zero parameter values toward zero. It thus encourages a sparse solutions—models with many parameters with a zero value. The L1 regularizer is also called a sparse prior or lasso </p>
</blockquote>
<h3 id="2-3-Elastic-Net"><a href="#2-3-Elastic-Net" class="headerlink" title="2.3     Elastic-Net"></a>2.3     Elastic-Net</h3><blockquote>
<p>The elastic-net regularization [Zou and Hastie, 2005] combines both L1 and L2regularization:</p>
</blockquote>
<p>$R_{elastic-net} = λ_1R_{L1} (W)+  λ_2R_{L2}(W)$</p>
<h3 id="2-4-Dropout"><a href="#2-4-Dropout" class="headerlink" title="2.4 Dropout"></a>2.4 Dropout</h3><p>除了以上正则化方法之外，Dropout方法也是一个十分常用的正则化手段<br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558794215795.png" alt="Alt text"></p>
<p>假设我们在上图的神经网络结构中进行训练，验证时发现了较严重的过拟合现象。 那么Dropout的基本原理如下： </p>
<ul>
<li>遍历每层的神经元节点，并设置每层节点随机消失的概率。 </li>
<li>例如，我们设置所有节点都是有0.5的概率会消失。 </li>
<li>那么，在完成这个过程后，我们会发现有一些节点现在已经被失效：<br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793465755.png" alt="Alt text"></li>
</ul>
<ul>
<li>然后，我们删除掉与这些节点关联的连线：<br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793471128.png" alt="Alt text"></li>
</ul>
<ul>
<li>此时，我们将会得到一个节点更少，网络更加简单的模型结构。<br>对于该样本，以同样的结构进行前向传播和反向传播。<br>而当下一样本输入时，我们需要重新随机选择节点置为失效并进行前向传播和反向传播。 <blockquote>
<p><strong>注意：</strong><br>需要注意的是，在使用Dropout进行训练得到的模型，在进行验证，测试或应用时，应该不再适用Dropout函数进行随机失效处理。 主要原因是因为在测试或验证阶段，我们不希望输出的结果是随机的。<br>添加了Dropout函数后，我们无法对某些神经元或特征进行强依赖，而是更多地会依赖于整个的权重分配。</p>
</blockquote>
</li>
<li>在使用Dropout函数时，不同层的keep_prob可以变化。例如，对于某些神经元较多的层，我们可以设置更低的keep_prob来避免过拟合，因此理论上，在参数越多的层，造成过拟合的可能性通常更大。</li>
<li>Dropout函数在计算机视觉领域应用相对较多。</li>
<li>只有在模型过拟合的时候才考虑使用Dropout函数。</li>
<li>缺点是在使用了Dropout函数后，代价函数J不再能够被明确的定义。每次迭代中会随机删除一些节点。此时，我们很难进行调试验证。因此，我们通常建议先将keep_prob设置为1，进行调试，保证代价函数单调递减时，再开启Dropout层进行训练。</li>
</ul>
<h2 id="3-优化算法"><a href="#3-优化算法" class="headerlink" title="3. 优化算法"></a>3. 优化算法</h2><h3 id="3-1-Mini-batch-梯度下降法"><a href="#3-1-Mini-batch-梯度下降法" class="headerlink" title="3.1 Mini-batch 梯度下降法"></a>3.1 Mini-batch 梯度下降法</h3><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。</p>
<p>但是如果每次处理训练数据的一部分，即用其子集进行梯度下降，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。</p>
<p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。</p>
<p><strong>不同size大小的比较</strong><br>普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示：</p>
<p><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793481124.png" alt="Alt text"></p>
<ul>
<li>batch梯度下降：</li>
</ul>
<ol>
<li>对所有m个训练样本执行一次梯度下降，每一次迭代时间较长；</li>
<li>Cost function 总是向减小的方向下降。</li>
</ol>
<ul>
<li>随机梯度下降：</li>
</ul>
<ol>
<li>对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；</li>
<li>Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。</li>
</ol>
<ul>
<li>Mini-batch梯度下降：</li>
</ul>
<ol>
<li>选择一个 1&lt;size&lt;m 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处；</li>
<li>Cost function的下降处于前两者之间。</li>
</ol>
<ul>
<li>Mini-batch 大小的选择：</li>
</ul>
<ol>
<li>如果训练样本的大小比较小时，如 m\leqslant 2000 时 ——— 选择batch梯度下降法；</li>
<li>如果训练样本的大小比较大时，典型的大小为： $2^{6}、2^{7}、\cdots、2^{10} $；</li>
<li>Mini-batch的大小要符合CPU/GPU内存。</li>
</ol>
<h3 id="3-2-动量（Momentum）梯度下降法"><a href="#3-2-动量（Momentum）梯度下降法" class="headerlink" title="3.2 动量（Momentum）梯度下降法"></a>3.2 动量（Momentum）梯度下降法</h3><p>动量梯度下降的基本思想就是计算梯度的指数加权平均数，并利用该梯度来更新权重。</p>
<p>在我们优化 Cost function 的时候，以下图所示的函数图为例：</p>
<p><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793488590.png" alt="Alt text"></p>
<p>在利用梯度下降法来最小化该函数的时候，每一次迭代所更新的代价函数值如图中蓝色线所示在上下波动，而这种幅度比较大波动，减缓了梯度下降的速度，而且我们只能使用一个较小的学习率来进行迭代。</p>
<p>如果用较大的学习率，结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率。</p>
<p>但是我们又希望在如图的纵轴方向梯度下降的缓慢一些，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点，而这里用动量梯度下降法既可以实现，如红色线所示。</p>
<p><strong>算法实现:</strong><br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793493870.png" alt="Alt text"></p>
<p>$\beta$ 常用的值是0.9。</p>
<p>在我们进行动量梯度下降算法的时候，由于使用了指数加权平均的方法。原来在纵轴方向上的上下波动，经过平均以后，接近于0，纵轴上的波动变得非常的小；但在横轴方向上，所有的微分都指向横轴方向，因此其平均值仍然很大。最终实现红色线所示的梯度下降曲线。</p>
<h3 id="3-3-RMSprop"><a href="#3-3-RMSprop" class="headerlink" title="3.3 RMSprop"></a>3.3 RMSprop</h3><p>除了上面所说的Momentum梯度下降法，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法。<br>$S_{dw}=βS_{dw}+(1-β)dw^2$<br>$S_{db}=βS_{db}+(1-β)db^2$<br>$w:=w-α\frac{dw}{\sqrt{S_{dw}}}$<br>$b:=b-α\frac{db}{\sqrt{S_{db}}}$<br>这里假设参数b的梯度处于纵轴方向，参数w的梯度处于横轴方向（当然实际中是处于高维度的情况），利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，如图中蓝色线所示，使其梯度下降的速度变得更快，如图绿色线所示。</p>
<p>在如图所示的实现中，RMSprop将微分项进行平方，然后使用平方根进行梯度更新，同时为了确保算法不会除以0，平方根分母中在实际使用会加入一个很小的值如$ \varepsilon=10^{-8}$ 。</p>
<h3 id="3-4-Adam-优化算法"><a href="#3-4-Adam-优化算法" class="headerlink" title="3.4  Adam 优化算法"></a>3.4  Adam 优化算法</h3><p>Adam （Adaptive Moment Estimation）优化算法的基本思想就是将 Momentum 和  RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法。<br><strong>算法实现：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793508697.png" alt="Alt text"></p>
<p><strong>超参数的选择：</strong></p>
<ul>
<li>$\alpha$ ：需要进行调试；</li>
<li>$\beta_{1}$ ：常用缺省值为0.9，$ dw $的加权平均；</li>
<li>$\beta_{2}$ ：推荐使用0.999， $dw^{2} $的加权平均值；</li>
<li>$\varepsilon$ ：推荐使用 $10^{-8} $。</li>
</ul>
<h3 id="3-5-学习率衰减"><a href="#3-5-学习率衰减" class="headerlink" title="3.5 学习率衰减"></a>3.5 学习率衰减</h3><p>在我们利用 mini-batch 梯度下降法来寻找Cost  function的最小值的时候，如果我们设置一个固定的学习速率$ \alpha$ ，则算法在到达最小值点附近后，由于不同batch中存在一定的噪声，使得不会精确收敛，而一直会在一个最小值点较大的范围内波动，如下图中蓝色线所示。</p>
<p>但是如果我们使用学习率衰减，逐渐减小学习速率$ \alpha$ ，在算法开始的时候，学习速率还是相对较快，能够相对快速的向最小值点的方向下降。但随着 $\alpha $的减小，下降的步伐也会逐渐变小，最终会在最小值附近的一块更小的区域里波动，如图中绿色线所示。<br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793517430.png" alt="Alt text"></p>
<p><strong>学习率衰减的实现</strong></p>
<ul>
<li>常用：$ \alpha = \dfrac{1}{1+decay_rate*epoch_num}\alpha_{0} $</li>
<li>指数衰减： $decayed$_ $learning$_ $rate$ = $learning$_ $rate$ $*$ $decay$_ $rate^{(global-step/decay-step)}$</li>
<li>其他： $\alpha = \dfrac{k}{epoch_num}\cdot\alpha_{0} $</li>
<li>离散下降（不同阶段使用不同的学习速率）</li>
</ul>
<h2 id="4-VANISHING-AND-EXPLODING-GRADIENTS"><a href="#4-VANISHING-AND-EXPLODING-GRADIENTS" class="headerlink" title="4. VANISHING AND EXPLODING GRADIENTS"></a>4. VANISHING AND EXPLODING GRADIENTS</h2><p>Dealing with the vanishing gradients problem is still an open research question<br><strong>Solutions include:</strong> </p>
<ul>
<li>making the networks shallower,</li>
<li>step-wise training(first train the first layers based on some auxiliary output signal, then fix them and train the upper layers of the complete network based on the real task signal)</li>
<li>performing batch-normalization<br>(for every minibatch, normalizing the inputs to each of the network<br>layers to have zero mean and unit variance) </li>
<li>using specialized architectures that are designed to<br>assist in gradient flow (e.g., the LSTM and GRU architectures for recurrent networks. </li>
<li>Dealing with the exploding gradients has a simple but very effective solution: clipping the gradients if their norm exceeds a given threshold.<br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793524763.png" alt="Alt text"></li>
</ul>
<h2 id="5-各类激活函数"><a href="#5-各类激活函数" class="headerlink" title="5. 各类激活函数"></a>5. 各类激活函数</h2><blockquote>
<p>Layers with tanh and sigmoid activations can become saturated—resulting in output values for that layer that are all close to one, the upper-limit of the activation function. Saturated neurons have very small gradients, and should be avoided. Layers with the ReLU activation cannot be saturated, but can “die”—most or all values are negative and thus clipped at zero for all inputs, resulting in a gradient of zero for that layer. If your network does not train well, it is advisable to monitor the network for layers with many saturated or dead neurons. Saturated neurons are caused by too large values entering the layer. This may be controlled for by changing the initialization, scaling the range of the input values, or changing the learning rate. Dead neurons are caused by all signals entering the layer being negative (for example this can happen after a large gradient update)</p>
</blockquote>
<h2 id="6-Normalizing-activations-in-a-network"><a href="#6-Normalizing-activations-in-a-network" class="headerlink" title="6. Normalizing activations in a network"></a>6. Normalizing activations in a network</h2><h3 id="6-1-原理"><a href="#6-1-原理" class="headerlink" title="6.1 原理"></a>6.1 原理</h3><p>Sergey Ioffe和Christian Szegedy两位学者提出了Batch Normalization方法。Batch Normalization不仅可以让调试超参数更加简单，而且可以让神经网络模型更加“健壮”。也就是说较好模型可接受的超参数范围更大一些，包容性更强，使得更容易去训练一个深度神经网络。</p>
<p>训练神经网络时，标准化输入可以提高训练的速度。方法是对训练数据集进行归一化的操作，即将原始数据减去其均值μ后，再除以其方差σ2。但是标准化输入只是对输入进行了处理，那么对于神经网络，又该如何对各隐藏层的输入进行标准化处理呢？</p>
<p>其实在神经网络中，第l层隐藏层的输入就是第l−1层隐藏层的输出A[l−1]。对A[l−1]进行标准化处理，从原理上来说可以提高W[l]和b[l]的训练速度和准确度。这种对各隐藏层的标准化处理就是Batch Normalization。值得注意的是，实际应用中，一般是对Z[l−1]进行标准化处理而不是A[l−1]，其实差别不是很大。</p>
<p>Batch Normalization对第l层隐藏层的输入$Z^{[l−1]}$做如下标准化处理，忽略上标[l−1]:</p>
<p>$\mu=\frac1m\sum_iz^{(i)}$<br>$\sigma^2=\frac1m\sum_i(z_i-\mu)^2$<br>$z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}$</p>
<p>其中，m是单个mini-batch包含样本个数，$ε$是为了防止分母为零，可取值$10^{−8}$。这样，使得该隐藏层的所有输入$z^{(i)}$均值为0，方差为1。</p>
<p>但是，大部分情况下并不希望所有的$z^{(i)}$均值都为0，方差都为1，也不太合理。通常需要对$z^{(i)}$进行进一步处理:<br>$\tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$</p>
<p>上式中，$γ$和$β$是learnable parameters，类似于W和b一样，可以通过梯度下降等算法求得。这里，γ和β的作用是让 $\tilde z^{(i)}$ 的均值和方差为任意值，只需调整其值就可以了。例如，令：<br>$\gamma=\sqrt{\sigma^2+\varepsilon},  \beta=u$<br>则$\tilde z^{(i)}=z^{(i)}$, 即identity function。可见，设置γ和β为不同的值，可以得到任意的均值和方差。</p>
<p>这样，通过Batch Normalization，对隐藏层的各个$z^{<a href="i">l</a>}$进行标准化处理，得到$\tilde z^{<a href="i">l</a>}$，替代$z^{<a href="i">l</a>}$。</p>
<h3 id="6-2-Fitting-Batch-Norm-into-a-neural-network"><a href="#6-2-Fitting-Batch-Norm-into-a-neural-network" class="headerlink" title="6.2 Fitting Batch Norm into a neural network"></a>6.2 Fitting Batch Norm into a neural network</h3><p>我们已经知道了如何对某单一隐藏层的所有神经元进行Batch Norm，接下来将研究如何把Bath Norm应用到整个神经网络中。</p>
<p>对于L层神经网络，经过Batch Norm的作用，整体流程如下：<br><img src= "/img/loading.gif" data-src="/2019/05/25/NN/1558793537077.png" alt="Alt text"></p>
<p>实际上，Batch Norm经常使用在mini-batch上，这也是其名称的由来。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Stoner</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/05/25/NN/">http://yoursite.com/2019/05/25/NN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">Stoner的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="http://qe0z9wdl5.bkt.clouddn.com/20200725214526.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/05/25/tfrecords-1/"><img class="prev-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725230026.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">原始数据划分以及TFrecords实战</div></div></a></div><div class="next-post pull-right"><a href="/2019/05/25/PIL/"><img class="next-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725230241.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PIL缩放切图,抗锯齿</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Stoner</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/search/local-search.js"></script><script>if (document.getElementsByClassName('mermaid').length) {
  loadScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js',function () {
    mermaid.initialize({
      theme: 'default',
  })
})
}</script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>