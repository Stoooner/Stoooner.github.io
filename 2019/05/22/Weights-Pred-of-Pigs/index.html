<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Weights of Predictions of Pigs | Stoner的博客</title><meta name="description" content="使用深度学习的方法对死猪体重进行预测.  1. 项目情况说明当前需求场景发生在一个特定的保险产品上：育肥猪保险  生猪养殖户会对育肥猪进行投保，如果发生理赔，保险公司会按照猪的体重进行赔付，所以保险公司需要对育肥猪的体重进行勘定。 如果猪可以上秤，那么保险公司就可以获得准确的体重。但在实务操作中，有很多无法上秤的场景，比如疫病猪需要快速无害化处理，没有办法等到保险公司来查勘定损，或者因为一些原因"><meta name="keywords" content="TensorFlow,卷积神经网络,全连接神经网络,回归,标签归一化"><meta name="author" content="Stoner"><meta name="copyright" content="Stoner"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2019/05/22/Weights-Pred-of-Pigs/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="Weights of Predictions of Pigs"><meta property="og:url" content="http://yoursite.com/2019/05/22/Weights-Pred-of-Pigs/"><meta property="og:site_name" content="Stoner的博客"><meta property="og:description" content="使用深度学习的方法对死猪体重进行预测.  1. 项目情况说明当前需求场景发生在一个特定的保险产品上：育肥猪保险  生猪养殖户会对育肥猪进行投保，如果发生理赔，保险公司会按照猪的体重进行赔付，所以保险公司需要对育肥猪的体重进行勘定。 如果猪可以上秤，那么保险公司就可以获得准确的体重。但在实务操作中，有很多无法上秤的场景，比如疫病猪需要快速无害化处理，没有办法等到保险公司来查勘定损，或者因为一些原因"><meta property="og:image" content="http://qe0z9wdl5.bkt.clouddn.com/20200725232147.png"><meta property="article:published_time" content="2019-05-22T05:12:11.000Z"><meta property="article:modified_time" content="2020-07-25T15:22:03.592Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="Tensorflow-GPU with Win10 Installing" href="http://yoursite.com/2019/05/22/tensorflow-gpu-win10-installing/"><link rel="next" title="Application of Batch Normalization in Tensorflow" href="http://yoursite.com/2019/05/22/Batch-Norm/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Stoner的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">48</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">62</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content toc-div-class" style="display:none"></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://qe0z9wdl5.bkt.clouddn.com/20200725232147.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Stoner的博客</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Weights of Predictions of Pigs</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-05-22 13:12:11"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2019-05-22</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 23:22:03"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><blockquote>
<p><strong>使用深度学习的方法对死猪体重进行预测.</strong><br><a id="more"></a></p>
</blockquote>
<h2 id="1-项目情况说明"><a href="#1-项目情况说明" class="headerlink" title="1. 项目情况说明"></a>1. 项目情况说明</h2><p>当前需求场景发生在一个特定的保险产品上：<strong>育肥猪保险</strong></p>
<blockquote>
<p>生猪养殖户会对育肥猪进行投保，如果发生理赔，保险公司会按照猪的体重进行赔付，所以保险公司需要对育肥猪的体重进行勘定。</p>
<p>如果猪可以上秤，那么保险公司就可以获得准确的体重。但在实务操作中，有很多无法上秤的场景，比如疫病猪需要快速无害化处理，没有办法等到保险公司来查勘定损，或者因为一些原因无法人为移动上秤，就只能对猪的体重进行估算。</p>
<p>这个时候，一些保险公司会通过查勘员估算、或无害化处理站工作人员估算等方式，也能得到粗略的结果。</p>
<p>猪重识别项目的价值在于：</p>
<ul>
<li>对大量的历史样本进行学习，获得猪体重的更优估计;</li>
<li>自动计算，减少估算过程中的人为因素;</li>
<li>节约查勘成本;</li>
<li>自动定损，更顺畅便捷的理赔过程;</li>
</ul>
</blockquote>
<hr>
<h2 id="2-项目分析"><a href="#2-项目分析" class="headerlink" title="2. 项目分析"></a>2. 项目分析</h2><h3 id="2-1-开发环境"><a href="#2-1-开发环境" class="headerlink" title="2.1 开发环境"></a>2.1 开发环境</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">环境</th>
<th style="text-align:right">对应版本</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">operating system</td>
<td style="text-align:right">win10</td>
</tr>
<tr>
<td style="text-align:left">GPU</td>
<td style="text-align:right">NVIDIA GeForce RTX 2080 Ti</td>
</tr>
<tr>
<td style="text-align:left">CPU</td>
<td style="text-align:right">Inter(R) i7-9700K</td>
</tr>
<tr>
<td style="text-align:left">Memory</td>
<td style="text-align:right">16G</td>
</tr>
<tr>
<td style="text-align:left">python</td>
<td style="text-align:right">3.6.8</td>
</tr>
<tr>
<td style="text-align:left">tensorflow-gpu</td>
<td style="text-align:right">1.12.0</td>
</tr>
<tr>
<td style="text-align:left">IDE</td>
<td style="text-align:right">pycharm-community-2019.1.1</td>
</tr>
<tr>
<td style="text-align:left">Anaconda</td>
<td style="text-align:right">Anaconda3-5.3.0-Windows-x86_64.exe</td>
</tr>
<tr>
<td style="text-align:left">CUDA® Toolkit</td>
<td style="text-align:right">cuda-9.0.176-win10.exe</td>
</tr>
<tr>
<td style="text-align:left">cuDNN SDK</td>
<td style="text-align:right">cudnn-9.0-windows10-x64-v7.4.2.24.zip</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-2-原始数据"><a href="#2-2-原始数据" class="headerlink" title="2.2 原始数据"></a>2.2 原始数据</h3><p>整个项目的图片总数为192张，刨除一些存在标签标准错误以及同一图片存在多头死猪的图片之后，整个数据集还剩189张图片。整个图片集中所有图片的分辨率分为两种:    $640 \times 480$ 以及 $640 \times 853$，除此之外，每一张图片都包含有图片提供方关于图片的相应信息描述的水印，而图片中的死猪身上都有一张<strong>A4</strong>大小的纸张，纸张包含该死猪的一些相应信息，具体图片样式如下：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/19.jpg" alt="Alt text"></p>
<h3 id="2-3-设计思路1"><a href="#2-3-设计思路1" class="headerlink" title="2.3 设计思路1"></a>2.3 设计思路1</h3><p>首先，原始数据除了这189张图片之外，就只有一个<strong>Excel</strong>文档，记录了相应照片的对应体重，再无其他特征，因此首先想到的就是利用卷积神经网络(<strong>Convolutional Neural Network</strong>)对于原始图片进行特征提取，利用卷积的架构让模型自动帮我们提取有用的特征；然后再在卷积后面接入全连接神经网络(<strong>Fully Connected Neural Network</strong>)，通过修改神经网络最后一层的神经元的个数来达到预测的效果。由于全连接神经网络多用于分类，而此项目的预测结果为连续值，实则为回归问题，因此第一个设计思路就是按照分类的方式，将整个体重区间进行离散化以符合分类的要求，整个体重所处范围在$[20,100)$以及$[100+, \inf]$之间，离散化了之后的区间间隔如下：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/分类标签分段统计.png" alt="Alt text"><br>最后，为了避免数据较少但区间划分过细造成分类类别较多以及区间划分过粗造成分类不精确的情况出现，在此选择了间隔为<strong>4</strong>的情况，并按照<strong>卷积 + 全连接</strong>的方式搭建了第一个模型。</p>
<h4 id="2-3-1-数据预处理"><a href="#2-3-1-数据预处理" class="headerlink" title="2.3.1 数据预处理"></a>2.3.1 数据预处理</h4><p>由于原始数据存在水印的情况，在模型1初始的版本中考虑到水印可能会影响到分类结果，因此在初始版本中采取的思路是对所有的图片进行水印去除的预处理过程，得到的图片大体如下图：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1.jpg" alt="Alt text"><br>可以看到图像中的水印已经去除，但是相应的，连同白色的<strong>A4</strong>纸以及背景有白色的部分都被去除了，在后续的版本中发现这样的方式是不对的，因为<strong>A4</strong>大小其实是相同的，但是猪的大小则是不同的，因此<strong>A4</strong>纸相当于是一个体型大小的参照物，是一个很好的特征，这在后面<strong>CNN可视化</strong>中会有提及，在此不再详述。</p>
<h4 id="2-3-2-数据增广"><a href="#2-3-2-数据增广" class="headerlink" title="2.3.2 数据增广"></a>2.3.2 数据增广</h4><blockquote>
<p><strong>神经网络的好坏取决于输入的数据，通过增强数据集，可以防止神经网络学习到不相关的模式，根本上提升整体性能。(<a href="https://www.leiphone.com/news/201805/avOH5g1ZX3lAbmjp.html" target="_blank" rel="noopener">数据增强：数据有限时如何使用深度学习？</a>)</strong></p>
</blockquote>
<p>当我们训练一个深度学习模型时，我们的实际工作就是调参，以便将特定的输入（一幅图像）映射到输出(标签)。我们优化的目标是使模型的损失最小化， 以正确的方式调节优化参数即可实现这一目标。成功的神经网络拥有数以百万计的参数！自然，如果有大量参数，就需要提供的深度学习模型同比例的实例，以获得优秀的性能。相应的，需要的参数数量与需要执行的任务复杂性也成比例。但是在没有大量数据的情况之下，如何获取更多的数据呢?    其实，你并不需要添加大量的图像到你的数据集，为什么？ 因为，神经网络从一开始就不是智能的，例如，缺乏训练的神经网络会认为下面这3个网球是不同的、独立的图像：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557743761956.png" alt="完全一样的网球，神经网络的解释却不相同"><br>所以，为了获得更多数据，我们仅需要对已有的数据集做微小的调整。比如翻转、平移或旋转。神经网络会认为这些数据是不同的：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557743843604.png" alt="Alt text"><br>卷积神经网络有一个称作不变性的性质，即使卷积神经网络被放在不同方向上，它也能进行对象分类。更具体的说，卷积神经网络对<strong>平移、视角、尺寸或照度（或以上组合）</strong>保持不变性。(<a href="https://zhuanlan.zhihu.com/p/27642620" target="_blank" rel="noopener">YJango的卷积神经网络——介绍</a>)<br>这就是数据增强的本质前提。在现实世界中，我们可能会有一组在有限的条件下拍摄的图像 。但是，我们的目标应用可能是在多变的环境中，例如，不同的方向、位置、比例、亮度等。我们通过使用经综合修改过的数据来训练神经网络，以应对这些情形。</p>
<p>那么在深度学习的什么位置进行数据增广呢？一般有两种选择：</p>
<blockquote>
<ol>
<li>是预先对图片进行所有必要的变换，从根本上增加数据集的规模。</li>
<li>另外一个是小批量执行变换，仅仅在输入机器学习模型之前。</li>
</ol>
</blockquote>
<p>第一项被称为离线增强。这个方法常被用于相对较小的数据集。因为你最终会通过一个与执行的转换数量相等的因子来增加数据集的大小（例如，通过翻转所有图像，数据集数量会增加2倍）。</p>
<p>第二个选项称为在线增强，或称为动态增强。主要应用于规模较大的数据集，因为你无法负担数据量爆炸性增长。反而，你可以通过对即将输入模型的小批量数据的执行相应的变化。很多机器学习架构已经支持在线增强，并可以利用GPU进行加速。</p>
<p>在本项目中使用到的数据增强技术包括如下：</p>
<blockquote>
<ol>
<li><strong>随机裁切：</strong><code>tf.random_crop(value, size, seed=None, name=None)
参数：
value：向裁剪输入张量。
size：一维张量，大小等级为 value。
seed：Python 整数。用于创建一个随机的种子。查看 tf.set_random_seed 的行为。
name：此操作的名称（可选）</code><br><strong>在原有图像上随机的裁剪出一块，1.12版本可以使用tf.image.random_crop</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557750675402.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557750683978.png" alt="Alt text"></li>
<li><strong>随机左右翻转：</strong><code>tf.image.random_flip_left_right(image, seed=None)
参数：
image：形状为[height, width, channels]的三维张量。
seed：一个Python整数，用于创建一个随机种子</code><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557750872286.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557750884245.png" alt="Alt text"></li>
<li><strong>随机上下翻转：</strong><code>tf.image.random_flip_up_down(image, seed=None)
参数：
image：形状为[height, width, channels]的三维张量。
seed：一个Python整数，用于创建一个随机种子。</code><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557750891215.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557750894727.png" alt="Alt text"></li>
<li><strong>随机色调变换：(通过随机因子调整RGB图像的色调)</strong><code>tf.image.random_hue(image, max_delta, seed=None)
参数：
image：RGB图像，最后维度的大小必须为3。
max_delta：浮点型，随机增量的最大值。
seed：特定于操作的seed，它将与图层seed一起使用，以确定将在此操作中使用的实际seed</code><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751001109.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751010250.png" alt="Alt text"></li>
<li><strong>随机对比度变换：(函数是给图像乘一个常量(区间在[lower,upper]))</strong><code>tf.image.random_contrast(image, lower, upper, seed=None)
参数：
image：具有3个或更多维度的图像张量。
lower：浮点型，随机对比因子的下限。
upper：浮点型，随机对比因子的上限。
seed：一个Python整数，用于创建一个随机种子</code><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751145195.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751152414.png" alt="Alt text"></li>
<li><strong>随机亮度变换：(随机亮度变换(从-0.5到+0.5[max_delta=0.5代表最大的变换范围]，亮度变换就是给图像加上一个数或者减去一个数))</strong><code>tf.image.random_brightness(image, max_delta, seed=None)
参数：
image： 一个图像。
max_delta：float，必须是非负的。
seed：一个Python整数。用于创建一个随机种子</code><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751228048.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751235218.png" alt="Alt text"></li>
<li><strong>随机饱和度变换：(通过随机因子调整RGB图像的饱和度)</strong><code>tf.image.random_saturation(image, lower, upper, seed=None)
参数：
image：RGB图像，最后维度的大小必须为3。
lower：浮点型，随机饱和因子的下界。
upper：浮点型，随机饱和因子的上界。
seed：特定于操作的seed，它将与图层seed一起使用，以确定将在此操作中使用的实际seed</code><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751301716.png" alt="Alt text"><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557751310134.png" alt="Alt text"><br>对于每一个文件夹，都扩充为20张图片，最后扩充完成后图片总数为<strong>2349</strong>张图片。</li>
</ol>
<hr>
<p>在数据扩充的过程当中对于数据的读取使用的是<strong>tensorflow</strong>内置的一些函数，在此相关特点记录如下：</p>
<ol>
<li><code>tf.gfile.FastGFile(path,decodestyle)
函数功能：实现对图片的读取，类似于python提供的文本操作open()函数，path是要打开的文件名，decodestyle是以何种方式去读写
函数参数：(1)path：图片所在路径 (2)decodestyle:图片的解码方式。(&#39;r&#39;:UTF-8编码; &#39;rb&#39;:非UTF-8编码)</code></li>
<li><code>tf.image.decode_jpeg &amp; tf.image.encode_jpeg
tensorflow里面给出了一个函数用来读取图像，不过得到的结果是最原始的图像，是没有有经过解码的图像，这个函数为tf.gfile.FastGFile（&#39;path&#39;， &#39;r&#39;）.read()。如果要显示读入的图像，那就需要经过解码过程， tensorflow里面提供解码的函数有两个, tf.image.decode_jepg和tf.image.decode_png分别用于解码jpg格式和png格式的图像进行解码，得到图像的像素值，这个像素值可以用于显示图像。如果没有有解码，读取的图像是一个字符串，没法显示。 一张RGB图像可以看成一个三维的矩阵，矩阵中的每一个数表示了图像上不同位置，不同颜色的亮度。然而图像在存储时并不是直接记录这些矩阵中的数字，而是记录经过压缩编码之后的结果。所以要将一张压缩编码了的图象还原成一个三维矩阵。需要解码的过程。下面的代码示范了如何tensorflow对 JPEG 格式图片的编码/解码函数：
解码：
  将图像使用JPEG的格式解码从而得到图像对应的三维矩阵。Tensorflow还提供了tf.image.decode_png函数对png格式的图像进行编码。解码之后的结果(image_data)为一个张量， 在使用他的取值之前需要明确调用运行的过程。 Decode a JPEG-encoded image to a uint8 tensor 所以这里的 image_data 已经是一个tensor。image_data = tf.image.decode_jpeg(image_raw_data)
编码：
  将表示一张图像的三维矩阵重新按照jpeg格式编码并存入文件中。打开这张图像就可以得到和原始图像一样的图像， encoded_image = tf.image.encode_jpeg(image_data)</code><br><a href="https://blog.csdn.net/qq_43225437/article/details/88018408" target="_blank" rel="noopener">图像处理：编码解码处理(tf.image.decode_jpeg)</a></li>
</ol>
<hr>
<p><strong>最后补充一些数据增广的博文以作参考：</strong></p>
<ol>
<li><a href="https://www.leiphone.com/news/201805/avOH5g1ZX3lAbmjp.html" target="_blank" rel="noopener">数据增强：数据有限时如何使用深度学习 ？</a></li>
<li><a href="https://blog.csdn.net/m0_38100350/article/details/84559361" target="_blank" rel="noopener">使用tensorflow实现数据增强</a></li>
<li><a href="https://blog.csdn.net/sinat_29957455/article/details/83279741" target="_blank" rel="noopener">tensorflow实现inception Net数据增强</a></li>
<li><a href="https://blog.csdn.net/sinat_29957455/article/details/80629098" target="_blank" rel="noopener">tensorflow实现数据增强(随机裁剪、翻转、对比度设置、亮度设置)</a></li>
<li><a href="https://blog.csdn.net/jeffery0207/article/details/79897333" target="_blank" rel="noopener">tensorflow数据增强</a></li>
<li><a href="https://blog.csdn.net/wc781708249/article/details/80060345" target="_blank" rel="noopener">Tensorflow数据增强</a></li>
<li><a href="http://www.cnblogs.com/bonelee/p/9017114.html" target="_blank" rel="noopener">迁移学习——使用Tensorflow和VGG16预训模型进行预测</a></li>
<li><a href="https://www.jianshu.com/p/c3a9573ecb0d" target="_blank" rel="noopener">TensorFlow 可用的数据增强</a></li>
</ol>
</blockquote>
<h4 id="2-3-3-创建自己的训练数据集"><a href="#2-3-3-创建自己的训练数据集" class="headerlink" title="2.3.3 创建自己的训练数据集"></a>2.3.3 创建自己的训练数据集</h4><p>刚开始学习<strong>TensorFlow</strong>的时候都是跑官方的例子：比如我们熟知的：<strong>MNIST</strong>手写体数据集或者<strong>cifar-10/100</strong>数据集，这些数据集都是官方以及整理好了的，直接使用就好。但是后面实际上不可能总是官方的数据集跑一跑就能学好<strong>TensorFlow</strong>的，所以一些项目需要处理自己的数据，整理自己的数据集。做过<strong>Kaggle</strong>竞赛的应该很熟悉<strong>.csv</strong>文件了，<strong>.csv</strong>文件非常方便,但是通常读取的时候,是一次性读取到内存里面的.要是内存小的话,就要想其他的办法了,那就变得很麻烦了.</p>
<p>或者有时候,从硬盘上面直接读取图片啊什么的,因为图片的文件格式,存放位置各种各样等等一些因素,要是想在训练阶段直接这么使用的话,就更加麻烦了.所以,对于数据进行统一的管理是很有必要的<strong>.TFRecord</strong>就是对于输入数据做统一管理的格式.加上一些多线程的处理方式,使得在训练期间对于数据管理把控的效率和舒适度都好于暴力的方法. 小的任务什么方法差别不大,但是对于大的任务,使用统一格式管理的好处就非常显著了.因此, 有必要熟悉<strong>.TFRecord</strong>的使用方法.</p>
<p>在本项目中主要用于将自己的数据做成<strong>Tfrecords</strong>，以便<strong>tensorflow</strong>能够很好的进行划分<strong>batch</strong>和<strong>Tensor</strong>读取，对于数据量较小而言，可能一般选择直接将数据加载进内存，然后再分<strong>batch</strong>输入网络进行训练，如果数据量较大，这样的方法就不适用了，因为太耗内存，所以这时最好使用<strong>tensorflow</strong>提供的队列<strong>queue</strong>，也就是从文件读取数据。对于一些特定的读取，比如<strong>csv</strong>文件格式，官网有相关的描述，因此这里使用<strong>tensorflow</strong>内定标准格式——<strong>TFRecords</strong>，实现比较高效的数据读取方式。</p>
<p><strong>TFRecords</strong>其实是一种二进制文件，虽然它不如其他格式好理解，但是它能更好的利用内存，更方便复制和移动，并且不需要单独的标签文件。<strong>TFRecords</strong>文件包含了<strong>tf.train.Example</strong> 协议内存块(<strong>protocol buffer</strong>)(协议内存块包含了字段 <strong>Features</strong>)。我们可以写一段代码获取你的数据， 将数据填入到<strong>Example</strong>协议内存块(<strong>protocol buffer</strong>)，将协议内存块序列化为一个字符串，并且通过<strong>tf.python_io.TFRecordWriter</strong> 写入到<strong>TFRecords</strong>文件。</p>
<p>从<strong>TFRecords</strong>文件中读取数据， 可以使用<strong>tf.TFRecordReader</strong>的<strong>tf.parse_single_example</strong>解析器。这个操作可以将<strong>Example</strong>协议内存块(<strong>protocol buffer</strong>)解析为张量。</p>
<hr>
<p>对于数据扩充完成之后的<strong>2349</strong>张图片，按照<strong>90%</strong>训练集，<strong>0.5%</strong>交叉验证集，<strong>0.5%</strong>评估集的划分比例，最后得到的各部分数据情况如下：</p>
<blockquote>
<p><strong>NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 2048</strong><br><strong>NUM_EXAMPLES_PER_EPOCH_FOR_VAL = 227</strong><br><strong>NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 21</strong><br><strong>NUM_EXAMPLES_PER_EPOCH_FOR_hun = 53</strong><br><strong>NUM_EXAMPLES_PER_EPOCH_FOR_other = 3</strong></p>
</blockquote>
<hr>
<p><strong>最后补充一些有关</strong>.tfrecord<strong>文件的博文以作参考：</strong></p>
<blockquote>
<ol>
<li><a href="https://github.com/Blssel/Knowledge-learning/blob/master/tfrecord/readme.md" target="_blank" rel="noopener">Github：TF-learing/tfrecord/readme.md</a></li>
<li><a href="https://www.cnblogs.com/Stoner/p/9051030.html" target="_blank" rel="noopener">TensorFlow高效读取数据 | Ycszen-物语</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27238630" target="_blank" rel="noopener">十图详解tensorflow数据读取机制(附代码)</a></li>
<li><a href="https://blog.csdn.net/xierhacker/article/details/72357651" target="_blank" rel="noopener">TensorFlow学习（十一）：保存TFRecord文件</a></li>
<li><a href="https://lguduy.github.io/2017/05/20/TensorFlow%E6%95%99%E7%A8%8B-%E5%A4%84%E7%90%86%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE/#%E5%89%8D%E8%A8%80" target="_blank" rel="noopener">TensorFlow教程：利用TensorFlow处理自己的数据</a></li>
<li><a href="https://blog.csdn.net/shenxiaolu1984/article/details/52857437" target="_blank" rel="noopener">TensorFlow动手玩——数据导入2</a></li>
<li><a href="https://blog.csdn.net/wiinter_fdd/article/details/72835939" target="_blank" rel="noopener">用Tensorflow处理自己的数据：制作自己的TFRecords数据集——Cats_vs_Dogs</a></li>
<li><a href="https://blog.csdn.net/Best_Coder/article/details/70141075?locationNum=3&amp;fps=1" target="_blank" rel="noopener">Tensorflow 训练自己的数据集（一）（数据直接导入到内存）</a></li>
<li><a href="https://www.cnblogs.com/wktwj/p/7227544.html" target="_blank" rel="noopener">tensorflow训练自己的数据集实现CNN图像分类1</a></li>
<li><a href="https://blog.csdn.net/yimi_ac/article/details/79008555" target="_blank" rel="noopener">tensorflow实现逻辑回归，在kaggle《泰坦尼克》训练并测试准确率</a></li>
<li><a href="https://blog.csdn.net/briblue/article/details/80789608" target="_blank" rel="noopener">你可能无法回避的 TFRecord 文件格式详细讲解</a></li>
<li><a href="https://blog.csdn.net/happyhorizion/article/details/77894055" target="_blank" rel="noopener">tensorflow读取数据-tfrecord格式</a></li>
<li><a href="https://blog.csdn.net/weixin_42001089/article/details/81172028" target="_blank" rel="noopener">tensorflow中的tfrecord数据操作</a></li>
</ol>
</blockquote>
<h4 id="2-3-4-模型1效果"><a href="#2-3-4-模型1效果" class="headerlink" title="2.3.4 模型1效果"></a>2.3.4 模型1效果</h4><blockquote>
<p>由于采用分类的思想去建立模型的，最后的效果并不是很好，且分类的扩充的数据以及分类训练完毕的保存的模型参数已被删除，所以在此只是简要说一下最后的结果，最后的预测结果与真实结果相差在<strong>10 kg</strong>以内的数目仅为<strong>7</strong>个左右，由于采用分类的思想，使得本身的预测结果就是一个区间范围的不准确的值，而预测结果大体正确的个数又实在太少，即使调参多次，效果也不甚明显，所以<strong>模型1</strong>最后没有被采用.</p>
</blockquote>
<h3 id="2-4-设计思路2"><a href="#2-4-设计思路2" class="headerlink" title="2.4 设计思路2"></a>2.4 设计思路2</h3><blockquote>
<p>在第二种设计思路中，前面依旧采用的是卷积层获得数据特征，后面也是跟着几层全连接神经网络，只是最后不再是使用分类的方式对数据进行预测，而是采用了回归的方式进行数据预测，将全连接神经网络最后一层的多个分类神经元切换为只有一个回归神经元，以此得到对体重的准确预测.</p>
</blockquote>
<h4 id="2-4-1-网络结构"><a href="#2-4-1-网络结构" class="headerlink" title="2.4.1 网络结构"></a>2.4.1 网络结构</h4><p>整个网络架构如下图所示：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/Weights_Predict_of_Pig_Framework.jpg" alt="Alt text"><br>其中最好的结果中：<strong>卷积层有5层，池化层有3层，全连接层有3层.</strong></p>
<h4 id="2-4-2-损失定义"><a href="#2-4-2-损失定义" class="headerlink" title="2.4.2 损失定义"></a>2.4.2 损失定义</h4><p>由于<strong>模型2</strong>采用的是<strong>MSE均方误差损失</strong>，因此有定义如下：</p>
<blockquote>
<ol>
<li>TensorFlow官方提供的MSE损失：<br><strong>reg_loss = tf.losses.mean_squared_error(labels_holder, logits)</strong></li>
<li>自定义实现MSE损失：<br><strong>reg_loss = tf.reduce_mean(tf.square(labels_holder - logits))</strong></li>
<li><strong>经验风险：对训练集中的所有样本点损失函数的平均最小化.</strong><script type="math/tex; mode=display">R_{emp} = \frac{1}{N}\sum^{N}_{i=1}L(y_i, f(x_i))</script></li>
<li><strong>结构风险：在经验风险函数后面加一个正则化项（惩罚项）便是结构风险.</strong><script type="math/tex; mode=display">R_{srm} = \frac{1}{N}\sum^{N}_{i=1}L(y_i, f(x_i)) + \lambda J(f)</script></li>
<li><strong>经验风险</strong>越小，模型决策函数越复杂，其包含的参数越多，当<strong>经验风险</strong>函数小到一定程度就出现了过拟合现象。也可以理解为模型决策函数的复杂程度是过拟合的必要条件，那么我们要想防止过拟合现象的方式，就要破坏这个必要条件，即降低决策函数的复杂度。也即，让惩罚项$J(f)$最小化，现在出现两个需要最小化的函数了。我们需要同时保证<strong>经验风险函数</strong>和<strong>模型决策函数</strong>的复杂度都达到最小化，一个简单的办法把两个式子融合成一个式子得到<strong>结构风险函数</strong>然后对这个<strong>结构风险函数</strong>进行最小化。</li>
<li>需要注意的是相较于全连接神经网络，卷积神经网络的参数个数是比较小的，而整个模型中参数暴涨的部分来源于全连接神经网络，我们以2012年的经典之作<strong>AlexNet</strong>为例可以看出其中的端倪，下面是<strong>AlexNet</strong>的网络结构：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557757489219.png" alt="Alt text"></li>
</ol>
<p><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557757503753.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557757601052.png" alt="Alt text"></p>
<p> <img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557757837591.png" alt="Alt text"><br> 从最后一张图可以看出一个比较有意思的一点，在前面几个卷积层中，虽然计算量很大，但是参数量很小，只占用<strong>AlexNet</strong>总参数量的很小一部分，但是后面的全连接层的参数却比卷积层要大的多的多。因此，为了避免过拟合，我在全连接层的所有隐藏层中都加入了<strong>L2_Regularization</strong>.最后通过<strong>tf.add_to_collection()</strong>和<strong>tf.add_n(tf.get_collection(‘losses’), name=’total_loss’)</strong>将正则化项加入损失中.</p>
<p> <strong>其他：</strong><br> <strong>总体损失 = MSE损失 + 所有权重的L2损失</strong>.</p>
<ol>
<li><strong>tf.add_to_collection(‘losses’, cross_entropy_loss)</strong>代表将损失放入到计算图中关键字为<strong>losses</strong>下，这样可以方便一起获得所有损失.</li>
<li><strong>tf.get_collection(‘losses’)</strong>把前面所有的损失获取得到，<strong>tf.add_n</strong>则是把所有获取得到的损失加起来. 所以也就是说<strong>losses</strong>作为关键字(类似文件夹一样), <strong>add_to_collection</strong>将所有的损失放在一起，<strong>get_collection</strong>获取同样的关键字下的内容.</li>
<li><strong>add_to_collection</strong>以列表的形式存放，所以<strong>get_collection</strong>获取的是列表.</li>
<li><strong>get_collection</strong>返回的列表的顺序和<strong>add_to_collection</strong>加入的值的顺序是一致的，也就是谁先加入，谁先输出.</li>
</ol>
</blockquote>
<h4 id="2-4-3-优化函数"><a href="#2-4-3-优化函数" class="headerlink" title="2.4.3 优化函数"></a>2.4.3 优化函数</h4><p>在本项目中使用了三种不用的优化函数，分别是：</p>
<blockquote>
<ol>
<li><strong>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</strong></li>
<li><strong>optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)</strong></li>
<li><strong>optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</strong><br>其中效果最好的是<strong>AdamOptimizer</strong>.</li>
</ol>
</blockquote>
<h4 id="2-4-4-训练结果"><a href="#2-4-4-训练结果" class="headerlink" title="2.4.4 训练结果"></a>2.4.4 训练结果</h4><p>在每一个<strong>batch_size</strong>有<strong>64</strong>个训练样本，每个<strong>Epoch</strong>有<strong>32</strong>个<strong>batch_size</strong>，一共训练<strong>40</strong>个<strong>Epoch</strong>个情况下，训练集损失和交叉验证集损失如下：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/Reg_Loss_Plot.jpg" alt="Alt text"><br>其中最好结果为预测值与真实值(评估集有<strong>21</strong>个样本的情况下)相差<strong>10 kg</strong>以内的样本个数为<strong>16</strong>个，达到较好水平，详细结果如下：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557759001960.png" alt="Alt text"><br>其中对于体重为<strong>100+</strong>体重的猪的预测则是全部预测准确：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557759064536.png" alt="Alt text"><br>整体的调参结果如下图记录：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557759148516.png" alt="Alt text"></p>
<h4 id="2-4-5-卷积层可视化"><a href="#2-4-5-卷积层可视化" class="headerlink" title="2.4.5 卷积层可视化"></a>2.4.5 卷积层可视化</h4><p>对于卷积层可视化可以有效的发现卷积核是否提取到了有用的特征，因此代码中加入了相应的可视化部分用于可视化卷积层和池化层，接下来展示两张不同重量的照片的可视化结果。</p>
<ol>
<li><p>下面第一张照片真实重量为<strong>21.60 kg</strong>, 预测重量为<strong>22.07 kg</strong>，展示了原始图片以及<strong>5</strong>个卷积层和<strong>3</strong>个池化层的输出效果：</p>
<blockquote>
<p><strong>Original_Image：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1_Original_Image.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv1_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/2_Conv1.jpg" alt="Alt text"><br>卷积层<strong>1</strong>可以明显的看出在多个卷积核的作用下，原始图像的边缘轮廓(包括<strong>垂直和水平边缘</strong>)信号被放大，而且由于该图片的背景与猪的主体区分度比较明显，因此整个猪的形态较好的被检测出来了.</p>
<ol>
<li>需要注意的是卷积核(滤波器核)的尺寸数目按照已有的经验选择的都是奇数大小，这里不再详述；</li>
<li>卷积尺寸计算公式如下：<blockquote>
<p>输入数据体的尺寸为$W_1 \times H_1 \times D_1$，卷积核的<strong>4</strong>个超参数为：<strong>卷积核的数目K</strong>；<strong>卷积核的空间尺寸f</strong>；<strong>卷积步长s</strong>；<strong>Padding类型(SAME 和 VALID)</strong>.<br>输出尺寸：</p>
<script type="math/tex; mode=display">n = \left\lfloor\frac{n + 2p - f}{s}\right\rfloor + 1</script></blockquote>
</li>
<li>在本项目中所有卷积层都采用的<strong>SAME Padding</strong>，避免图像块尺寸缩减过快，所谓<strong>VALID Padding</strong>，其实就是卷积过程中不进行填充，每次卷积完后整个图像块的尺寸依照卷积计算公式减小；而<strong>SAME Padding</strong>则是在图像块四周进行<strong>0填充</strong>，这样对于图像四周的边角也会更好的提取信息，图像卷积完毕大小不变，有效避免的了图像丢失角落和边沿的信息.</li>
</ol>
<hr>
<p><strong>Pool1_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/3_Pool1.jpg" alt="Alt text"><br>在本项目中采用了使用频率最高的<strong>Max Pooling</strong>方式，对前一层卷积过后的特征图进行缩减，逐渐降低图像块体的空间尺寸，有效减少网络中的参数数量，使得计算资源耗费变少，有效控制过拟合，而最大池化还可以保留主要特征，筛选掉次要的特征.</p>
<hr>
<p><strong>Conv2_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/4_Conv2.jpg" alt="Alt text"><br>在第一层卷积层中，主要的是提取一些简单的边缘特征，越往后的卷积层提取的特征就越高级，在本层中有一个非常明显的特征被卷积核提取到，那就是放在猪身上的<strong>A4</strong>纸，图中明显可以看到<strong>A4</strong>的边缘轮廓被提取出来，这样印证了<strong>A4</strong>这一特征的重要性: 作为和猪的体型做参照对比的重要特征，可以很好的侧面反应出猪的体重大小，因此在<strong>模型1</strong>中使用了去水印的图片作为输入的想法是不正确的，而且在<strong>模型2</strong>中输入的图片并没有去掉水印，但是卷积过程并没有被这一特征给干扰，因此很有可能将水印当做噪点给处理掉了.</p>
<hr>
<p><strong>Pool2_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/5_Pool2.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv3_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/6_Conv3.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv4_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/7_Conv4.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv5_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/8_Conv5.jpg" alt="Alt text"></p>
<hr>
<p><strong>Pool5_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/9_Pool5.jpg" alt="Alt text"></p>
</blockquote>
</li>
<li><p>第二张照片真实重量为<strong>52.85 kg</strong>，预测重量为<strong>53.32 kg</strong>，卷积和池化过程如下：</p>
<blockquote>
<p><strong>Original_Image：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv1_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/c1.jpg" alt="Alt text"></p>
<hr>
<p><strong>Pool1_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/p1.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv2_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/c2.jpg" alt="Alt text"></p>
<hr>
<p><strong>Pool2_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/p2.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv3_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/c3.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv4_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/c4.jpg" alt="Alt text"></p>
<hr>
<p><strong>Conv5_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/c5.jpg" alt="Alt text"></p>
<hr>
<p><strong>Pool5_Layer：</strong><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/c5.jpg" alt="Alt text"></p>
</blockquote>
</li>
</ol>
<p>以上是整个卷积层的可视化过程，卷积的过程通过从初始的轮廓边缘检测到后面一层一层的低级特征组合成高级特征，最后将重要的特征提取出来以供后面特定的任务使用，整个过程可以用下面两种图来总结：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557800289969.png" alt="Alt text"></p>
<p><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557800336588.png" alt="Alt text"></p>
<h2 id="3-TensorBoard与数据可视化"><a href="#3-TensorBoard与数据可视化" class="headerlink" title="3. TensorBoard与数据可视化"></a>3. TensorBoard与数据可视化</h2><h3 id="3-1-TensorBoard简要介绍"><a href="#3-1-TensorBoard简要介绍" class="headerlink" title="3.1 TensorBoard简要介绍"></a>3.1 TensorBoard简要介绍</h3><blockquote>
<p><strong>Tensorboard</strong> 是 <strong>tensorflow</strong> 内置的一个可视化工具，也是一个 <strong>web</strong> 应用套件，支持七种可视化包括 <strong>SCALARS</strong> (标量)、<strong>IMAGES</strong>(图像)、<strong>AUDIO</strong>(音频)、<strong>GRAPHS</strong>(数据流图)、<strong>DISTRIBUTIONS</strong>(训练数据分布图)、<strong>HISOGRAMS</strong>(训练过程中数据的柱状图)和<strong>EMBEDDINGS</strong>(展示词向量的投影分布)。</p>
<p>它通过将 <strong>tensorflow</strong> 程序输出的日志文件的信息可视化使得 <strong>tensorflow</strong> 程序的理解、调试和优化更加简单高效。<strong>Tensorboard</strong> 的可视化依赖于 <strong>tensorflow</strong> 程序运行输出的日志文件，因而 <strong>tensorboard</strong> 和 <strong>tensorflow</strong> 程序在不同的进程中运行，<strong>TensorBoard</strong> 会自动读取最新的 <strong>TensorFlow</strong> 日志文件，并呈现当前 <strong>TensorFlow</strong> 程序运行的最新状态。</p>
</blockquote>
<h3 id="3-2-本项目中的应用"><a href="#3-2-本项目中的应用" class="headerlink" title="3.2 本项目中的应用"></a>3.2 本项目中的应用</h3><p>由于 <strong>TensorBoard</strong> 的便利，因此在本项目的模型中添加相应的对张量进行汇总的节点，包含相应的 <strong>MEAN</strong>(均值)、<strong>STDDEV</strong>(标准差)、<strong>MAX</strong>(最大值)、<strong>MIN</strong>(最小值)、<strong>HISTOGRAM</strong>(参数分布直方图)、<strong>Activation_Summary</strong>(激活值汇总)、<strong>Losses_Summary</strong>(损失值汇总)以及<strong>Image_Summary</strong>(图像汇总)等，对于整体的分析十分便利，且如果不是用<strong>TensorBoard</strong>的情况下要对各个信息进行汇总会产生各个不同的图表，而使用<strong>TensorBoard</strong>进行汇总则仅仅产生一个日志文件，而这个日志文件包含各个需要分析的图表等，因此有利于对模型进行更好的分析和调参。</p>
<h3 id="3-2-1-TensorBoard可视化计算图"><a href="#3-2-1-TensorBoard可视化计算图" class="headerlink" title="3.2.1 TensorBoard可视化计算图"></a>3.2.1 TensorBoard可视化计算图</h3><p>在<strong>2.4.1 网络结构</strong>中列出了整个模型的框架，但是那是手动使用<strong>PPT</strong>画出的，而在<strong>TensorBoard</strong>中，直接可以可视化我们的模型结构：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557995617379.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557995674798.png" alt="Alt text"><br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557995708566.png" alt="Alt text"><br>整个结构和<strong>2.4.1 网络结构</strong>中的结构是一致的，只不过这里还可以打开每个节点看到每个张量流的流动方向以及经过具体什么样的操作节点。</p>
<h3 id="3-2-1-TensorBoard可视化卷积过程"><a href="#3-2-1-TensorBoard可视化卷积过程" class="headerlink" title="3.2.1 TensorBoard可视化卷积过程"></a>3.2.1 TensorBoard可视化卷积过程</h3><p>与手动设计卷积层可视化不同，在<strong>TensorFlow</strong>中可以使用内置的库进行可视化，只要在相应的节点处添加图像汇总节点即可在<strong>TensorBoard</strong>中查看结果，如下：</p>
<ol>
<li><p>原始图像：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557995968562.png" alt="Alt text"></p>
</li>
<li><p>经过两层卷积之后的结果：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557995941613.png" alt="Alt text"></p>
</li>
</ol>
<p>从<strong>TensorBoard</strong>的卷积可视化中也可以印证前面手动可视化卷积中分析的那样：卷积层有效的检测出图像中猪的边缘轮廓信息，除此之外，在卷积可视图中可以清晰的看到一个类似矩形的物体边缘轮廓，而这个物体正是放在猪身上用于起到体型大小参考作用的<strong>A4</strong>纸，因此<strong>这是一个重要的特征，而且卷积核有效的提取出了这个特征，从而影响到最终的预测结果。相较于在机器学习中需要人工的配合着相应的经验去构造一系列特征而言，深度学习能够很好完成这项任务，自动的去学习提取对任务有用的特征，而无需外部人工干涉！</strong></p>
<h3 id="3-2-3-TensorBoard可视化损失变化过程"><a href="#3-2-3-TensorBoard可视化损失变化过程" class="headerlink" title="3.2.3 TensorBoard可视化损失变化过程"></a>3.2.3 TensorBoard可视化损失变化过程</h3><p><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557996622008.png" alt="Alt text"><br>相较于需要每隔一定步数就要收集相应的损失，以便手动构造代码画出损失图而言，<strong>TensorBoard</strong>直接通过添加节点即可将损失的信息汇总到日志文件中。</p>
<h3 id="3-2-4-TensorBoard验证Batch-Norm"><a href="#3-2-4-TensorBoard验证Batch-Norm" class="headerlink" title="3.2.4 TensorBoard验证Batch_Norm"></a>3.2.4 TensorBoard验证Batch_Norm</h3><p>在<strong>第4部分</strong>中比较详细的表述了<strong>Batch_Norm</strong>的作用，在此主要是通过手动绘出的图像和<strong>TensorBoard</strong>中的可视化用以验证加入<strong>Batch_Norm</strong>的正确性。</p>
<ol>
<li><p>手动绘图：</p>
<blockquote>
<p>1.1 添加<strong>Batch_Norm</strong>：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557999395593.png" alt="Alt text"></p>
<p>1.2 关闭<strong>Batch_Norm</strong>：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/1557999373890.png" alt="Alt text"></p>
</blockquote>
</li>
<li><p><strong>tensorboard</strong>可视化：</p>
<blockquote>
<p>2.1 添加<strong>Batch_Norm</strong>：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/TIM截图20190516172839.png" alt="Alt text"></p>
<p>2.2 关闭<strong>Batch_Norm</strong>：<br><img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/0e573ef2c4b36572b6f796cfef6899b.png" alt="Alt text"><br>在图中明显可以看到对于全连接层三个隐藏层中激活值的变化，未添加<strong>Batch_Norm</strong>层的激活值很快便缩小到0附近，这样是不能提供梯度的，因此损失值一致在一个值左右震荡，无法收敛。</p>
</blockquote>
</li>
</ol>
<h2 id="4-遇到的问题"><a href="#4-遇到的问题" class="headerlink" title="4. 遇到的问题"></a>4. 遇到的问题</h2><h3 id="4-1-神经网络如何处理回归问题？"><a href="#4-1-神经网络如何处理回归问题？" class="headerlink" title="4.1 神经网络如何处理回归问题？"></a>4.1 神经网络如何处理回归问题？</h3><p>首先需要明确一点的是，神经网络多用于分类，用于连续型变量的回归预测问题(比如预测<strong>身高、体重、年龄、温度、质量得分</strong>等)很少，即使非要用神经网络处理这类问题，多数的处理方式也是连续型变量按照区间进行划分，转化为离散型的类别变量，这样也就变成了本项目中的<strong>模型1</strong>了。</p>
<p>对于本项目的<strong>模型2</strong>，初始的考虑方式是直接将最后的分类层转换为只有一个神经元的回归层，然后将前面的隐藏层具有非线性映射的激活函数全部改为线性映射，最后的结果是<strong>Total Loss</strong>高达上千乃至上万，整个模型的<strong>Loss</strong>完全无法降下去，<strong>40</strong>个<strong>Epoch</strong>结束之后，<strong>Loss</strong>完全是在一个值左右震荡。</p>
<p>后来改进的方法如下：</p>
<blockquote>
<ol>
<li>前面的卷积层依旧不变，主要的变化是对于后面标签值进行改变的；</li>
<li>对于输入的标签值，进行标准化(<strong>sklearn.preprocessing.StandardScaler</strong>);</li>
<li>由于使用神经网络的其中一个主要原因就是考虑到激活函数非线性映射带来的强大拟合能力，因此如果中间所有的激活函数都改为线性的了则神经网络就只是一个线性映射了，没有实际的作用了，所以隐藏层还是加上激活函数；</li>
<li>由于激活函数存在区间压缩的功能，所以如果激活函数使用的是<strong>sigmoid</strong>，则标签值在传入网络做训练之前需要先放缩到$[0, 1]$之间(<strong>MinMaxScaler(feature_range=(0, 1))</strong>)，相应的如果激活函数是<strong>tanh</strong>，则需要将标签放缩到$[-1, 1]$之间；</li>
<li>最后一层输出层我这里采用的是不适用非线性的激活函数，考虑到输出值不将其看作某一类概率的情况，我选择使用了线性映射输出，实际的结果也要比使用非线性映射输出要好；</li>
<li>最后需要查看真实的标签以及预测值的情况的时候，需要将放缩后的结果进行反向放缩，这一点尤为重要(<strong>mm_train_scaler.inverse_transform(labels_batch)</strong>)</li>
<li>修改交叉熵损失函数为<strong>MSE</strong>均方误差损失；</li>
<li><a href="https://www.zhihu.com/question/68388247" target="_blank" rel="noopener">如何使用卷积神经网络做数值回归任务?</a></li>
</ol>
</blockquote>
<h3 id="4-2-网络损失震荡，无法降下来？"><a href="#4-2-网络损失震荡，无法降下来？" class="headerlink" title="4.2 网络损失震荡，无法降下来？"></a>4.2 网络损失震荡，无法降下来？</h3><p>按照<strong>3.1</strong>的方式修改网络结构为回归结构之后，虽然损失的值变成了小数，但是依旧是在某个值上下震荡，无法将损失降下来，后来考虑可能是由于网络的层数的原因造成数据在不同层之间的分布改变了，导致每一层其实是在训练不同的数据，结果自然降不下来。</p>
<p>机器学习领域有个很重要的假设： <strong>IID独立同分布假设</strong>，$\underline{就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障}$。那<strong>BatchNorm</strong>的作用是什么呢？$\underline{ BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的}$。</p>
<p><strong>BatchNorm</strong>就是用来解决 <strong>Internal Covariate Shift</strong> 问题的，在引入<strong>Internal Covariate Shift</strong>是什么之前？先引入<strong>covariate shift</strong>的概念: $\underline{如果ML系统实例集合[X,Y]中的输入值X的分布老是变，网络模型很难稳定的学规律}$，这不符合<strong>IID假设</strong>，<strong>ML</strong>系统还得去学习怎么迎合这种分布变化。</p>
<p>对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临<strong>covariate shift</strong>的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的 <strong>Internal Covariate Shift</strong>，<strong>Internal</strong>指的是深层网络的隐层，是发生在网络内部的事情，而不像<strong>covariate shift</strong>问题只发生在输入层。</p>
<p><strong>BatchNorm</strong>的基本思想如下：<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong></p>
<p>在本项目中通过添加<strong>BatchNorm</strong>取得了巨大的效果，之前完全降不下的损失也随着迭代步数的增加在逐步下降，而非震荡甚至发散了；在代码中主要是添加在了卷积层和全连接网络的隐藏层的激活值还没有通过非线性激活函数之前的位置。整个训练过程相较于没有添加之前，训练速度提升，收敛速度大大加快，而且还有类似<strong>Dropout</strong>的正则化效果.</p>
<hr>
<p><strong>最后补充一些有关</strong>Batch_Normalization<strong>的博文以作参考：</strong></p>
<blockquote>
<ol>
<li><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">深入理解Batch Normalization批标准化</a></li>
<li><a href="https://blog.csdn.net/dongjbstrong/article/details/80447110" target="_blank" rel="noopener">CSDN——正确使用Tensorflow Batch_normalization</a></li>
<li><a href="https://blog.csdn.net/huitailangyz/article/details/85015611" target="_blank" rel="noopener">CSDN——tensorflow中的batch_norm以及tf.control_dependencies和tf.GraphKeys.UPDATE_OPS的探究</a></li>
<li><a href="https://blog.csdn.net/u014061630/article/details/85104491" target="_blank" rel="noopener">CSDN——TensorFlow 中 Batch Normalization API 的一些坑</a></li>
<li><a href="https://blog.csdn.net/computerme/article/details/80836060?utm_source=blogxgwz1" target="_blank" rel="noopener">CSDN——tensorflow batch_normalization的正确使用姿势</a></li>
</ol>
</blockquote>
<h3 id="4-3-不同MinMaxScaler对象之间不能混着用"><a href="#4-3-不同MinMaxScaler对象之间不能混着用" class="headerlink" title="4.3 不同MinMaxScaler对象之间不能混着用"></a>4.3 不同MinMaxScaler对象之间不能混着用</h3><p>训练到后期的时候，遇到过这样一个问题：在对<strong>21</strong>个体重区间小于<strong>100+</strong>的样本进行测试的时候，得到的真实值结果居然全部在<strong>100+</strong>左右，如何修改都没有效果，后来发现<strong>MinMaxScaler</strong>对象对体重<strong>100+</strong>进行放缩和对体重小于<strong>100+</strong>进行放缩的顺序不一样的时候会出现正确结果或错误结果各有出现的情况，最后发现是由于申明<strong>MinMaxScaler</strong>对象的时候只申明了一个对象，而对所有的数据都只用一个对象进行放缩和反向放缩的时候，就会出现上面的错误现象，通过查找资料发现：</p>
<blockquote>
<p>一般情况下，或者严格点说，在监督学习中，我们需要利用训练集数据对测试集数据进行预测。<br>这里隐含了一个假设，就是训练数据和测试数据实际上是同分布的（因此我们才可以使用训练数据集来预测测试数据集），来自于同一个总体。<br>在进行标准化的过程中就将训练集的均值和方差当做是总体的均值和方差，因此对测试集使用训练集的均值和方差进行预处理。</p>
</blockquote>
<p>也就是说体重<strong>100+</strong>的测试数据和体重<strong>100+</strong>以下的测试数据的分布是不相同的，所以，如果更改两者的放缩顺序，很有可能就造成上面的错误，从而<strong>100+</strong>以下的样本被反向放缩的时候，得到的结果却是<strong>100+</strong>样本反向放缩的结果，因此修改方法就是：将<strong>MinMaxScaler</strong>对象各自申请<strong>4</strong>个，对应<strong>train\val\eval\100+</strong>这<strong>4</strong>个样本集，在反向放缩的时候也是各自对应各自的<strong>MinMaxScaler</strong>对象，这样错误就解决了。</p>
<h3 id="4-4-Tensor-Feed-Error"><a href="#4-4-Tensor-Feed-Error" class="headerlink" title="4.4 Tensor Feed Error"></a>4.4 Tensor Feed Error</h3><p>在制作<strong>Tfrecords</strong>文件的<strong>.py</strong>文件中有一个函数叫<strong>read_and_decode</strong>，主要用于读取已经制作完成的<strong>tfrecords</strong>文件，并以多线程的方式将数据以<strong>batch</strong>大小返回，但是在训练的时候出现过直接给<strong>Session</strong>喂<strong>read_and_decode</strong>返回的数据报错的情况，后面发现原因如下：</p>
<blockquote>
<p>需要注意的是<strong>read_and_decode</strong>函数返回的<strong>images_test</strong>和<strong>labels_test</strong>仅仅是<strong>tensorflow</strong>数据流图上定义好的。实际上它并不是一个实际的数据，仅仅是一个<strong>Tensor</strong>张量，而<strong>feed_dict</strong>必须接受的是实际的数据。实际的数据需要<strong>sees.run()</strong>来获得.</p>
<p><strong>sees.run()</strong>返回<strong>type</strong>为<strong>np.ndarray</strong>格式的数据才是真正的数据，也就是说在<strong>sess.run()</strong>之前的都只是在数据流图中流动的<strong>tensor</strong>，只有真正需要<strong>train</strong>的时候是需要使用<strong>sees.run()</strong>获得真正的数据然后进行<strong>feed</strong>的:</p>
<pre><code class="lang-flow">st=&gt;start: read_and_decode
e=&gt;end: feed_dict(images)
op1=&gt;operation: image_batch(tensor)
op2=&gt;operation: images=sess.run(image_batch)(real data)
st-&gt;op1-&gt;op2-&gt;e
</code></pre>
<p>需要注意的是，如果已经获取了<strong>np.ndarray</strong>(真实数据格式)的数据后在<strong>feed</strong>的时候需要注意<strong>images_placeholder</strong>的<strong>shape</strong>，也就是传入数据要和<strong>placeholder</strong>的<strong>shape</strong>一致，如果不一致，可以使用<strong>tf.reshape</strong>进行<strong>shape</strong>转换<strong>(tf.reshape会将数据格式转换为tensor，切记)</strong>，然后转换为<strong>np.ndarray</strong>; 要么就在<strong>sess.run()</strong>之后使用<strong>np</strong>的方式，也就是<strong>data.reshape()</strong>的方式直接转换，这样避免还需要数据格式的转换.</p>
</blockquote>
<h3 id="4-5-Batch-size与输入数据的placeholder"><a href="#4-5-Batch-size与输入数据的placeholder" class="headerlink" title="4.5 Batch_size与输入数据的placeholder"></a>4.5 Batch_size与输入数据的placeholder</h3><p>在<strong>模型1</strong>与<strong>模型2</strong>最开始的测试部分的代码中，使用的方式是一次性将多张测试图片做成<strong>.tfrecords</strong>文件，然后利用代码读取<strong>.tfrecords</strong>文件的方式进行数据预测，这样的方式存在比较大的几个问题。</p>
<p>首先，每一次有新的预测任务的时候，都需要手动的去执行<strong>tfrecord</strong>文件制作的函数，生成文件，然后才能读取文件进行预测，这样是无法做到自动化的；其次，有一个很大的问题是在模型所在文件中，最开始定义数据输入的<strong>placeholder</strong>的时候，<strong>shape = [batch_size, img_size, img_size, img_channel]</strong>。</p>
<p>对应的，在卷积完毕之后，准备传入全连接层之前，有一步将卷积块拉长成向量的操作的，其中进行<strong>reshape</strong>的时候，开始的操作是: <strong>[batch_size, -1]</strong>, 这样，可以直接根据<strong>batch_size</strong>的大小去计算向量应该的长度，而无需因为卷积层的参数变化而每次都要重新计算向量的长度，但是这里恰巧是手动转为自动的关键所在。</p>
<p>要想转为自动，首先，数据输入的<strong>placeholder</strong>就不能写死，这样就将<strong>shape</strong>修改为<strong>[None, img_size, img_size, img_channel]</strong>, 这样每次不管传入的数据的批次大小如何都能应对，相应的，测试的时候数据的批次大小和训练的时候数据的批次大小肯定是不相同的，之前一直是需要在测试和训练的时候根据不同的数据大小做相应的手动<strong>batch_size</strong>调整的，这是个很麻烦且无法自动化的事情，修改输入数据的<strong>placeholder</strong>的<strong>shape</strong>可以很好的解决这个问题。</p>
<p>除了这个地方需要修改，还有一个地方需要修改了才能起作用，之前只是单纯的修改了输入数据的<strong>placeholder</strong>的<strong>shape</strong>，但是代码运行的时候报错了，后来想了很久才发现问题所在： 主要的问题就在于上面提到的那个重点——也就是在卷积块拉长成向量那里，不能在<strong>reshape</strong>的时候还是设置参数为<strong>[batch_size, -1]</strong>，即使这个<strong>batch_size</strong>手动设置成了匹配测试数据批次大小的值，报错的根本原因就是因为将<strong>batch_size</strong>设置为测试数据批次的大小了之后，由于在测试的时候是调用训练完毕的模型的参数的，而保存好了的模型的参数中<strong>batch_size</strong>是一个保存好了的，和模型训练有关的值，而这个值是和测试时候的值是不一样的，所以存在<strong>reshape</strong>的时候大小不匹配的情况。</p>
<p>正确的方式是在上面<strong>[batch_size, -1]</strong>的地方改为<strong>[-1, feats_dim]</strong>,其中<strong>feats_dim = image_Pooled_size $\times$ image_Pooled_size $\times$ conv_kernel_num</strong>，其中<strong>image_Pooled_size</strong>是和整个结构中池化层的个数又直接关系的，而卷积层采用的<strong>SAME Padding</strong> ，因此跟卷积无关；图像每经过一个池化层，图像尺寸就缩小为原来的$\frac{1}{2}$，因此假设池化层有$n$个，则<strong>image_Pooled_size = original_image_size $\times$  $(\frac{1}{2})^n$</strong>，这样就可以直接在文件夹读取文件并预测了，无需再做成<strong>tfrecord</strong>文件去预测了.</p>
<h3 id="4-6-原始数据集问题"><a href="#4-6-原始数据集问题" class="headerlink" title="4.6 原始数据集问题"></a>4.6 原始数据集问题</h3><p>问题就不再描述了，主要希望后期的照片能达到以下要求：</p>
<blockquote>
<ol>
<li>照片尽量对焦，不要拍的模糊不清；</li>
<li>对不同的猪的拍照角度尽量为这种方向：<img src= "/img/loading.gif" data-src="/2019/05/22/Weights-Pred-of-Pigs/136.jpg" alt="Alt text"><br>当然，也可以在这基础上多拍几张，且猪的身体尽量被包含在图像中，不要有缺失；</li>
<li>选择的拍照背景尽量不要和猪的毛色混杂在一起，避免造成背景和主题混在一起无法分辨(就像狙击手身穿迷彩服，躲在背景的样子)</li>
<li>放在猪身上的纸尽量要大小一致，放的位置尽量统一，比如都放在猪肚子附近最好；</li>
<li>一张照片中不要拍摄超过一头猪；</li>
<li>体重标签在标记的时候不要出错，会影响结果精度.</li>
<li>体重超过<strong>100+</strong>的如果能有具体详细的值最好，如果都按照<strong>100</strong>来处理，应该是会影响到预测精度的。</li>
</ol>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Stoner</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/05/22/Weights-Pred-of-Pigs/">http://yoursite.com/2019/05/22/Weights-Pred-of-Pigs/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">Stoner的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/TensorFlow/">TensorFlow</a><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a><a class="post-meta__tags" href="/tags/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">全连接神经网络</a><a class="post-meta__tags" href="/tags/%E5%9B%9E%E5%BD%92/">回归</a><a class="post-meta__tags" href="/tags/%E6%A0%87%E7%AD%BE%E5%BD%92%E4%B8%80%E5%8C%96/">标签归一化</a></div><div class="post_share"><div class="social-share" data-image="http://qe0z9wdl5.bkt.clouddn.com/20200725214526.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/05/22/tensorflow-gpu-win10-installing/"><img class="prev-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725232051.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Tensorflow-GPU with Win10 Installing</div></div></a></div><div class="next-post pull-right"><a href="/2019/05/22/Batch-Norm/"><img class="next-cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725232225.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Application of Batch Normalization in Tensorflow</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/05/22/Batch-Norm/" title="Application of Batch Normalization in Tensorflow"><img class="relatedPosts_cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725232225.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2019-05-22</div><div class="relatedPosts_title">Application of Batch Normalization in Tensorflow</div></div></a></div><div class="relatedPosts_item"><a href="/2019/05/30/tensorflow-save-and-restore/" title="预测代码中遇到的问题"><img class="relatedPosts_cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725224620.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2019-05-30</div><div class="relatedPosts_title">预测代码中遇到的问题</div></div></a></div><div class="relatedPosts_item"><a href="/2019/06/04/transfer-learning/" title="VGG16迁移学习分析"><img class="relatedPosts_cover" data-src="http://qe0z9wdl5.bkt.clouddn.com/20200725224325.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-04</div><div class="relatedPosts_title">VGG16迁移学习分析</div></div></a></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Stoner</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/search/local-search.js"></script><script>if (document.getElementsByClassName('mermaid').length) {
  loadScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js',function () {
    mermaid.initialize({
      theme: 'default',
  })
})
}</script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>